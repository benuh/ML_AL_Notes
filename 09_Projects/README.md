# Module 09: Capstone Projects - Real-World ML Applications

## üéØ Learning Objectives
By the end of this module, you will:
- Apply ML knowledge to solve real-world problems end-to-end
- Build complete ML pipelines from data collection to deployment
- Gain experience with different domains: vision, NLP, time series, recommendation systems
- Develop portfolio projects that demonstrate your ML expertise

## üöÄ Project Overview

These capstone projects are designed to integrate everything you've learned across all modules. Each project includes:
- **Problem Definition**: Clear business objectives
- **Data Collection & Exploration**: Real datasets with challenges
- **Model Development**: Multiple approaches and comparisons
- **Evaluation & Validation**: Proper metrics and testing
- **Deployment**: Production-ready solutions

## üìä Project 1: E-commerce Recommendation System

### Problem Statement
Build a complete recommendation system for an e-commerce platform that suggests products to users based on their behavior, preferences, and similar users.

*Business Impact: Recommendation systems drive 35% of Amazon's revenue and 75% of Netflix viewing*

```python
"""
E-commerce Recommendation System
==============================

This project demonstrates:
- Collaborative filtering (user-based, item-based)
- Content-based filtering
- Matrix factorization (SVD, NMF)
- Deep learning approaches (Neural Collaborative Filtering)
- Hybrid systems
- Cold start problem solutions
- Real-time inference

Dataset: Amazon Product Reviews or MovieLens
Business Metrics: CTR, Conversion Rate, Revenue per User
Technical Metrics: RMSE, MAP@K, NDCG@K
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import TruncatedSVD, NMF
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import mean_squared_error
import warnings
warnings.filterwarnings('ignore')

class RecommendationSystem:
    """Complete recommendation system with multiple algorithms"""

    def __init__(self):
        self.user_item_matrix = None
        self.item_features = None
        self.user_similarity = None
        self.item_similarity = None
        self.svd_model = None
        self.content_model = None

    def prepare_data(self, ratings_file=None, products_file=None):
        """Load and prepare e-commerce data"""

        print("üì• Loading E-commerce Data")
        print("-" * 40)

        # Simulate e-commerce data if files not provided
        if ratings_file is None:
            # Generate synthetic e-commerce data
            np.random.seed(42)

            n_users = 1000
            n_products = 500
            n_ratings = 50000

            # Create realistic rating patterns
            user_ids = np.random.choice(n_users, n_ratings)
            product_ids = np.random.choice(n_products, n_ratings)

            # Simulate user preferences (some users prefer certain categories)
            ratings = []
            for user_id, product_id in zip(user_ids, product_ids):
                # Base rating influenced by user and product characteristics
                base_rating = 3.0
                user_bias = np.random.normal(0, 0.5)
                product_bias = np.random.normal(0, 0.3)
                rating = base_rating + user_bias + product_bias + np.random.normal(0, 0.2)
                rating = np.clip(rating, 1, 5)
                ratings.append(rating)

            # Create ratings dataframe
            self.ratings_df = pd.DataFrame({
                'user_id': user_ids,
                'product_id': product_ids,
                'rating': ratings
            })

            # Remove duplicate user-product pairs (keep last rating)
            self.ratings_df = self.ratings_df.drop_duplicates(['user_id', 'product_id'], keep='last')

            # Create product features
            categories = ['Electronics', 'Books', 'Clothing', 'Home', 'Sports', 'Beauty']
            brands = ['Brand_A', 'Brand_B', 'Brand_C', 'Brand_D', 'Brand_E']

            self.products_df = pd.DataFrame({
                'product_id': range(n_products),
                'category': np.random.choice(categories, n_products),
                'brand': np.random.choice(brands, n_products),
                'price': np.random.lognormal(3, 0.5, n_products),
                'description': [f"Product {i} description with features" for i in range(n_products)]
            })

        else:
            # Load real data
            self.ratings_df = pd.read_csv(ratings_file)
            self.products_df = pd.read_csv(products_file)

        print(f"‚úÖ Loaded {len(self.ratings_df)} ratings from {self.ratings_df['user_id'].nunique()} users")
        print(f"‚úÖ Loaded {len(self.products_df)} products")

        # Create user-item matrix
        self.user_item_matrix = self.ratings_df.pivot_table(
            index='user_id',
            columns='product_id',
            values='rating',
            fill_value=0
        )

        print(f"üìä User-item matrix shape: {self.user_item_matrix.shape}")
        print(f"üìä Sparsity: {(self.user_item_matrix == 0).sum().sum() / (self.user_item_matrix.shape[0] * self.user_item_matrix.shape[1]):.2%}")

        return self.ratings_df, self.products_df

    def exploratory_analysis(self):
        """Perform comprehensive EDA"""

        print("\\nüîç Exploratory Data Analysis")
        print("-" * 40)

        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('üõí E-commerce Recommendation System - EDA', fontsize=16, fontweight='bold')

        # 1. Rating distribution
        axes[0, 0].hist(self.ratings_df['rating'], bins=10, alpha=0.7, edgecolor='black')
        axes[0, 0].set_title('Rating Distribution')
        axes[0, 0].set_xlabel('Rating')
        axes[0, 0].set_ylabel('Frequency')
        axes[0, 0].grid(True, alpha=0.3)

        # 2. User activity distribution
        user_activity = self.ratings_df.groupby('user_id').size()
        axes[0, 1].hist(user_activity, bins=50, alpha=0.7, edgecolor='black')
        axes[0, 1].set_title('User Activity Distribution')
        axes[0, 1].set_xlabel('Number of Ratings per User')
        axes[0, 1].set_ylabel('Number of Users')
        axes[0, 1].set_yscale('log')
        axes[0, 1].grid(True, alpha=0.3)

        # 3. Product popularity distribution
        product_popularity = self.ratings_df.groupby('product_id').size()
        axes[0, 2].hist(product_popularity, bins=50, alpha=0.7, edgecolor='black')
        axes[0, 2].set_title('Product Popularity Distribution')
        axes[0, 2].set_xlabel('Number of Ratings per Product')
        axes[0, 2].set_ylabel('Number of Products')
        axes[0, 2].set_yscale('log')
        axes[0, 2].grid(True, alpha=0.3)

        # 4. Category popularity
        category_ratings = self.ratings_df.merge(self.products_df, on='product_id')
        category_counts = category_ratings['category'].value_counts()
        axes[1, 0].bar(category_counts.index, category_counts.values, alpha=0.7)
        axes[1, 0].set_title('Ratings by Category')
        axes[1, 0].set_xlabel('Category')
        axes[1, 0].set_ylabel('Number of Ratings')
        axes[1, 0].tick_params(axis='x', rotation=45)
        axes[1, 0].grid(True, alpha=0.3)

        # 5. Average rating by category
        avg_rating_by_category = category_ratings.groupby('category')['rating'].mean()
        axes[1, 1].bar(avg_rating_by_category.index, avg_rating_by_category.values, alpha=0.7, color='orange')
        axes[1, 1].set_title('Average Rating by Category')
        axes[1, 1].set_xlabel('Category')
        axes[1, 1].set_ylabel('Average Rating')
        axes[1, 1].tick_params(axis='x', rotation=45)
        axes[1, 1].grid(True, alpha=0.3)

        # 6. Price distribution
        axes[1, 2].hist(self.products_df['price'], bins=50, alpha=0.7, edgecolor='black', color='green')
        axes[1, 2].set_title('Product Price Distribution')
        axes[1, 2].set_xlabel('Price')
        axes[1, 2].set_ylabel('Number of Products')
        axes[1, 2].set_xscale('log')
        axes[1, 2].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

        # Print statistics
        print(f"üìä Dataset Statistics:")
        print(f"   Average rating: {self.ratings_df['rating'].mean():.2f}")
        print(f"   Rating std: {self.ratings_df['rating'].std():.2f}")
        print(f"   Most active user: {user_activity.max()} ratings")
        print(f"   Most popular product: {product_popularity.max()} ratings")
        print(f"   Average ratings per user: {user_activity.mean():.1f}")
        print(f"   Average ratings per product: {product_popularity.mean():.1f}")

    def collaborative_filtering(self):
        """Implement collaborative filtering approaches"""

        print("\\nü§ù Collaborative Filtering")
        print("-" * 40)

        # User-based collaborative filtering
        print("Computing user similarity matrix...")
        user_similarity = cosine_similarity(self.user_item_matrix)
        self.user_similarity = pd.DataFrame(
            user_similarity,
            index=self.user_item_matrix.index,
            columns=self.user_item_matrix.index
        )

        # Item-based collaborative filtering
        print("Computing item similarity matrix...")
        item_similarity = cosine_similarity(self.user_item_matrix.T)
        self.item_similarity = pd.DataFrame(
            item_similarity,
            index=self.user_item_matrix.columns,
            columns=self.user_item_matrix.columns
        )

        print(f"‚úÖ User similarity matrix: {self.user_similarity.shape}")
        print(f"‚úÖ Item similarity matrix: {self.item_similarity.shape}")

    def matrix_factorization(self):
        """Implement matrix factorization techniques"""

        print("\\nüî¢ Matrix Factorization")
        print("-" * 40)

        # SVD
        print("Training SVD model...")
        self.svd_model = TruncatedSVD(n_components=50, random_state=42)
        user_factors = self.svd_model.fit_transform(self.user_item_matrix)
        item_factors = self.svd_model.components_

        # Reconstruct ratings matrix
        predicted_ratings = user_factors @ item_factors
        self.predicted_ratings_svd = pd.DataFrame(
            predicted_ratings,
            index=self.user_item_matrix.index,
            columns=self.user_item_matrix.columns
        )

        print(f"‚úÖ SVD model trained with {self.svd_model.n_components} factors")
        print(f"‚úÖ Explained variance ratio: {self.svd_model.explained_variance_ratio_.sum():.3f}")

    def content_based_filtering(self):
        """Implement content-based filtering"""

        print("\\nüìù Content-Based Filtering")
        print("-" * 40)

        # Create content features
        content_features = []

        for _, product in self.products_df.iterrows():
            # Combine textual features
            content = f"{product['category']} {product['brand']} {product['description']}"
            content_features.append(content)

        # TF-IDF vectorization
        tfidf = TfidfVectorizer(max_features=1000, stop_words='english')
        content_matrix = tfidf.fit_transform(content_features)

        # Compute content similarity
        content_similarity = cosine_similarity(content_matrix)
        self.content_similarity = pd.DataFrame(
            content_similarity,
            index=self.products_df['product_id'],
            columns=self.products_df['product_id']
        )

        print(f"‚úÖ Content similarity matrix: {self.content_similarity.shape}")
        print(f"‚úÖ TF-IDF features: {content_matrix.shape[1]}")

    def recommend_for_user(self, user_id, method='collaborative', n_recommendations=10):
        """Generate recommendations for a specific user"""

        if user_id not in self.user_item_matrix.index:
            print(f"‚ùå User {user_id} not found in dataset")
            return []

        user_ratings = self.user_item_matrix.loc[user_id]
        unrated_items = user_ratings[user_ratings == 0].index

        if method == 'collaborative_user':
            # User-based collaborative filtering
            user_sim = self.user_similarity.loc[user_id]
            similar_users = user_sim.sort_values(ascending=False)[1:51]  # Top 50 similar users

            recommendations = {}
            for item in unrated_items:
                weighted_sum = 0
                similarity_sum = 0

                for similar_user, similarity in similar_users.items():
                    if self.user_item_matrix.loc[similar_user, item] > 0:
                        weighted_sum += similarity * self.user_item_matrix.loc[similar_user, item]
                        similarity_sum += abs(similarity)

                if similarity_sum > 0:
                    recommendations[item] = weighted_sum / similarity_sum

        elif method == 'collaborative_item':
            # Item-based collaborative filtering
            recommendations = {}
            user_mean_rating = user_ratings[user_ratings > 0].mean()

            for item in unrated_items:
                item_sim = self.item_similarity.loc[item]
                rated_items = user_ratings[user_ratings > 0].index

                if len(rated_items) > 0:
                    relevant_similarities = item_sim[rated_items]
                    weighted_sum = sum(sim * user_ratings[rated_item]
                                     for rated_item, sim in relevant_similarities.items())
                    similarity_sum = sum(abs(sim) for sim in relevant_similarities.values())

                    if similarity_sum > 0:
                        recommendations[item] = weighted_sum / similarity_sum
                    else:
                        recommendations[item] = user_mean_rating

        elif method == 'matrix_factorization':
            # SVD-based recommendations
            user_idx = self.user_item_matrix.index.get_loc(user_id)
            predicted_ratings = self.predicted_ratings_svd.loc[user_id]
            recommendations = predicted_ratings[unrated_items].to_dict()

        elif method == 'content':
            # Content-based recommendations
            user_ratings_nonzero = user_ratings[user_ratings > 0]
            user_profile = {}

            for item, rating in user_ratings_nonzero.items():
                content_sim = self.content_similarity.loc[item]
                for similar_item, similarity in content_sim.items():
                    if similar_item in user_profile:
                        user_profile[similar_item] += rating * similarity
                    else:
                        user_profile[similar_item] = rating * similarity

            recommendations = {item: score for item, score in user_profile.items()
                             if item in unrated_items}

        # Sort and return top recommendations
        sorted_recommendations = sorted(recommendations.items(),
                                      key=lambda x: x[1], reverse=True)

        return sorted_recommendations[:n_recommendations]

    def evaluate_model(self, test_size=0.2):
        """Evaluate recommendation system performance"""

        print("\\nüìä Model Evaluation")
        print("-" * 40)

        # Split data for evaluation
        train_data, test_data = train_test_split(
            self.ratings_df, test_size=test_size, random_state=42
        )

        # Create train user-item matrix
        train_matrix = train_data.pivot_table(
            index='user_id', columns='product_id', values='rating', fill_value=0
        )

        # Retrain model on training data
        svd_train = TruncatedSVD(n_components=50, random_state=42)
        user_factors_train = svd_train.fit_transform(train_matrix)
        item_factors_train = svd_train.components_
        predicted_train = user_factors_train @ item_factors_train

        predicted_train_df = pd.DataFrame(
            predicted_train,
            index=train_matrix.index,
            columns=train_matrix.columns
        )

        # Evaluate on test data
        test_predictions = []
        test_actuals = []

        for _, row in test_data.iterrows():
            user_id, item_id, actual_rating = row['user_id'], row['product_id'], row['rating']

            if user_id in predicted_train_df.index and item_id in predicted_train_df.columns:
                predicted_rating = predicted_train_df.loc[user_id, item_id]
                test_predictions.append(predicted_rating)
                test_actuals.append(actual_rating)

        # Calculate metrics
        rmse = np.sqrt(mean_squared_error(test_actuals, test_predictions))
        mae = np.mean(np.abs(np.array(test_actuals) - np.array(test_predictions)))

        print(f"üéØ RMSE: {rmse:.4f}")
        print(f"üéØ MAE: {mae:.4f}")

        # Visualize predictions vs actuals
        plt.figure(figsize=(10, 6))
        plt.scatter(test_actuals[:1000], test_predictions[:1000], alpha=0.6)
        plt.plot([1, 5], [1, 5], 'r--', linewidth=2)
        plt.xlabel('Actual Ratings')
        plt.ylabel('Predicted Ratings')
        plt.title('Predicted vs Actual Ratings (SVD)')
        plt.grid(True, alpha=0.3)
        plt.show()

        return rmse, mae

def run_recommendation_project():
    """Run the complete recommendation system project"""

    print("üõí E-commerce Recommendation System Project")
    print("=" * 60)

    # Initialize system
    rec_system = RecommendationSystem()

    # Prepare data
    ratings_df, products_df = rec_system.prepare_data()

    # Exploratory analysis
    rec_system.exploratory_analysis()

    # Build different recommendation approaches
    rec_system.collaborative_filtering()
    rec_system.matrix_factorization()
    rec_system.content_based_filtering()

    # Generate sample recommendations
    print("\\nüéÅ Sample Recommendations")
    print("-" * 40)

    sample_user = ratings_df['user_id'].iloc[0]
    methods = [
        ('User-based CF', 'collaborative_user'),
        ('Item-based CF', 'collaborative_item'),
        ('Matrix Factorization', 'matrix_factorization'),
        ('Content-based', 'content')
    ]

    for method_name, method_code in methods:
        print(f"\\n{method_name} recommendations for user {sample_user}:")
        recommendations = rec_system.recommend_for_user(sample_user, method_code, 5)
        for i, (item_id, score) in enumerate(recommendations, 1):
            product_info = products_df[products_df['product_id'] == item_id].iloc[0]
            print(f"  {i}. Product {item_id} ({product_info['category']}) - Score: {score:.3f}")

    # Evaluate model
    rmse, mae = rec_system.evaluate_model()

    print("\\n‚úÖ Project Complete!")
    print(f"üìä Final Model Performance: RMSE = {rmse:.4f}, MAE = {mae:.4f}")

    return rec_system

# Run the recommendation system project
recommendation_system = run_recommendation_project()
```

## üñºÔ∏è Project 2: Computer Vision - Medical Image Classification

### Problem Statement
Develop a deep learning system to classify medical images (chest X-rays) to assist radiologists in detecting pneumonia and other lung conditions.

*Clinical Impact: AI can reduce diagnostic errors by 85% and speed up diagnosis by 150x*

```python
"""
Medical Image Classification System
=================================

This project demonstrates:
- Medical image preprocessing and augmentation
- Transfer learning with pre-trained CNNs
- Class imbalance handling
- Model interpretability (Grad-CAM)
- Clinical evaluation metrics
- Deployment considerations for healthcare

Dataset: Chest X-ray images (Pneumonia detection)
Clinical Metrics: Sensitivity, Specificity, PPV, NPV
Technical Metrics: AUC-ROC, Precision-Recall
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import cv2
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
from sklearn.model_selection import train_test_split
import seaborn as sns

class MedicalImageClassifier:
    """Medical image classification system with interpretability"""

    def __init__(self, img_height=224, img_width=224):
        self.img_height = img_height
        self.img_width = img_width
        self.model = None
        self.history = None

    def create_synthetic_medical_data(self, n_samples=1000):
        """Create synthetic medical image data for demonstration"""

        print("üè• Creating Synthetic Medical Image Dataset")
        print("-" * 50)

        np.random.seed(42)

        # Create synthetic chest X-ray like images
        images = []
        labels = []

        for i in range(n_samples):
            # Create base chest X-ray like image
            img = np.random.randn(self.img_height, self.img_width, 3) * 30 + 100

            # Add chest cavity structure
            center_x, center_y = self.img_width // 2, self.img_height // 2

            # Simulate lung areas (darker regions)
            lung_left = cv2.ellipse(img, (center_x - 40, center_y), (60, 80), 0, 0, 360, -20, -1)
            lung_right = cv2.ellipse(img, (center_x + 40, center_y), (60, 80), 0, 0, 360, -20, -1)

            # Simulate pneumonia (50% chance)
            if i < n_samples // 2:  # Pneumonia cases
                # Add pneumonia patterns (brighter patches)
                n_patches = np.random.randint(1, 4)
                for _ in range(n_patches):
                    patch_x = np.random.randint(50, self.img_width - 50)
                    patch_y = np.random.randint(50, self.img_height - 50)
                    patch_size = np.random.randint(20, 50)
                    cv2.circle(img, (patch_x, patch_y), patch_size, 40, -1)

                labels.append(1)  # Pneumonia
            else:  # Normal cases
                labels.append(0)  # Normal

            # Clip values and normalize
            img = np.clip(img, 0, 255).astype(np.uint8)
            images.append(img)

        images = np.array(images)
        labels = np.array(labels)

        print(f"‚úÖ Created {len(images)} synthetic medical images")
        print(f"üìä Normal cases: {np.sum(labels == 0)}")
        print(f"üìä Pneumonia cases: {np.sum(labels == 1)}")

        return images, labels

    def preprocess_images(self, images):
        """Preprocess medical images"""

        # Normalize pixel values
        images = images.astype('float32') / 255.0

        # Optional: Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)
        # Common preprocessing for medical images
        processed_images = []
        for img in images:
            # Convert to grayscale for CLAHE, then back to RGB
            gray = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
            enhanced = clahe.apply(gray)
            enhanced_rgb = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2RGB) / 255.0
            processed_images.append(enhanced_rgb)

        return np.array(processed_images)

    def create_data_augmentation(self):
        """Create data augmentation pipeline for medical images"""

        # Medical image augmentation should be conservative
        return keras.Sequential([
            layers.RandomRotation(0.1),  # Small rotations
            layers.RandomZoom(0.1),      # Small zoom
            layers.RandomFlip("horizontal"),  # Horizontal flip only
            layers.RandomBrightness(0.1),     # Small brightness changes
            layers.RandomContrast(0.1)        # Small contrast changes
        ])

    def build_model(self, base_model_name='ResNet50'):
        """Build transfer learning model for medical image classification"""

        print(f"üèóÔ∏è Building Medical Image Classifier ({base_model_name})")
        print("-" * 50)

        # Load pre-trained model
        if base_model_name == 'ResNet50':
            base_model = keras.applications.ResNet50(
                weights='imagenet',
                include_top=False,
                input_shape=(self.img_height, self.img_width, 3)
            )
        elif base_model_name == 'DenseNet121':
            base_model = keras.applications.DenseNet121(
                weights='imagenet',
                include_top=False,
                input_shape=(self.img_height, self.img_width, 3)
            )

        # Freeze base model layers
        base_model.trainable = False

        # Add custom classification head
        model = keras.Sequential([
            # Data augmentation
            self.create_data_augmentation(),

            # Pre-trained base
            base_model,

            # Custom classifier
            layers.GlobalAveragePooling2D(),
            layers.Dropout(0.3),
            layers.Dense(128, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.5),
            layers.Dense(1, activation='sigmoid', name='pneumonia_prediction')
        ])

        # Compile with appropriate metrics for medical classification
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=[
                'accuracy',
                keras.metrics.Precision(name='precision'),
                keras.metrics.Recall(name='recall'),
                keras.metrics.AUC(name='auc')
            ]
        )

        self.model = model

        print(f"‚úÖ Model built with {model.count_params():,} parameters")
        print(f"‚úÖ Trainable parameters: {sum([tf.size(var) for var in model.trainable_variables]):,}")

        return model

    def train_model(self, X_train, y_train, X_val, y_val, epochs=20):
        """Train the medical image classifier"""

        print("üéì Training Medical Image Classifier")
        print("-" * 50)

        # Callbacks for medical model training
        callbacks = [
            keras.callbacks.EarlyStopping(
                monitor='val_auc',
                patience=5,
                restore_best_weights=True,
                mode='max'
            ),
            keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=3,
                min_lr=1e-6
            ),
            keras.callbacks.ModelCheckpoint(
                'best_medical_model.h5',
                monitor='val_auc',
                save_best_only=True,
                mode='max'
            )
        ]

        # Handle class imbalance with class weights
        class_weights = {
            0: 1.0,  # Normal
            1: len(y_train[y_train == 0]) / len(y_train[y_train == 1])  # Pneumonia
        }

        print(f"‚öñÔ∏è Class weights: {class_weights}")

        # Train model
        self.history = self.model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=epochs,
            batch_size=32,
            callbacks=callbacks,
            class_weight=class_weights,
            verbose=1
        )

        return self.history

    def evaluate_clinical_metrics(self, X_test, y_test):
        """Evaluate model with clinical metrics"""

        print("\\nüè• Clinical Evaluation")
        print("-" * 40)

        # Get predictions
        y_pred_proba = self.model.predict(X_test)
        y_pred = (y_pred_proba > 0.5).astype(int).flatten()

        # Confusion matrix
        cm = confusion_matrix(y_test, y_pred)
        tn, fp, fn, tp = cm.ravel()

        # Clinical metrics
        sensitivity = tp / (tp + fn)  # Recall, True Positive Rate
        specificity = tn / (tn + fp)  # True Negative Rate
        ppv = tp / (tp + fp)          # Precision, Positive Predictive Value
        npv = tn / (tn + fn)          # Negative Predictive Value

        # Additional metrics
        accuracy = (tp + tn) / (tp + tn + fp + fn)
        f1_score = 2 * (ppv * sensitivity) / (ppv + sensitivity)

        # ROC curve
        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
        roc_auc = auc(fpr, tpr)

        # Print clinical report
        print(f"üìä Clinical Performance Metrics:")
        print(f"   Sensitivity (Recall): {sensitivity:.3f}")
        print(f"   Specificity:          {specificity:.3f}")
        print(f"   PPV (Precision):      {ppv:.3f}")
        print(f"   NPV:                  {npv:.3f}")
        print(f"   Accuracy:             {accuracy:.3f}")
        print(f"   F1-Score:             {f1_score:.3f}")
        print(f"   AUC-ROC:              {roc_auc:.3f}")

        # Visualizations
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('üè• Medical Image Classifier - Clinical Evaluation', fontsize=16, fontweight='bold')

        # 1. Confusion Matrix
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])
        axes[0, 0].set_title('Confusion Matrix')
        axes[0, 0].set_xlabel('Predicted')
        axes[0, 0].set_ylabel('Actual')
        axes[0, 0].set_xticklabels(['Normal', 'Pneumonia'])
        axes[0, 0].set_yticklabels(['Normal', 'Pneumonia'])

        # 2. ROC Curve
        axes[0, 1].plot(fpr, tpr, color='darkorange', linewidth=2,
                       label=f'ROC Curve (AUC = {roc_auc:.3f})')
        axes[0, 1].plot([0, 1], [0, 1], color='navy', linewidth=2, linestyle='--')
        axes[0, 1].set_xlim([0.0, 1.0])
        axes[0, 1].set_ylim([0.0, 1.05])
        axes[0, 1].set_xlabel('False Positive Rate (1 - Specificity)')
        axes[0, 1].set_ylabel('True Positive Rate (Sensitivity)')
        axes[0, 1].set_title('ROC Curve')
        axes[0, 1].legend(loc="lower right")
        axes[0, 1].grid(True, alpha=0.3)

        # 3. Training History
        if self.history:
            epochs = range(1, len(self.history.history['loss']) + 1)
            axes[1, 0].plot(epochs, self.history.history['loss'], 'bo-', label='Training Loss')
            axes[1, 0].plot(epochs, self.history.history['val_loss'], 'ro-', label='Validation Loss')
            axes[1, 0].set_title('Model Loss')
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('Loss')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)

        # 4. Clinical Metrics Bar Chart
        metrics = ['Sensitivity', 'Specificity', 'PPV', 'NPV', 'Accuracy', 'F1-Score']
        values = [sensitivity, specificity, ppv, npv, accuracy, f1_score]
        colors = ['lightcoral', 'lightblue', 'lightgreen', 'lightyellow', 'lightpink', 'lightgray']

        bars = axes[1, 1].bar(metrics, values, color=colors, alpha=0.8)
        axes[1, 1].set_title('Clinical Metrics Summary')
        axes[1, 1].set_ylabel('Score')
        axes[1, 1].set_ylim(0, 1)

        # Add value labels on bars
        for bar, value in zip(bars, values):
            axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                           f'{value:.3f}', ha='center', va='bottom')

        plt.setp(axes[1, 1].xaxis.get_majorticklabels(), rotation=45)

        plt.tight_layout()
        plt.show()

        return {
            'sensitivity': sensitivity,
            'specificity': specificity,
            'ppv': ppv,
            'npv': npv,
            'accuracy': accuracy,
            'f1_score': f1_score,
            'auc_roc': roc_auc
        }

def run_medical_image_project():
    """Run the complete medical image classification project"""

    print("üè• Medical Image Classification Project")
    print("=" * 60)

    # Initialize classifier
    classifier = MedicalImageClassifier()

    # Create synthetic medical data
    images, labels = classifier.create_synthetic_medical_data(n_samples=1000)

    # Preprocess images
    processed_images = classifier.preprocess_images(images)

    # Split data
    X_train, X_temp, y_train, y_temp = train_test_split(
        processed_images, labels, test_size=0.4, random_state=42, stratify=labels
    )
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
    )

    print(f"üìä Data Split:")
    print(f"   Training: {len(X_train)} samples")
    print(f"   Validation: {len(X_val)} samples")
    print(f"   Testing: {len(X_test)} samples")

    # Build and train model
    model = classifier.build_model('ResNet50')
    history = classifier.train_model(X_train, y_train, X_val, y_val, epochs=10)

    # Evaluate with clinical metrics
    clinical_metrics = classifier.evaluate_clinical_metrics(X_test, y_test)

    print("\\n‚úÖ Medical Image Classification Project Complete!")
    print(f"üéØ Key Results:")
    print(f"   Sensitivity: {clinical_metrics['sensitivity']:.3f}")
    print(f"   Specificity: {clinical_metrics['specificity']:.3f}")
    print(f"   AUC-ROC: {clinical_metrics['auc_roc']:.3f}")

    return classifier, clinical_metrics

# Run the medical image classification project
medical_classifier, metrics = run_medical_image_project()
```

## üìà Project 3: Time Series Forecasting - Financial Market Prediction

### Problem Statement
Build an advanced time series forecasting system to predict stock prices and market volatility using multiple data sources and sophisticated models.

*Financial Impact: Accurate forecasting can improve portfolio returns by 15-25% and reduce risk by 30%*

```python
"""
Financial Time Series Forecasting System
=======================================

This project demonstrates:
- Multi-variate time series analysis
- Feature engineering from financial data
- LSTM/GRU for sequence modeling
- ARIMA and seasonal decomposition
- Technical indicators integration
- Risk metrics and portfolio optimization
- Real-time prediction pipeline

Dataset: Stock prices, economic indicators, news sentiment
Financial Metrics: Sharpe Ratio, Maximum Drawdown, Alpha, Beta
Technical Metrics: RMSE, MAPE, Directional Accuracy
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import warnings
warnings.filterwarnings('ignore')

class FinancialTimeSeriesForecaster:
    """Advanced financial time series forecasting system"""

    def __init__(self, sequence_length=60):
        self.sequence_length = sequence_length
        self.scalers = {}
        self.models = {}
        self.features = []

    def generate_synthetic_financial_data(self, n_days=2000):
        """Generate realistic synthetic financial data"""

        print("üí∞ Generating Synthetic Financial Data")
        print("-" * 50)

        np.random.seed(42)

        # Generate date range
        dates = pd.date_range(start='2018-01-01', periods=n_days, freq='D')

        # Base stock price using geometric Brownian motion
        initial_price = 100
        returns = np.random.normal(0.0005, 0.02, n_days)  # Daily returns
        prices = [initial_price]

        for i in range(1, n_days):
            price = prices[-1] * (1 + returns[i])
            prices.append(price)

        # Add trend and seasonality
        trend = np.linspace(0, 0.5, n_days)
        seasonal = 0.1 * np.sin(2 * np.pi * np.arange(n_days) / 252)  # Annual cycle
        prices = np.array(prices) * (1 + trend + seasonal)

        # Generate additional features
        volume = np.random.lognormal(15, 0.5, n_days)
        high_prices = prices * (1 + np.random.uniform(0, 0.05, n_days))
        low_prices = prices * (1 - np.random.uniform(0, 0.05, n_days))

        # Market indicators
        sp500_returns = np.random.normal(0.0003, 0.015, n_days)
        vix = 15 + 10 * np.random.gamma(2, 1, n_days)  # Volatility index
        interest_rate = 2 + 3 * np.sin(2 * np.pi * np.arange(n_days) / 1000) + np.random.normal(0, 0.1, n_days)

        # News sentiment (simplified)
        sentiment = np.random.beta(5, 5, n_days)  # Values between 0 and 1

        # Create DataFrame
        self.data = pd.DataFrame({
            'date': dates,
            'close': prices,
            'high': high_prices,
            'low': low_prices,
            'volume': volume,
            'sp500_return': sp500_returns,
            'vix': vix,
            'interest_rate': interest_rate,
            'sentiment': sentiment
        })

        print(f"‚úÖ Generated {len(self.data)} days of financial data")
        print(f"üìä Date range: {self.data['date'].min()} to {self.data['date'].max()}")

        return self.data

    def create_technical_indicators(self):
        """Create comprehensive technical indicators"""

        print("üìä Creating Technical Indicators")
        print("-" * 40)

        df = self.data.copy()

        # Price-based indicators
        df['returns'] = df['close'].pct_change()
        df['log_returns'] = np.log(df['close'] / df['close'].shift(1))

        # Moving averages
        for window in [5, 10, 20, 50]:
            df[f'ma_{window}'] = df['close'].rolling(window=window).mean()
            df[f'price_ma_{window}_ratio'] = df['close'] / df[f'ma_{window}']

        # Exponential moving averages
        for span in [12, 26]:
            df[f'ema_{span}'] = df['close'].ewm(span=span).mean()

        # MACD
        df['macd'] = df['ema_12'] - df['ema_26']
        df['macd_signal'] = df['macd'].ewm(span=9).mean()
        df['macd_histogram'] = df['macd'] - df['macd_signal']

        # RSI
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df['rsi'] = 100 - (100 / (1 + rs))

        # Bollinger Bands
        df['bb_middle'] = df['close'].rolling(window=20).mean()
        bb_std = df['close'].rolling(window=20).std()
        df['bb_upper'] = df['bb_middle'] + (bb_std * 2)
        df['bb_lower'] = df['bb_middle'] - (bb_std * 2)
        df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])

        # Volatility indicators
        df['volatility_10'] = df['returns'].rolling(window=10).std()
        df['volatility_30'] = df['returns'].rolling(window=30).std()

        # Volume indicators
        df['volume_ma_10'] = df['volume'].rolling(window=10).mean()
        df['volume_ratio'] = df['volume'] / df['volume_ma_10']

        # Price patterns
        df['high_low_ratio'] = df['high'] / df['low']
        df['close_position'] = (df['close'] - df['low']) / (df['high'] - df['low'])

        # Lag features
        for lag in [1, 2, 3, 5]:
            df[f'returns_lag_{lag}'] = df['returns'].shift(lag)
            df[f'volume_lag_{lag}'] = df['volume'].shift(lag)

        self.data = df

        # Define feature columns (excluding target and non-predictive features)
        exclude_cols = ['date', 'close', 'high', 'low', 'returns', 'log_returns']
        self.features = [col for col in df.columns if col not in exclude_cols]

        print(f"‚úÖ Created {len(self.features)} technical indicators")

        return df

    def prepare_sequences(self, target_col='close', test_size=0.2):
        """Prepare sequences for time series modeling"""

        print("üîÑ Preparing Time Series Sequences")
        print("-" * 40)

        # Remove NaN values
        df_clean = self.data.dropna()

        # Scale features
        feature_data = df_clean[self.features].values
        target_data = df_clean[target_col].values.reshape(-1, 1)

        self.scalers['features'] = MinMaxScaler()
        self.scalers['target'] = MinMaxScaler()

        scaled_features = self.scalers['features'].fit_transform(feature_data)
        scaled_target = self.scalers['target'].fit_transform(target_data)

        # Create sequences
        X, y = [], []

        for i in range(self.sequence_length, len(scaled_features)):
            X.append(scaled_features[i-self.sequence_length:i])
            y.append(scaled_target[i])

        X = np.array(X)
        y = np.array(y)

        # Split data temporally (important for time series)
        split_idx = int(len(X) * (1 - test_size))

        self.X_train = X[:split_idx]
        self.X_test = X[split_idx:]
        self.y_train = y[:split_idx]
        self.y_test = y[split_idx:]

        print(f"‚úÖ Created sequences:")
        print(f"   Training: {self.X_train.shape}")
        print(f"   Testing: {self.X_test.shape}")
        print(f"   Features per timestep: {X.shape[2]}")

        return self.X_train, self.X_test, self.y_train, self.y_test

    def build_lstm_model(self, lstm_units=[50, 50], dropout_rate=0.2):
        """Build LSTM model for financial forecasting"""

        print("üß† Building LSTM Model")
        print("-" * 30)

        model = keras.Sequential()

        # First LSTM layer
        model.add(layers.LSTM(
            lstm_units[0],
            return_sequences=True,
            input_shape=(self.sequence_length, len(self.features))
        ))
        model.add(layers.Dropout(dropout_rate))

        # Additional LSTM layers
        for units in lstm_units[1:]:
            model.add(layers.LSTM(units, return_sequences=True))
            model.add(layers.Dropout(dropout_rate))

        # Final LSTM layer
        model.add(layers.LSTM(25))
        model.add(layers.Dropout(dropout_rate))

        # Dense layers
        model.add(layers.Dense(25, activation='relu'))
        model.add(layers.Dense(1))

        # Compile model
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.001),
            loss='mse',
            metrics=['mae']
        )

        self.models['lstm'] = model

        print(f"‚úÖ LSTM model built with {model.count_params():,} parameters")

        return model

    def train_models(self, epochs=50, batch_size=32):
        """Train the forecasting models"""

        print("üéì Training Financial Forecasting Models")
        print("-" * 50)

        # Callbacks
        callbacks = [
            keras.callbacks.EarlyStopping(
                monitor='val_loss',
                patience=10,
                restore_best_weights=True
            ),
            keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=5,
                min_lr=1e-6
            )
        ]

        # Train LSTM
        print("Training LSTM...")
        lstm_history = self.models['lstm'].fit(
            self.X_train, self.y_train,
            validation_split=0.2,
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )

        self.lstm_history = lstm_history

        return lstm_history

    def evaluate_predictions(self):
        """Comprehensive evaluation of forecasting performance"""

        print("\\nüìä Evaluating Forecasting Performance")
        print("-" * 50)

        # Generate predictions
        lstm_pred_scaled = self.models['lstm'].predict(self.X_test)

        # Inverse transform predictions
        lstm_pred = self.scalers['target'].inverse_transform(lstm_pred_scaled)
        y_true = self.scalers['target'].inverse_transform(self.y_test)

        # Calculate metrics
        lstm_rmse = np.sqrt(mean_squared_error(y_true, lstm_pred))
        lstm_mae = mean_absolute_error(y_true, lstm_pred)
        lstm_mape = np.mean(np.abs((y_true - lstm_pred) / y_true)) * 100

        # Directional accuracy
        true_direction = np.sign(np.diff(y_true.flatten()))
        pred_direction = np.sign(np.diff(lstm_pred.flatten()))
        directional_accuracy = np.mean(true_direction == pred_direction)

        print(f"üìà LSTM Performance:")
        print(f"   RMSE: ${lstm_rmse:.2f}")
        print(f"   MAE: ${lstm_mae:.2f}")
        print(f"   MAPE: {lstm_mape:.2f}%")
        print(f"   Directional Accuracy: {directional_accuracy:.2%}")

        # Financial metrics
        returns_true = np.diff(y_true.flatten()) / y_true[:-1].flatten()
        returns_pred = np.diff(lstm_pred.flatten()) / y_true[:-1].flatten()  # Use true prices for realistic returns

        # Sharpe ratio (assuming risk-free rate of 2%)
        sharpe_true = (np.mean(returns_true) * 252 - 0.02) / (np.std(returns_true) * np.sqrt(252))
        sharpe_pred = (np.mean(returns_pred) * 252 - 0.02) / (np.std(returns_pred) * np.sqrt(252))

        print(f"\\nüí∞ Financial Metrics:")
        print(f"   True Returns Sharpe Ratio: {sharpe_true:.3f}")
        print(f"   Predicted Returns Sharpe Ratio: {sharpe_pred:.3f}")

        # Visualizations
        self.create_forecasting_visualizations(y_true, lstm_pred, returns_true, returns_pred)

        return {
            'rmse': lstm_rmse,
            'mae': lstm_mae,
            'mape': lstm_mape,
            'directional_accuracy': directional_accuracy,
            'sharpe_true': sharpe_true,
            'sharpe_pred': sharpe_pred
        }

    def create_forecasting_visualizations(self, y_true, y_pred, returns_true, returns_pred):
        """Create comprehensive visualizations for forecasting results"""

        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('üí∞ Financial Time Series Forecasting Results', fontsize=16, fontweight='bold')

        # 1. Price predictions
        test_days = range(len(y_true))
        axes[0, 0].plot(test_days, y_true, label='Actual Prices', linewidth=2, alpha=0.8)
        axes[0, 0].plot(test_days, y_pred, label='LSTM Predictions', linewidth=2, alpha=0.8)
        axes[0, 0].set_title('Stock Price Predictions')
        axes[0, 0].set_xlabel('Days')
        axes[0, 0].set_ylabel('Price ($)')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)

        # 2. Prediction errors
        errors = y_true.flatten() - y_pred.flatten()
        axes[0, 1].hist(errors, bins=50, alpha=0.7, edgecolor='black')
        axes[0, 1].set_title('Prediction Error Distribution')
        axes[0, 1].set_xlabel('Error ($)')
        axes[0, 1].set_ylabel('Frequency')
        axes[0, 1].axvline(0, color='red', linestyle='--', linewidth=2)
        axes[0, 1].grid(True, alpha=0.3)

        # 3. Scatter plot: Actual vs Predicted
        axes[0, 2].scatter(y_true, y_pred, alpha=0.6)
        axes[0, 2].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', linewidth=2)
        axes[0, 2].set_title('Actual vs Predicted Prices')
        axes[0, 2].set_xlabel('Actual Price ($)')
        axes[0, 2].set_ylabel('Predicted Price ($)')
        axes[0, 2].grid(True, alpha=0.3)

        # 4. Returns comparison
        axes[1, 0].plot(returns_true, label='Actual Returns', alpha=0.7)
        axes[1, 0].plot(returns_pred, label='Predicted Returns', alpha=0.7)
        axes[1, 0].set_title('Daily Returns Comparison')
        axes[1, 0].set_xlabel('Days')
        axes[1, 0].set_ylabel('Returns')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)

        # 5. Training history
        if hasattr(self, 'lstm_history'):
            epochs = range(1, len(self.lstm_history.history['loss']) + 1)
            axes[1, 1].plot(epochs, self.lstm_history.history['loss'], label='Training Loss')
            axes[1, 1].plot(epochs, self.lstm_history.history['val_loss'], label='Validation Loss')
            axes[1, 1].set_title('Model Training History')
            axes[1, 1].set_xlabel('Epoch')
            axes[1, 1].set_ylabel('Loss')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)
            axes[1, 1].set_yscale('log')

        # 6. Cumulative returns
        cum_returns_true = np.cumprod(1 + returns_true)
        cum_returns_pred = np.cumprod(1 + returns_pred)

        axes[1, 2].plot(cum_returns_true, label='Actual Strategy', linewidth=2)
        axes[1, 2].plot(cum_returns_pred, label='Predicted Strategy', linewidth=2)
        axes[1, 2].set_title('Cumulative Returns')
        axes[1, 2].set_xlabel('Days')
        axes[1, 2].set_ylabel('Cumulative Return')
        axes[1, 2].legend()
        axes[1, 2].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

def run_financial_forecasting_project():
    """Run the complete financial forecasting project"""

    print("üí∞ Financial Time Series Forecasting Project")
    print("=" * 60)

    # Initialize forecaster
    forecaster = FinancialTimeSeriesForecaster(sequence_length=60)

    # Generate financial data
    data = forecaster.generate_synthetic_financial_data(n_days=2000)

    # Create technical indicators
    enhanced_data = forecaster.create_technical_indicators()

    # Prepare sequences
    X_train, X_test, y_train, y_test = forecaster.prepare_sequences()

    # Build and train model
    lstm_model = forecaster.build_lstm_model()
    training_history = forecaster.train_models(epochs=30)

    # Evaluate performance
    metrics = forecaster.evaluate_predictions()

    print("\\n‚úÖ Financial Forecasting Project Complete!")
    print(f"üéØ Key Results:")
    print(f"   RMSE: ${metrics['rmse']:.2f}")
    print(f"   Directional Accuracy: {metrics['directional_accuracy']:.2%}")
    print(f"   Sharpe Ratio: {metrics['sharpe_pred']:.3f}")

    return forecaster, metrics

# Run the financial forecasting project
financial_forecaster, forecast_metrics = run_financial_forecasting_project()

print("\\nüéâ All Capstone Projects Complete!")
print("=" * 60)
print("üèÜ You have successfully implemented:")
print("   ‚úÖ E-commerce Recommendation System")
print("   ‚úÖ Medical Image Classification")
print("   ‚úÖ Financial Time Series Forecasting")
print("\\nüíº These projects demonstrate your ability to:")
print("   ‚Ä¢ Apply ML to diverse real-world problems")
print("   ‚Ä¢ Handle different data types (tabular, images, time series)")
print("   ‚Ä¢ Use appropriate algorithms for each domain")
print("   ‚Ä¢ Evaluate models with domain-specific metrics")
print("   ‚Ä¢ Build end-to-end ML pipelines")
```

## üöÄ Deployment and Production Considerations

### Project 4: MLOps Pipeline Setup

```python
"""
MLOps Pipeline for Production ML Systems
=======================================

This section covers:
- Model versioning and experiment tracking
- Automated testing and validation
- Containerization with Docker
- API development with FastAPI
- Monitoring and logging
- CI/CD pipelines
"""

# Example API deployment structure
from fastapi import FastAPI
from pydantic import BaseModel
import joblib
import numpy as np

app = FastAPI(title="ML Model API", version="1.0.0")

class PredictionRequest(BaseModel):
    features: list

class PredictionResponse(BaseModel):
    prediction: float
    confidence: float

# Load model (in production, use model registry)
model = None  # Load your trained model

@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    """Make prediction using deployed model"""

    # Preprocess input
    features = np.array(request.features).reshape(1, -1)

    # Make prediction
    prediction = model.predict(features)[0]

    # Calculate confidence (if available)
    confidence = 0.95  # Placeholder

    return PredictionResponse(
        prediction=float(prediction),
        confidence=confidence
    )

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "model_loaded": model is not None}
```

## üìö Sources & Project References

### üìñ **Real-World ML Applications**:

1. **"Building Machine Learning Powered Applications"** - Emmanuel Ameisen
   - **üí° Focus**: End-to-end ML project lifecycle

2. **"Machine Learning Design Patterns"** - Lakshmanan, Robinson, Munn
   - **üí° Focus**: Production ML patterns and best practices

3. **"Hands-On Machine Learning"** - Aur√©lien G√©ron
   - **üí° Focus**: Practical implementation with scikit-learn and TensorFlow

### üè≠ **Industry Case Studies**:
- **Netflix**: Recommendation systems at scale
- **Uber**: Real-time demand forecasting
- **Tesla**: Computer vision for autonomous driving
- **Google**: Search ranking and ad optimization

## ‚úÖ Portfolio Development

### Project Checklist:
- [ ] **Problem Definition**: Clear business value
- [ ] **Data Pipeline**: Robust data processing
- [ ] **Model Development**: Multiple approaches compared
- [ ] **Evaluation**: Domain-appropriate metrics
- [ ] **Documentation**: Clear README and notebooks
- [ ] **Code Quality**: Clean, tested, reproducible
- [ ] **Deployment**: Production-ready API or app

### GitHub Portfolio Structure:
```
your-ml-portfolio/
‚îú‚îÄ‚îÄ project-1-recommendation-system/
‚îú‚îÄ‚îÄ project-2-medical-imaging/
‚îú‚îÄ‚îÄ project-3-financial-forecasting/
‚îú‚îÄ‚îÄ project-4-nlp-sentiment/
‚îî‚îÄ‚îÄ README.md (portfolio overview)
```

## üöÄ Next Steps

Congratulations! You've completed comprehensive capstone projects that demonstrate real-world ML expertise. Consider:

1. **Deploy one project** to cloud platforms (AWS, GCP, Azure)
2. **Write blog posts** about your projects and learnings
3. **Contribute to open source** ML projects
4. **Apply to ML roles** with confidence in your abilities

Continue learning with [Module 10: Resources](../10_Resources/README.md) for ongoing development!

---
*Estimated completion time: 20-30 hours*
*Prerequisites: Completion of Modules 01-07*