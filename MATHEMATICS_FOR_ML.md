# üìê Mathematics for Machine Learning

## Complete Mathematical Foundation for ML/AI

This comprehensive guide covers all essential mathematics needed to deeply understand and implement machine learning algorithms.

---

## üìã Table of Contents

1. [Linear Algebra](#linear-algebra)
2. [Calculus](#calculus)
3. [Probability Theory](#probability-theory)
4. [Statistics](#statistics)
5. [Optimization](#optimization)
6. [Information Theory](#information-theory)

---

## üî¢ Linear Algebra

Linear algebra is the foundation of machine learning. Neural networks, dimensionality reduction, and most ML algorithms rely heavily on linear algebra operations.

### Vectors and Matrices

**Vectors** (1D arrays):
```
v = [1, 2, 3]

Geometric interpretation:
- Direction and magnitude in space
- Point in n-dimensional space

ML applications:
- Feature vector: [age, income, credit_score]
- Word embedding: 300-dimensional vector representing a word
- Model parameters: weights and biases
```

**Matrices** (2D arrays):
```
A = [[1, 2, 3],
     [4, 5, 6],
     [7, 8, 9]]

Shape: 3√ó3 (rows √ó columns)

ML applications:
- Dataset: Each row is a sample, each column is a feature
- Weight matrix in neural network layer
- Covariance matrix
- Transformation matrix
```

### Key Operations

**Dot Product** (scalar result):
```python
import numpy as np

a = np.array([1, 2, 3])
b = np.array([4, 5, 6])

dot_product = np.dot(a, b)
# = 1√ó4 + 2√ó5 + 3√ó6 = 4 + 10 + 18 = 32

# Alternative notation
dot_product = a @ b
dot_product = np.sum(a * b)

# Geometric meaning:
# dot(a, b) = |a| √ó |b| √ó cos(Œ∏)
# Measures similarity between vectors

# ML application: Similarity between embeddings
similarity = word_embedding_1 @ word_embedding_2
```

**Matrix Multiplication**:
```python
# A (m√ón) √ó B (n√óp) = C (m√óp)

A = np.array([[1, 2],
              [3, 4],
              [5, 6]])  # 3√ó2

B = np.array([[7, 8, 9],
              [10, 11, 12]])  # 2√ó3

C = A @ B  # 3√ó3

# C[i,j] = sum of (row i of A) √ó (column j of B)
# C[0,0] = 1√ó7 + 2√ó10 = 27
# C[0,1] = 1√ó8 + 2√ó11 = 30
# C[0,2] = 1√ó9 + 2√ó12 = 33

# ML application: Forward pass in neural network
# X (batch_size, input_dim) @ W (input_dim, hidden_dim) = H (batch_size, hidden_dim)
```

**Matrix Transpose**:
```python
A = np.array([[1, 2, 3],
              [4, 5, 6]])  # 2√ó3

A_T = A.T  # 3√ó2
# [[1, 4],
#  [2, 5],
#  [3, 6]]

# Properties:
# (A^T)^T = A
# (AB)^T = B^T A^T

# ML application: Backpropagation
# dL/dW = X^T @ dL/dY
```

### Advanced Concepts

**Eigenvalues and Eigenvectors**:
```python
# Definition: Av = Œªv
# v is eigenvector, Œª is eigenvalue

A = np.array([[4, 2],
              [1, 3]])

eigenvalues, eigenvectors = np.linalg.eig(A)

# eigenvalues: [5, 2]
# eigenvectors: [[0.89, -0.71],
#                [0.45,  0.71]]

# Meaning: v is a direction that only gets scaled (not rotated) by A

# ML applications:
# 1. Principal Component Analysis (PCA)
#    - Find directions of maximum variance
#    - Eigenvectors of covariance matrix

# 2. PageRank algorithm
#    - Dominant eigenvector of web graph

# 3. Spectral clustering
#    - Eigenvectors of graph Laplacian
```

**Singular Value Decomposition (SVD)**:
```python
# Any matrix A (m√ón) can be decomposed:
# A = U Œ£ V^T

A = np.random.rand(4, 3)
U, S, VT = np.linalg.svd(A, full_matrices=False)

# U (4√ó3): Left singular vectors (orthonormal)
# S (3,): Singular values (diagonal of Œ£)
# VT (3√ó3): Right singular vectors (orthonormal)

# Reconstruction:
A_reconstructed = U @ np.diag(S) @ VT
# A_reconstructed ‚âà A (within numerical precision)

# ML applications:

# 1. Low-rank approximation
def low_rank_approximation(A, k):
    """Keep only top k singular values"""
    U, S, VT = np.linalg.svd(A, full_matrices=False)
    return U[:, :k] @ np.diag(S[:k]) @ VT[:k, :]

# 2. PCA implementation
def pca_via_svd(X, n_components):
    """PCA using SVD"""
    # Center data
    X_centered = X - X.mean(axis=0)

    # SVD
    U, S, VT = np.linalg.svd(X_centered, full_matrices=False)

    # Principal components
    components = VT[:n_components]

    # Transformed data
    X_transformed = X_centered @ components.T

    return X_transformed, components

# 3. Recommender systems (Matrix Factorization)
# User-item matrix ‚âà U @ Œ£ @ V^T
# Latent factors representation
```

**Matrix Norms**:
```python
A = np.array([[1, 2],
              [3, 4]])

# Frobenius norm (most common)
frobenius_norm = np.linalg.norm(A, 'fro')
# = sqrt(1¬≤ + 2¬≤ + 3¬≤ + 4¬≤) = sqrt(30) ‚âà 5.48

# L1 norm (maximum absolute column sum)
l1_norm = np.linalg.norm(A, 1)  # 6

# L2 norm (largest singular value)
l2_norm = np.linalg.norm(A, 2)  # ‚âà 5.46

# ML application: Regularization
# L2 regularization: minimize ||w||¬≤
# Frobenius norm for weight decay: minimize ||W||_F¬≤
```

### Practical ML Examples

**Linear Regression (Matrix Form)**:
```python
# y = Xw + noise
# Closed-form solution: w = (X^T X)^(-1) X^T y

def linear_regression_normal_equation(X, y):
    """
    Solve linear regression analytically

    X: (n_samples, n_features)
    y: (n_samples,)
    """
    # Add intercept term (column of ones)
    X_with_intercept = np.column_stack([np.ones(len(X)), X])

    # Normal equation
    XTX = X_with_intercept.T @ X_with_intercept
    XTy = X_with_intercept.T @ y

    # Solve: w = (X^T X)^(-1) X^T y
    w = np.linalg.solve(XTX, XTy)

    return w

# Example
X = np.random.rand(100, 5)
y = X @ np.array([1, 2, 3, 4, 5]) + np.random.randn(100) * 0.1

weights = linear_regression_normal_equation(X, y)
print(f"Learned weights: {weights}")
# Should be close to [1, 2, 3, 4, 5]
```

**PCA (Dimensionality Reduction)**:
```python
def pca(X, n_components=2):
    """
    Principal Component Analysis

    Steps:
    1. Center data
    2. Compute covariance matrix
    3. Find eigenvectors (principal components)
    4. Project data onto top components
    """
    # 1. Center data (mean = 0)
    X_centered = X - np.mean(X, axis=0)

    # 2. Covariance matrix
    # Cov = (1/n) X^T X
    n_samples = X_centered.shape[0]
    cov_matrix = (X_centered.T @ X_centered) / n_samples

    # 3. Eigendecomposition
    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)

    # Sort by eigenvalue (descending)
    idx = eigenvalues.argsort()[::-1]
    eigenvalues = eigenvalues[idx]
    eigenvectors = eigenvectors[:, idx]

    # 4. Select top k eigenvectors
    principal_components = eigenvectors[:, :n_components]

    # 5. Project data
    X_transformed = X_centered @ principal_components

    # Explained variance
    explained_variance_ratio = eigenvalues[:n_components] / eigenvalues.sum()

    return X_transformed, principal_components, explained_variance_ratio

# Example: Compress 784-dimensional MNIST to 2D
from sklearn.datasets import load_digits
digits = load_digits()
X, y = digits.data, digits.target

X_2d, components, var_ratio = pca(X, n_components=2)

print(f"Explained variance: {var_ratio.sum():.2%}")
# Typically 20-30% for 2 components

import matplotlib.pyplot as plt
plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y, cmap='tab10')
plt.xlabel(f'PC1 ({var_ratio[0]:.1%})')
plt.ylabel(f'PC2 ({var_ratio[1]:.1%})')
plt.title('MNIST Digits in 2D (PCA)')
plt.colorbar()
plt.show()
```

**Neural Network Forward Pass**:
```python
def neural_network_forward(X, weights, biases):
    """
    2-layer neural network using matrix operations

    X: (batch_size, input_dim)
    weights: list of weight matrices
    biases: list of bias vectors
    """
    # Layer 1: X (n, d_in) @ W1 (d_in, d_hidden) = H (n, d_hidden)
    z1 = X @ weights[0] + biases[0]
    a1 = np.maximum(0, z1)  # ReLU activation

    # Layer 2: H (n, d_hidden) @ W2 (d_hidden, d_out) = Y (n, d_out)
    z2 = a1 @ weights[1] + biases[1]

    # Softmax for classification
    exp_scores = np.exp(z2 - np.max(z2, axis=1, keepdims=True))  # Numerical stability
    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)

    return probs

# Example: MNIST classifier
batch_size = 32
input_dim = 784  # 28√ó28 images
hidden_dim = 128
output_dim = 10  # digits 0-9

W1 = np.random.randn(input_dim, hidden_dim) * 0.01
b1 = np.zeros(hidden_dim)
W2 = np.random.randn(hidden_dim, output_dim) * 0.01
b2 = np.zeros(output_dim)

X_batch = np.random.randn(batch_size, input_dim)
predictions = neural_network_forward(X_batch, [W1, W2], [b1, b2])

print(f"Predictions shape: {predictions.shape}")  # (32, 10)
print(f"Sample prediction: {predictions[0]}")  # Probabilities sum to 1
```

---

## üìà Calculus

Calculus enables optimization - the core of training ML models. Understanding derivatives and gradients is essential for backpropagation.

### Derivatives

**Definition**: Rate of change
```
f'(x) = lim (h‚Üí0) [f(x+h) - f(x)] / h

Geometric meaning: Slope of tangent line
ML meaning: How much loss changes when we change a parameter
```

**Common Derivatives**:
```python
# Power rule: d/dx [x^n] = n √ó x^(n-1)
# d/dx [x¬≤] = 2x
# d/dx [x¬≥] = 3x¬≤

# Exponential: d/dx [e^x] = e^x

# Logarithm: d/dx [ln(x)] = 1/x

# Trigonometric:
# d/dx [sin(x)] = cos(x)
# d/dx [cos(x)] = -sin(x)

# Chain rule: d/dx [f(g(x))] = f'(g(x)) √ó g'(x)
# Example: d/dx [sin(x¬≤)] = cos(x¬≤) √ó 2x

# Product rule: d/dx [f(x)g(x)] = f'(x)g(x) + f(x)g'(x)

# Quotient rule: d/dx [f(x)/g(x)] = [f'(x)g(x) - f(x)g'(x)] / g(x)¬≤
```

**Numerical Differentiation** (for verification):
```python
def numerical_derivative(f, x, h=1e-5):
    """Compute derivative using finite differences"""
    return (f(x + h) - f(x - h)) / (2 * h)

# Example
f = lambda x: x**2
f_prime_numerical = numerical_derivative(f, x=3.0)
f_prime_analytical = 2 * 3.0

print(f"Numerical: {f_prime_numerical:.6f}")  # 6.000000
print(f"Analytical: {f_prime_analytical:.6f}")  # 6.000000
```

### Gradients (Multivariate Derivatives)

**Gradient**: Vector of partial derivatives
```
f(x, y) = x¬≤ + y¬≤

Gradient: ‚àáf = [‚àÇf/‚àÇx, ‚àÇf/‚àÇy] = [2x, 2y]

Meaning: Direction of steepest increase
ML meaning: Which direction to move parameters to increase loss
           (we move opposite direction to decrease loss!)
```

**Gradient Descent**:
```python
def gradient_descent(f, grad_f, x0, learning_rate=0.1, num_iterations=100,
                    tolerance=1e-6, verbose=False):
    """
    Minimize function f using gradient descent

    Parameters:
    -----------
    f: function to minimize
    grad_f: gradient of f
    x0: initial point (numpy array)
    learning_rate: step size Œ± (also called learning rate)
    tolerance: convergence threshold

    Convergence Conditions (for convex f with L-Lipschitz gradient):
    - If Œ± ‚â§ 1/L: Guaranteed convergence to global minimum
    - Convergence rate: O(1/k) where k is iteration number

    For strongly convex functions (with parameter Œº):
    - Convergence rate improves to O(exp(-k¬∑Œº/L))
    """
    x = x0.copy()
    history = [x.copy()]
    grad_norms = []

    for i in range(num_iterations):
        # Compute gradient
        grad = grad_f(x)
        grad_norm = np.linalg.norm(grad)
        grad_norms.append(grad_norm)

        # Check convergence
        if grad_norm < tolerance:
            if verbose:
                print(f"Converged at iteration {i}, gradient norm: {grad_norm:.2e}")
            break

        # Update: x^(k+1) = x^(k) - Œ±‚àáf(x^(k))
        x = x - learning_rate * grad

        history.append(x.copy())

        if verbose and i % 10 == 0:
            print(f"Iter {i}: f(x) = {f(x):.6f}, ||‚àáf|| = {grad_norm:.2e}")

    return x, np.array(history), np.array(grad_norms)

# Example: Minimize f(x,y) = x¬≤ + y¬≤
f = lambda x: x[0]**2 + x[1]**2
grad_f = lambda x: np.array([2*x[0], 2*x[1]])

x_min, history = gradient_descent(f, grad_f, x0=np.array([5.0, 5.0]))

print(f"Minimum found at: {x_min}")  # Should be near [0, 0]
print(f"Minimum value: {f(x_min):.6f}")  # Should be near 0

# Visualize optimization path
import matplotlib.pyplot as plt

x = np.linspace(-6, 6, 100)
y = np.linspace(-6, 6, 100)
X, Y = np.meshgrid(x, y)
Z = X**2 + Y**2

plt.contour(X, Y, Z, levels=20)
plt.plot(history[:, 0], history[:, 1], 'ro-', markersize=3)
plt.plot(history[0, 0], history[0, 1], 'go', markersize=10, label='Start')
plt.plot(history[-1, 0], history[-1, 1], 'r*', markersize=15, label='End')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Gradient Descent Optimization')
plt.legend()
plt.show()
```

### Backpropagation (Chain Rule in Action)

**Chain Rule for Neural Networks**:
```python
# Network: x ‚Üí f ‚Üí g ‚Üí h ‚Üí loss
# x ‚Üí f ‚Üí a
# a ‚Üí g ‚Üí b
# b ‚Üí h ‚Üí loss

# Chain rule:
# dL/dx = (dL/db) √ó (db/da) √ó (da/dx)

# Backpropagation computes these efficiently!

def backpropagation_example():
    """
    Simple 2-layer network with explicit backprop

    x ‚Üí W1 ‚Üí a ‚Üí ReLU ‚Üí h ‚Üí W2 ‚Üí y ‚Üí Loss
    """
    # Forward pass
    x = np.array([[1.0, 2.0]])  # (1, 2)
    W1 = np.array([[0.1, 0.2, 0.3],
                   [0.4, 0.5, 0.6]])  # (2, 3)
    W2 = np.array([[0.7],
                   [0.8],
                   [0.9]])  # (3, 1)
    target = np.array([[1.0]])

    # Layer 1
    a = x @ W1  # (1, 3)
    # a = [1√ó0.1 + 2√ó0.4, 1√ó0.2 + 2√ó0.5, 1√ó0.3 + 2√ó0.6]
    #   = [0.9, 1.2, 1.5]

    # ReLU
    h = np.maximum(0, a)  # (1, 3)
    # h = [0.9, 1.2, 1.5] (all positive, no change)

    # Layer 2
    y = h @ W2  # (1, 1)
    # y = [0.9√ó0.7 + 1.2√ó0.8 + 1.5√ó0.9]
    #   = [0.63 + 0.96 + 1.35] = [2.94]

    # Loss (MSE)
    loss = 0.5 * (y - target)**2
    # loss = 0.5 √ó (2.94 - 1.0)¬≤ = 0.5 √ó 3.76 = 1.88

    print(f"Forward pass:")
    print(f"  a = {a}")
    print(f"  h = {h}")
    print(f"  y = {y}")
    print(f"  loss = {loss}")

    # Backward pass (compute gradients)

    # dL/dy
    dL_dy = y - target  # (1, 1)
    # dL/dy = 2.94 - 1.0 = 1.94

    # dL/dW2 = h^T @ dL/dy
    dL_dW2 = h.T @ dL_dy  # (3, 1)
    # = [[0.9], [1.2], [1.5]] √ó 1.94
    # = [[1.746], [2.328], [2.910]]

    # dL/dh = dL/dy @ W2^T
    dL_dh = dL_dy @ W2.T  # (1, 3)
    # = 1.94 √ó [0.7, 0.8, 0.9]
    # = [1.358, 1.552, 1.746]

    # dL/da (through ReLU derivative)
    # ReLU'(x) = 1 if x > 0 else 0
    dL_da = dL_dh * (a > 0)  # (1, 3)
    # = [1.358, 1.552, 1.746] √ó [1, 1, 1]
    # = [1.358, 1.552, 1.746]

    # dL/dW1 = x^T @ dL/da
    dL_dW1 = x.T @ dL_da  # (2, 3)
    # = [[1], [2]] √ó [1.358, 1.552, 1.746]
    # = [[1.358, 1.552, 1.746],
    #    [2.716, 3.104, 3.492]]

    print(f"\nBackward pass (gradients):")
    print(f"  dL/dW2 =\n{dL_dW2}")
    print(f"  dL/dW1 =\n{dL_dW1}")

    # Gradient descent update
    learning_rate = 0.01
    W1_new = W1 - learning_rate * dL_dW1
    W2_new = W2 - learning_rate * dL_dW2

    print(f"\nUpdated weights:")
    print(f"  W1_new =\n{W1_new}")
    print(f"  W2_new =\n{W2_new}")

backpropagation_example()
```

**Automatic Differentiation** (PyTorch):
```python
import torch

# PyTorch computes gradients automatically!

# Forward pass
x = torch.tensor([[1.0, 2.0]], requires_grad=False)
W1 = torch.tensor([[0.1, 0.2, 0.3],
                   [0.4, 0.5, 0.6]], requires_grad=True)
W2 = torch.tensor([[0.7],
                   [0.8],
                   [0.9]], requires_grad=True)
target = torch.tensor([[1.0]])

a = x @ W1
h = torch.relu(a)
y = h @ W2
loss = 0.5 * (y - target)**2

# Backward pass (automatic!)
loss.backward()

print(f"Gradients (automatic):")
print(f"  W1.grad =\n{W1.grad}")
print(f"  W2.grad =\n{W2.grad}")

# Update weights
learning_rate = 0.01
with torch.no_grad():
    W1 -= learning_rate * W1.grad
    W2 -= learning_rate * W2.grad

    # Clear gradients for next iteration
    W1.grad.zero_()
    W2.grad.zero_()
```

### Important ML Derivatives

**Sigmoid**:
```python
# œÉ(x) = 1 / (1 + e^(-x))
# œÉ'(x) = œÉ(x) √ó (1 - œÉ(x))

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    s = sigmoid(x)
    return s * (1 - s)

# Efficient for backprop: Already have œÉ(x) from forward pass!
```

**Softmax**:
```python
# Softmax: S_i = e^(x_i) / Œ£ e^(x_j)

# Derivative w.r.t. x_i:
# ‚àÇS_i/‚àÇx_i = S_i √ó (1 - S_i)
# ‚àÇS_i/‚àÇx_j = -S_i √ó S_j  (i ‚â† j)

# Combined with Cross-Entropy loss:
# ‚àÇL/‚àÇx = S - y  (beautiful simplification!)
# where y is one-hot encoded target

def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))  # Numerical stability
    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)

# Cross-entropy loss
def cross_entropy(predictions, targets):
    return -np.sum(targets * np.log(predictions + 1e-8))

# Gradient (simplified!)
def softmax_cross_entropy_gradient(predictions, targets):
    return predictions - targets

# Example
logits = np.array([[2.0, 1.0, 0.1]])
target = np.array([[1, 0, 0]])  # First class

probs = softmax(logits)
loss = cross_entropy(probs, target)
grad = softmax_cross_entropy_gradient(probs, target)

print(f"Probabilities: {probs}")
print(f"Loss: {loss:.4f}")
print(f"Gradient: {grad}")
```

**ReLU**:
```python
# ReLU(x) = max(0, x)
# ReLU'(x) = 1 if x > 0 else 0

def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return (x > 0).astype(float)

# Dead ReLU problem: If x < 0 always, gradient is 0, neuron never updates
# Solutions: Leaky ReLU, PReLU, ELU
```

---

## üé≤ Probability Theory

Probability underpins machine learning - from Bayesian inference to generative models to uncertainty quantification.

### Fundamentals

**Probability Axioms**:
```
1. P(A) ‚â• 0  (non-negative)
2. P(Œ©) = 1  (total probability = 1)
3. P(A ‚à™ B) = P(A) + P(B) if A and B are disjoint

Sample space Œ©: All possible outcomes
Event A: Subset of Œ©
```

**Conditional Probability**:
```
P(A|B) = P(A ‚à© B) / P(B)

"Probability of A given B has occurred"

Example: Email spam detection
P(spam | contains "FREE") = P(spam ‚à© contains "FREE") / P(contains "FREE")
```

**Bayes' Theorem**:
```
P(A|B) = P(B|A) √ó P(A) / P(B)

Where:
- P(A|B): Posterior probability (what we want to compute)
- P(B|A): Likelihood (probability of observing B given A)
- P(A): Prior probability (initial belief about A)
- P(B): Evidence or marginal likelihood (normalization constant)

In ML terms:
P(Œ∏ | D) = P(D | Œ∏) √ó P(Œ∏) / P(D)

Posterior = Likelihood √ó Prior / Evidence

Where:
- Œ∏: Model parameters (hypothesis)
- D: Observed data
- P(Œ∏ | D): Posterior distribution over parameters given data
- P(D | Œ∏): Likelihood of data given parameters
- P(Œ∏): Prior distribution over parameters
- P(D) = ‚à´ P(D | Œ∏) P(Œ∏) dŒ∏: Marginal likelihood (often intractable)

This is the foundation of Bayesian machine learning!

Key Properties:
1. Law of Total Probability: P(B) = Œ£ P(B|A_i) P(A_i)
2. Bayes' Rule is exact, not an approximation
3. Prior √ó Likelihood = Unnormalized Posterior
4. Conjugate priors simplify posterior computation
```

**Example: Medical Diagnosis**:
```python
def bayes_theorem_example():
    """
    Disease testing with Bayes' Theorem

    Disease prevalence: 1% (prior)
    Test sensitivity: 95% (true positive rate)
    Test specificity: 90% (true negative rate)

    Question: If test is positive, what's probability of having disease?
    """
    # Prior
    P_disease = 0.01
    P_no_disease = 0.99

    # Likelihood
    P_positive_given_disease = 0.95  # Sensitivity
    P_positive_given_no_disease = 0.10  # 1 - Specificity

    # Total probability of positive test
    P_positive = (P_positive_given_disease * P_disease +
                  P_positive_given_no_disease * P_no_disease)

    # Bayes' theorem
    P_disease_given_positive = (P_positive_given_disease * P_disease) / P_positive

    print(f"Probability of disease given positive test: {P_disease_given_positive:.1%}")
    # Only 8.7%! (because disease is rare)

    # Intuition: Many false positives because disease is rare

bayes_theorem_example()
# Output: Probability of disease given positive test: 8.7%
```

### Random Variables

**Discrete Random Variable**:
```python
# Probability Mass Function (PMF)

# Example: Rolling a die
outcomes = [1, 2, 3, 4, 5, 6]
probabilities = [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]

# Expected value (mean)
E_X = sum(x * p for x, p in zip(outcomes, probabilities))
# E[X] = 1√ó(1/6) + 2√ó(1/6) + ... + 6√ó(1/6) = 3.5

# Variance
Var_X = sum((x - E_X)**2 * p for x, p in zip(outcomes, probabilities))
# Var[X] = (1-3.5)¬≤√ó(1/6) + ... + (6-3.5)¬≤√ó(1/6) ‚âà 2.92

print(f"Expected value: {E_X}")
print(f"Variance: {Var_X:.2f}")
print(f"Standard deviation: {np.sqrt(Var_X):.2f}")
```

**Continuous Random Variable**:
```python
# Probability Density Function (PDF)

# Normal distribution: N(Œº, œÉ¬≤)
def normal_pdf(x, mu=0, sigma=1):
    return (1 / (sigma * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mu) / sigma)**2)

# Visualize
x = np.linspace(-5, 5, 1000)
y = normal_pdf(x, mu=0, sigma=1)

plt.plot(x, y)
plt.xlabel('x')
plt.ylabel('Probability Density')
plt.title('Standard Normal Distribution N(0,1)')
plt.grid(True)
plt.show()

# Cumulative Distribution Function (CDF)
from scipy.stats import norm

# P(X ‚â§ x)
cdf_value = norm.cdf(1.96)  # P(X ‚â§ 1.96) ‚âà 0.975 (95%)
print(f"P(X ‚â§ 1.96) = {cdf_value:.4f}")

# Inverse CDF (quantile function)
quantile = norm.ppf(0.95)  # Value where 95% of data is below
print(f"95th percentile: {quantile:.4f}")  # ‚âà 1.645
```

### Common Distributions

**Bernoulli Distribution** (Single coin flip):
```python
# X ‚àà {0, 1}
# P(X=1) = p

p = 0.7  # Probability of heads

X = np.random.binomial(n=1, p=p, size=1000)  # 1000 flips

print(f"Proportion of heads: {X.mean():.3f}")  # ‚âà 0.700
# E[X] = p = 0.7
# Var[X] = p(1-p) = 0.7√ó0.3 = 0.21

# ML application: Binary classification output
```

**Binomial Distribution** (n coin flips):
```python
# Number of successes in n trials
# X ~ Binomial(n, p)

n = 10  # 10 flips
p = 0.5  # Fair coin

X = np.random.binomial(n=n, p=p, size=10000)

plt.hist(X, bins=range(12), density=True, alpha=0.7, edgecolor='black')
plt.xlabel('Number of heads')
plt.ylabel('Probability')
plt.title(f'Binomial Distribution (n={n}, p={p})')
plt.show()

# E[X] = np = 5
# Var[X] = np(1-p) = 2.5
```

**Normal (Gaussian) Distribution**:
```python
# X ~ N(Œº, œÉ¬≤)
# Most important distribution in ML!

mu, sigma = 0, 1
X = np.random.normal(mu, sigma, size=10000)

plt.hist(X, bins=50, density=True, alpha=0.7, edgecolor='black')
x = np.linspace(-4, 4, 100)
plt.plot(x, norm.pdf(x, mu, sigma), 'r-', linewidth=2, label='PDF')
plt.xlabel('x')
plt.ylabel('Density')
plt.title('Normal Distribution N(0,1)')
plt.legend()
plt.show()

# Properties:
# - 68% of data within 1 std
# - 95% within 2 std
# - 99.7% within 3 std

# ML applications:
# - Weight initialization: N(0, 1/sqrt(n))
# - Noise model in regression
# - Gaussian processes
# - Variational autoencoders
```

**Multivariate Normal Distribution**:
```python
# X ~ N(Œº, Œ£)
# Œº: mean vector
# Œ£: covariance matrix

mean = [0, 0]
cov = [[1, 0.5],
       [0.5, 1]]  # Correlation = 0.5

X = np.random.multivariate_normal(mean, cov, size=1000)

plt.scatter(X[:, 0], X[:, 1], alpha=0.5)
plt.xlabel('X1')
plt.ylabel('X2')
plt.title('Bivariate Normal Distribution')
plt.axis('equal')
plt.grid(True)
plt.show()

# ML applications:
# - Gaussian mixture models
# - Generative models
# - Bayesian inference
```

### Maximum Likelihood Estimation (MLE)

```python
# Find parameters that maximize P(data | parameters)

def mle_normal_distribution(data):
    """
    Estimate Œº and œÉ¬≤ for normal distribution

    Likelihood: L(Œº, œÉ¬≤) = Œ† (1/‚àö(2œÄœÉ¬≤)) exp(-(x_i - Œº)¬≤/(2œÉ¬≤))

    Log-likelihood: log L = -n/2 log(2œÄœÉ¬≤) - Œ£(x_i - Œº)¬≤/(2œÉ¬≤)

    MLE estimates:
    Œº_MLE = sample mean
    œÉ¬≤_MLE = sample variance
    """
    mu_mle = np.mean(data)
    sigma2_mle = np.var(data, ddof=0)  # ddof=0 for MLE (ddof=1 for unbiased)

    return mu_mle, sigma2_mle

# Generate data
true_mu, true_sigma = 5, 2
data = np.random.normal(true_mu, true_sigma, size=1000)

# Estimate parameters
mu_hat, sigma2_hat = mle_normal_distribution(data)

print(f"True parameters: Œº={true_mu}, œÉ¬≤={true_sigma**2}")
print(f"MLE estimates: Œº={mu_hat:.3f}, œÉ¬≤={sigma2_hat:.3f}")
# MLE estimates should be very close to true parameters!
```

**MLE for Linear Regression**:
```python
# Assume: y = Xw + Œµ, where Œµ ~ N(0, œÉ¬≤)
# Likelihood: P(y | X, w, œÉ¬≤) = Œ† N(y_i | x_i^T w, œÉ¬≤)

# Maximizing likelihood ‚â° Minimizing MSE!

# log L = -n/2 log(2œÄœÉ¬≤) - Œ£(y_i - x_i^T w)¬≤/(2œÉ¬≤)
# Maximizing log L w.r.t. w ‚â° Minimizing Œ£(y_i - x_i^T w)¬≤
# This is exactly MSE!

# MLE for linear regression = Least squares solution
```

---

## üìä Statistics

Statistics provides tools for inference, hypothesis testing, and understanding data.

### Descriptive Statistics

**Measures of Central Tendency**:
```python
data = np.array([1, 2, 3, 4, 5, 100])  # Note outlier

# Mean (sensitive to outliers)
mean = np.mean(data)  # 19.17

# Median (robust to outliers)
median = np.median(data)  # 3.5

# Mode (most frequent value)
from scipy.stats import mode
mode_result = mode(data, keepdims=True)  # No mode (all unique)

print(f"Mean: {mean:.2f}")
print(f"Median: {median:.2f}")

# For skewed distributions (with outliers), median is better
```

**Measures of Spread**:
```python
# Variance
variance = np.var(data, ddof=1)  # Sample variance (ddof=1)

# Standard deviation
std = np.std(data, ddof=1)

# Interquartile range (IQR) - robust to outliers
Q1 = np.percentile(data, 25)
Q3 = np.percentile(data, 75)
IQR = Q3 - Q1

print(f"Variance: {variance:.2f}")
print(f"Std dev: {std:.2f}")
print(f"IQR: {IQR:.2f}")
```

**Covariance and Correlation**:
```python
# Two variables: height and weight

height = np.array([160, 165, 170, 175, 180])
weight = np.array([55, 60, 65, 70, 75])

# Covariance: How much do they vary together?
cov = np.cov(height, weight)[0, 1]  # 25.0

# Correlation: Normalized covariance (-1 to 1)
corr = np.corrcoef(height, weight)[0, 1]  # 1.0 (perfect positive correlation)

print(f"Covariance: {cov:.2f}")
print(f"Correlation: {corr:.3f}")

# Interpretation:
# corr = 1: Perfect positive correlation
# corr = 0: No linear correlation
# corr = -1: Perfect negative correlation

# ML application: Feature selection
# Remove highly correlated features (multicollinearity)
def remove_correlated_features(X, threshold=0.95):
    corr_matrix = np.corrcoef(X.T)
    to_remove = set()

    for i in range(len(corr_matrix)):
        for j in range(i+1, len(corr_matrix)):
            if abs(corr_matrix[i, j]) > threshold:
                to_remove.add(j)

    mask = np.ones(X.shape[1], dtype=bool)
    mask[list(to_remove)] = False
    return X[:, mask]
```

### Hypothesis Testing

**t-test** (Compare two means):
```python
from scipy.stats import ttest_ind

# A/B test: Did the new feature increase conversion?
control = np.random.binomial(1, 0.10, size=1000)  # 10% conversion
treatment = np.random.binomial(1, 0.12, size=1000)  # 12% conversion

# Two-sample t-test
t_stat, p_value = ttest_ind(treatment, control)

print(f"t-statistic: {t_stat:.3f}")
print(f"p-value: {p_value:.4f}")

alpha = 0.05
if p_value < alpha:
    print("Result: Statistically significant! ‚úÖ")
    print("The new feature improved conversion.")
else:
    print("Result: Not statistically significant ‚ùå")
    print("Cannot conclude the new feature is better.")

# Confidence interval
from scipy.stats import t as t_dist

mean_diff = treatment.mean() - control.mean()
se = np.sqrt(treatment.var()/len(treatment) + control.var()/len(control))
ci_lower = mean_diff - 1.96 * se
ci_upper = mean_diff + 1.96 * se

print(f"\nMean difference: {mean_diff:.4f}")
print(f"95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]")
```

**Chi-Square Test** (Categorical data):
```python
from scipy.stats import chi2_contingency

# Are gender and product preference independent?
#              Product A   Product B
# Male            30          70
# Female          50          50

observed = np.array([[30, 70],
                     [50, 50]])

chi2, p_value, dof, expected = chi2_contingency(observed)

print(f"Chi-square statistic: {chi2:.3f}")
print(f"p-value: {p_value:.4f}")
print(f"Expected frequencies:\n{expected}")

if p_value < 0.05:
    print("Gender and product preference are dependent")
else:
    print("Gender and product preference are independent")
```

---

## üéØ Optimization

Optimization is how we train ML models - finding parameters that minimize loss.

### Convex Optimization

**Convex Function**:
```
f is convex if: f(Œªx + (1-Œª)y) ‚â§ Œªf(x) + (1-Œª)f(y) for all Œª ‚àà [0,1]

Geometric meaning: Line segment between any two points lies above the function

Good news: Local minimum = Global minimum!
```

**Examples**:
```python
# Convex functions:
# - f(x) = x¬≤
# - f(x) = |x|
# - f(x) = exp(x)
# - f(x) = -log(x)

# Non-convex functions:
# - f(x) = x¬≥
# - f(x) = sin(x)
# - Neural networks (non-convex!)

# Visualize
x = np.linspace(-3, 3, 100)

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# Convex
axes[0].plot(x, x**2, label='x¬≤')
axes[0].set_title('Convex Function')
axes[0].legend()
axes[0].grid(True)

# Non-convex
axes[1].plot(x, x**3 - 3*x, label='x¬≥ - 3x')
axes[1].set_title('Non-Convex Function')
axes[1].legend()
axes[1].grid(True)

plt.show()
```

### Optimization Algorithms

**Gradient Descent** (covered earlier):
```python
# w_new = w_old - learning_rate √ó gradient
```

#### Convergence Theory for Gradient Descent Variants

**Mathematical Framework:**

**Assumptions and Definitions:**
```
L-Lipschitz Continuous Gradient:
||‚àáf(x) - ‚àáf(y)|| ‚â§ L¬∑||x - y|| for all x, y

Convexity:
f(y) ‚â• f(x) + ‚àáf(x)^T(y - x) for all x, y

Strong Convexity (with parameter Œº > 0):
f(y) ‚â• f(x) + ‚àáf(x)^T(y - x) + (Œº/2)||y - x||¬≤ for all x, y

Smoothness implies: f(y) ‚â§ f(x) + ‚àáf(x)^T(y - x) + (L/2)||y - x||¬≤
```

**Theorem 1: Gradient Descent Convergence for Smooth Convex Functions**
```
Problem: min_{x‚àà‚Ñù^d} f(x), where f is convex and L-smooth

Algorithm: x_{k+1} = x_k - Œ±¬∑‚àáf(x_k)

Step Size: Œ± ‚â§ 1/L

Convergence Rate:
f(x_k) - f(x*) ‚â§ (2L¬∑||x_0 - x*||¬≤) / (k + 4)
             = O(1/k)

Result: Sublinear convergence, need O(1/Œµ) iterations for Œµ-accuracy
```

**Proof Sketch:**
```
Key Lemma (Descent Lemma):
For Œ± ‚â§ 1/L and L-smooth f:
f(x_{k+1}) ‚â§ f(x_k) - (Œ±/2)||‚àáf(x_k)||¬≤

Proof of Lemma:
1. By L-smoothness:
   f(x_{k+1}) ‚â§ f(x_k) + ‚àáf(x_k)^T(x_{k+1} - x_k) + (L/2)||x_{k+1} - x_k||¬≤

2. Substitute x_{k+1} = x_k - Œ±¬∑‚àáf(x_k):
   f(x_{k+1}) ‚â§ f(x_k) - Œ±¬∑||‚àáf(x_k)||¬≤ + (LŒ±¬≤/2)||‚àáf(x_k)||¬≤
              = f(x_k) - Œ±(1 - LŒ±/2)||‚àáf(x_k)||¬≤

3. If Œ± ‚â§ 1/L, then 1 - LŒ±/2 ‚â• 1/2:
   f(x_{k+1}) ‚â§ f(x_k) - (Œ±/2)||‚àáf(x_k)||¬≤  ‚úì

Main Convergence Proof:
1. By convexity: f(x_k) - f(x*) ‚â§ ‚àáf(x_k)^T(x_k - x*)

2. Expand ||x_{k+1} - x*||¬≤:
   ||x_{k+1} - x*||¬≤ = ||x_k - Œ±¬∑‚àáf(x_k) - x*||¬≤
                     = ||x_k - x*||¬≤ - 2Œ±¬∑‚àáf(x_k)^T(x_k - x*) + Œ±¬≤||‚àáf(x_k)||¬≤

3. Rearrange:
   2Œ±¬∑‚àáf(x_k)^T(x_k - x*) = ||x_k - x*||¬≤ - ||x_{k+1} - x*||¬≤ + Œ±¬≤||‚àáf(x_k)||¬≤
                           ‚â• ||x_k - x*||¬≤ - ||x_{k+1} - x*||¬≤  (drop positive term)

4. Therefore:
   f(x_k) - f(x*) ‚â§ (||x_k - x*||¬≤ - ||x_{k+1} - x*||¬≤) / (2Œ±)

5. Sum from k=0 to K-1:
   Œ£_{k=0}^{K-1} [f(x_k) - f(x*)] ‚â§ (||x_0 - x*||¬≤ - ||x_K - x*||¬≤) / (2Œ±)
                                   ‚â§ ||x_0 - x*||¬≤ / (2Œ±)

6. Since f(x_k) is decreasing (by Descent Lemma):
   K¬∑[f(x_K) - f(x*)] ‚â§ Œ£_{k=0}^{K-1} [f(x_k) - f(x*)] ‚â§ ||x_0 - x*||¬≤ / (2Œ±)

7. Final bound:
   f(x_K) - f(x*) ‚â§ ||x_0 - x*||¬≤ / (2Œ±K)

   With Œ± = 1/L:
   f(x_K) - f(x*) ‚â§ L¬∑||x_0 - x*||¬≤ / (2K) = O(1/K)  ‚úì
```

**Theorem 2: Gradient Descent for Strongly Convex Functions**
```
Problem: min f(x), where f is Œº-strongly convex and L-smooth

Step Size: Œ± ‚â§ 2/(Œº + L) (or simply Œ± = 1/L)

Convergence Rate:
||x_k - x*||¬≤ ‚â§ (1 - Œº/L)^k ¬∑ ||x_0 - x*||¬≤
f(x_k) - f(x*) ‚â§ (L/2)(1 - Œº/L)^k ¬∑ ||x_0 - x*||¬≤

Result: Linear (exponential) convergence!
Need O(log(1/Œµ)) iterations for Œµ-accuracy
```

**Proof Sketch:**
```
Key Property: For Œº-strongly convex and L-smooth f:
||‚àáf(x)||¬≤ ‚â• 2Œº[f(x) - f(x*)]

Proof:
1. By strong convexity at x*:
   f(x) ‚â• f(x*) + ‚àáf(x*)^T(x - x*) + (Œº/2)||x - x*||¬≤
        = f(x*) + (Œº/2)||x - x*||¬≤  (since ‚àáf(x*) = 0)

2. By smoothness at x:
   f(x*) ‚â• f(x) + ‚àáf(x)^T(x* - x) - (L/2)||x* - x||¬≤

3. Combine:
   ||‚àáf(x)||¬≤ = ||‚àáf(x) - ‚àáf(x*)||¬≤ ‚â• ŒºL¬∑||x - x*||¬≤  (PL inequality)
              ‚â• 2Œº[f(x) - f(x*)]  ‚úì

Main Convergence:
1. Start with:
   ||x_{k+1} - x*||¬≤ = ||x_k - x*||¬≤ - 2Œ±¬∑‚àáf(x_k)^T(x_k - x*) + Œ±¬≤||‚àáf(x_k)||¬≤

2. By strong convexity:
   ‚àáf(x_k)^T(x_k - x*) ‚â• f(x_k) - f(x*) + (Œº/2)||x_k - x*||¬≤

3. Substitute Œ± = 1/L:
   ||x_{k+1} - x*||¬≤ ‚â§ ||x_k - x*||¬≤[1 - Œº/L] + (1/L¬≤)||‚àáf(x_k)||¬≤[1 - Œº/L]
                     ‚â§ (1 - Œº/L)||x_k - x*||¬≤

4. Iterate:
   ||x_k - x*||¬≤ ‚â§ (1 - Œº/L)^k ¬∑ ||x_0 - x*||¬≤  ‚úì

Condition Number: Œ∫ = L/Œº
- If Œ∫ is small (well-conditioned): Fast convergence
- If Œ∫ is large (ill-conditioned): Slow convergence
```

**Theorem 3: Stochastic Gradient Descent (SGD) Convergence**
```
Problem: min f(x) = E_{Œæ}[f(x; Œæ)]
         Where Œæ represents random data samples

Algorithm: x_{k+1} = x_k - Œ±_k¬∑‚àáf(x_k; Œæ_k)
          ‚àáf(x_k; Œæ_k) is unbiased: E[‚àáf(x_k; Œæ_k)] = ‚àáf(x_k)

Robbins-Monro Conditions (for learning rate Œ±_k):
1. Œ£_{k=1}^‚àû Œ±_k = ‚àû        (step sizes sum to infinity)
2. Œ£_{k=1}^‚àû Œ±_k¬≤ < ‚àû       (step sizes squared sum is finite)

Example: Œ±_k = Œ±_0/‚àök satisfies both conditions

Convergence Result (for convex f):
E[f(x_k)] - f(x*) = O(1/‚àök)

Result: Slower than batch GD (O(1/k)), but much cheaper per iteration!
```

**Proof Intuition:**
```
Key Inequality:
E[||x_{k+1} - x*||¬≤] = E[||x_k - Œ±_k¬∑g_k - x*||¬≤]
                     = ||x_k - x*||¬≤ - 2Œ±_k¬∑‚àáf(x_k)^T(x_k - x*) + Œ±_k¬≤¬∑E[||g_k||¬≤]

where g_k = ‚àáf(x_k; Œæ_k) is stochastic gradient with variance œÉ¬≤

Trade-off:
- Term 1: -2Œ±_k¬∑‚àáf(x_k)^T(x_k - x*) ‚Üí Progress towards optimum
- Term 2: +Œ±_k¬≤¬∑œÉ¬≤ ‚Üí Variance from stochastic gradient

As k ‚Üí ‚àû:
- Œ±_k ‚Üí 0 makes variance term ‚Üí 0 (condition 2)
- But Œ£Œ±_k = ‚àû ensures we reach optimum (condition 1)
```

**Theorem 4: Momentum Convergence (Nesterov Accelerated Gradient)**
```
Algorithm:
v_{k+1} = Œ≤¬∑v_k + ‚àáf(x_k)
x_{k+1} = x_k - Œ±¬∑v_{k+1}

Convergence for Convex L-smooth:
f(x_k) - f(x*) = O(1/k¬≤)  (compared to O(1/k) for vanilla GD!)

Convergence for Strongly Convex:
||x_k - x*|| = O((1 - ‚àö(Œº/L))^k)  (improved constant)

Result: Optimal first-order method for smooth convex optimization
```

**Theorem 5: Adam Convergence (Sketch)**
```
Algorithm:
m_k = Œ≤_1¬∑m_{k-1} + (1-Œ≤_1)¬∑g_k         (first moment)
v_k = Œ≤_2¬∑v_{k-1} + (1-Œ≤_2)¬∑g_k¬≤        (second moment)
mÃÇ_k = m_k / (1 - Œ≤_1^k)                 (bias correction)
vÃÇ_k = v_k / (1 - Œ≤_2^k)                 (bias correction)
x_{k+1} = x_k - Œ±¬∑mÃÇ_k / (‚àövÃÇ_k + Œµ)

Typical values: Œ≤_1 = 0.9, Œ≤_2 = 0.999, Œµ = 10^(-8)

Convergence (for convex case):
Regret bound: R_T = O(‚àöT)
Average convergence: (1/T)Œ£_{k=1}^T [f(x_k) - f(x*)] = O(1/‚àöT)

Note: Adam may NOT converge for some convex problems!
Fix: AMSGrad variant with max(v_1, ..., v_k) instead of v_k
```

**Summary Table: Convergence Rates**
```
Algorithm        | Convex      | Strongly Convex    | Per-iteration Cost
-----------------|-------------|--------------------|-----------------
GD               | O(1/k)      | O(exp(-Œºk/L))     | O(nd)
SGD              | O(1/‚àök)     | O(1/k)            | O(d)
Momentum (NAG)   | O(1/k¬≤)     | O(exp(-‚àöŒº/L¬∑k))   | O(nd)
Adam/RMSprop     | O(1/‚àök)     | O(1/‚àök)           | O(d)
Newton           | O(1/k¬≤)     | Quadratic         | O(nd¬≤+d¬≥)

Where:
- n: dataset size
- d: dimension
- k: iteration number
- Œº: strong convexity parameter
- L: smoothness parameter
- Œ∫ = L/Œº: condition number
```

**Newton's Method** (uses second derivative):
```python
def newtons_method(f, grad_f, hess_f, x0, num_iterations=10):
    """
    Newton's method for optimization

    Uses second-order information (Hessian)
    Update: x_new = x_old - H^(-1) @ grad

    Advantages: Faster convergence (quadratic vs linear)
    Disadvantages: Expensive (need to compute and invert Hessian)
    """
    x = x0.copy()

    for i in range(num_iterations):
        grad = grad_f(x)
        hess = hess_f(x)

        # Newton step
        delta = np.linalg.solve(hess, grad)  # H^(-1) @ grad
        x = x - delta

        print(f"Iteration {i+1}: x = {x}, f(x) = {f(x):.6f}")

    return x

# Example: f(x) = x^2 + 2x + 1
f = lambda x: x**2 + 2*x + 1
grad_f = lambda x: 2*x + 2
hess_f = lambda x: np.array([[2.0]])  # Second derivative

x_min = newtons_method(f, grad_f, hess_f, x0=np.array([5.0]))
# Converges in 1 iteration! (function is quadratic)
```

**Constrained Optimization** (Lagrange Multipliers):
```python
# Minimize f(x) subject to g(x) = 0

# Example: Maximize f(x,y) = xy subject to x + y = 1

# Lagrangian: L(x, y, Œª) = xy + Œª(1 - x - y)

# Set derivatives to 0:
# ‚àÇL/‚àÇx = y - Œª = 0
# ‚àÇL/‚àÇy = x - Œª = 0
# ‚àÇL/‚àÇŒª = 1 - x - y = 0

# Solve:
# y = Œª, x = Œª, x + y = 1
# 2Œª = 1 ‚Üí Œª = 0.5
# x = y = 0.5

# Maximum: f(0.5, 0.5) = 0.25

def lagrange_example():
    from scipy.optimize import minimize

    # Objective: Minimize -xy (negative for maximization)
    def objective(vars):
        x, y = vars
        return -x * y

    # Constraint: x + y - 1 = 0
    constraint = {'type': 'eq', 'fun': lambda vars: vars[0] + vars[1] - 1}

    # Initial guess
    x0 = [0, 0]

    # Optimize
    result = minimize(objective, x0, constraints=constraint)

    print(f"Optimal solution: x={result.x[0]:.3f}, y={result.x[1]:.3f}")
    print(f"Maximum value: {-result.fun:.3f}")

lagrange_example()
# Output: x=0.500, y=0.500, maximum=0.250
```

---

## üì° Information Theory

Information theory quantifies information, essential for understanding entropy, KL divergence, and mutual information in ML.

### Entropy

**Shannon Entropy**: Measure of uncertainty/surprise
```
H(X) = -Œ£ P(x) log‚ÇÇ P(x)

Units: bits (if log base 2)

Interpretation: Average number of bits needed to encode X
```

**Example**:
```python
def entropy(probabilities):
    """
    Compute Shannon entropy

    H(X) = -Œ£ p(x) log‚ÇÇ p(x)
    """
    # Remove zeros (0 log 0 = 0 by definition)
    p = np.array(probabilities)
    p = p[p > 0]

    return -np.sum(p * np.log2(p))

# Fair coin: P(H) = P(T) = 0.5
H_fair = entropy([0.5, 0.5])
print(f"Entropy of fair coin: {H_fair:.3f} bits")  # 1.000 bit

# Biased coin: P(H) = 0.9, P(T) = 0.1
H_biased = entropy([0.9, 0.1])
print(f"Entropy of biased coin: {H_biased:.3f} bits")  # 0.469 bits
# Less entropy = more predictable!

# Uniform distribution (maximum entropy)
H_uniform = entropy([0.25, 0.25, 0.25, 0.25])
print(f"Entropy of uniform (4 outcomes): {H_uniform:.3f} bits")  # 2.000 bits

# Deterministic (minimum entropy)
H_deterministic = entropy([1.0])
print(f"Entropy of deterministic: {H_deterministic:.3f} bits")  # 0.000 bits
```

**Cross-Entropy** (ML loss function):
```python
def cross_entropy(p, q):
    """
    Cross-entropy H(p, q) = -Œ£ p(x) log q(x)

    p: true distribution
    q: predicted distribution

    Measures: How many bits needed to encode p using q
    """
    q = np.clip(q, 1e-10, 1.0)  # Numerical stability
    return -np.sum(p * np.log(q))

# Example: 3-class classification
true_distribution = np.array([1, 0, 0])  # Class 0
predicted_distribution = np.array([0.7, 0.2, 0.1])

CE = cross_entropy(true_distribution, predicted_distribution)
print(f"Cross-Entropy: {CE:.3f}")  # 0.357

# Perfect prediction
perfect_prediction = np.array([1.0, 0.0, 0.0])
CE_perfect = cross_entropy(true_distribution, perfect_prediction)
print(f"Cross-Entropy (perfect): {CE_perfect:.3f}")  # ‚âà 0.000
```

**KL Divergence** (Relative Entropy):
```python
def kl_divergence(p, q):
    """
    KL(p || q) = Œ£ p(x) log(p(x) / q(x))

    Measures: How different q is from p
    Properties:
    - KL(p || q) ‚â• 0
    - KL(p || q) = 0 iff p = q
    - Not symmetric: KL(p || q) ‚â† KL(q || p)
    """
    p = np.array(p)
    q = np.array(q)
    q = np.clip(q, 1e-10, 1.0)

    return np.sum(p * np.log(p / q))

# Example
p = [0.5, 0.3, 0.2]
q1 = [0.5, 0.3, 0.2]  # Same as p
q2 = [0.4, 0.4, 0.2]  # Different from p

print(f"KL(p || q1): {kl_divergence(p, q1):.4f}")  # 0.0000
print(f"KL(p || q2): {kl_divergence(p, q2):.4f}")  # 0.0085

# ML application: Variational inference
# Minimize KL(q || p) to approximate true posterior p with q
```

**Mutual Information**:
```python
# I(X; Y) = H(X) + H(Y) - H(X, Y)
# Measures: How much knowing Y reduces uncertainty about X

def mutual_information(X, Y):
    """
    Estimate mutual information between X and Y
    """
    from sklearn.metrics import mutual_info_score

    return mutual_info_score(X, Y)

# Example: Feature selection
# Select features with high mutual information with target

from sklearn.feature_selection import mutual_info_classif

X = np.random.rand(1000, 10)
y = (X[:, 0] + X[:, 1] > 1).astype(int)  # Target depends on features 0 and 1

mi_scores = mutual_info_classif(X, y)

print("Mutual information scores:")
for i, score in enumerate(mi_scores):
    print(f"  Feature {i}: {score:.4f}")
# Features 0 and 1 should have highest MI with target
```

---

## üß† Neural Network Initialization Theory

Proper weight initialization is critical for successful neural network training. Poor initialization can lead to vanishing/exploding gradients, slow convergence, or complete training failure.

### The Initialization Problem

**Why Initialization Matters:**
```
Problem: Neural networks are highly non-convex
- Different initializations ‚Üí different local minima
- Bad initialization ‚Üí vanishing/exploding gradients
- Good initialization ‚Üí faster convergence, better final performance

Key Insight: Initialize weights to preserve signal variance across layers
```

**Naive Approaches (Don't Do This!):**
```python
# ‚ùå All zeros: Symmetry problem
W = np.zeros((n_out, n_in))
# All neurons learn the same function!
# Gradient for all neurons is identical
# Network effectively has only one neuron per layer

# ‚ùå All same value: Same problem
W = np.ones((n_out, n_in)) * 0.5

# ‚ùå Too large values
W = np.random.randn(n_out, n_in) * 10
# Output variance explodes: Var(output) = (n_in √ó 10¬≤) √ó Var(input)
# Gradients explode

# ‚ùå Too small values
W = np.random.randn(n_out, n_in) * 0.001
# Output variance vanishes: Var(output) ‚âà 0
# Gradients vanish
```

### Xavier/Glorot Initialization (2010)

**Mathematical Foundation:**

**Goal:** Preserve variance of activations and gradients across layers

**Assumption:** Linear activation (or near-linear like tanh around 0)

**Forward Pass Analysis:**
```
Layer computation: y = W¬∑x + b

For one neuron: y_i = Œ£_{j=1}^{n_in} w_{ij} x_j

Assumptions:
1. x_j are i.i.d. with mean 0 and variance œÉ¬≤_x
2. w_{ij} are i.i.d. with mean 0 and variance œÉ¬≤_w
3. x and w are independent

Variance of output:
Var(y_i) = Var(Œ£_j w_{ij} x_j)
         = Œ£_j Var(w_{ij} x_j)           (independence)
         = Œ£_j E[w_{ij}¬≤] E[x_j¬≤]         (independence)
         = Œ£_j Var(w_{ij}) Var(x_j)       (mean 0)
         = n_in ¬∑ œÉ¬≤_w ¬∑ œÉ¬≤_x

To preserve variance (Var(y_i) = œÉ¬≤_x):
n_in ¬∑ œÉ¬≤_w = 1
œÉ¬≤_w = 1 / n_in
```

**Backward Pass Analysis:**
```
Gradient backprop: ‚àÇL/‚àÇx = W^T ¬∑ ‚àÇL/‚àÇy

By similar analysis:
Var(‚àÇL/‚àÇx_j) = n_out ¬∑ œÉ¬≤_w ¬∑ Var(‚àÇL/‚àÇy)

To preserve gradient variance:
n_out ¬∑ œÉ¬≤_w = 1
œÉ¬≤_w = 1 / n_out
```

**Xavier/Glorot Compromise:**
```
Problem: Forward wants œÉ¬≤_w = 1/n_in, backward wants œÉ¬≤_w = 1/n_out

Solution: Average them!
œÉ¬≤_w = 2 / (n_in + n_out)

Xavier Uniform:
W ~ U[-‚àö(6/(n_in + n_out)), ‚àö(6/(n_in + n_out))]

Xavier Normal:
W ~ N(0, 2/(n_in + n_out))

Note: U[-a, a] has variance a¬≤/3, so a = ‚àö(3¬∑2/(n_in+n_out)) = ‚àö(6/(n_in+n_out))
```

**Implementation:**
```python
def xavier_uniform(n_in, n_out):
    """
    Xavier/Glorot uniform initialization

    Used for: tanh, sigmoid activations
    """
    limit = np.sqrt(6.0 / (n_in + n_out))
    return np.random.uniform(-limit, limit, size=(n_out, n_in))

def xavier_normal(n_in, n_out):
    """
    Xavier/Glorot normal initialization

    Used for: tanh, sigmoid activations
    """
    std = np.sqrt(2.0 / (n_in + n_out))
    return np.random.randn(n_out, n_in) * std
```

### He Initialization (2015)

**Motivation:** Xavier assumes linear activation, but ReLU is non-linear!

**ReLU Analysis:**
```
ReLU(x) = max(0, x)

Property: Kills half the neurons (negative values ‚Üí 0)

Effect on variance:
- Input variance: œÉ¬≤
- After ReLU: œÉ¬≤/2 (approximately, for zero-mean input)

Derivation:
For x ~ N(0, œÉ¬≤):
E[ReLU(x)] = E[x | x > 0] ¬∑ P(x > 0) = (œÉ/‚àö(2œÄ)) ¬∑ 0.5

Var(ReLU(x)) = E[ReLU(x)¬≤] - E[ReLU(x)]¬≤
             = E[x¬≤ | x > 0] ¬∑ P(x > 0) - (œÉ/‚àö(2œÄ) ¬∑ 0.5)¬≤
             = œÉ¬≤/2 - small term
             ‚âà œÉ¬≤/2

So ReLU halves the variance!
```

**He Initialization:**
```
Forward pass with ReLU:
Var(y_i) = n_in ¬∑ œÉ¬≤_w ¬∑ œÉ¬≤_x / 2  (ReLU kills half)

To preserve variance (Var(y_i) = œÉ¬≤_x):
n_in ¬∑ œÉ¬≤_w / 2 = 1
œÉ¬≤_w = 2 / n_in

He Normal (most common):
W ~ N(0, 2/n_in)

He Uniform:
W ~ U[-‚àö(6/n_in), ‚àö(6/n_in)]
```

**Implementation:**
```python
def he_normal(n_in, n_out):
    """
    He initialization (Kaiming initialization)

    Used for: ReLU, Leaky ReLU, PReLU activations

    Reference: He et al., "Delving Deep into Rectifiers: Surpassing
    Human-Level Performance on ImageNet Classification", ICCV 2015
    """
    std = np.sqrt(2.0 / n_in)
    return np.random.randn(n_out, n_in) * std

def he_uniform(n_in, n_out):
    """He uniform initialization"""
    limit = np.sqrt(6.0 / n_in)
    return np.random.uniform(-limit, limit, size=(n_out, n_in))
```

### Comparison and Guidelines

**Initialization Summary:**
```
Activation       | Forward Preserve | Backward Preserve | Recommended
-----------------|------------------|-------------------|-------------
Linear/None      | Var = 1/n_in    | Var = 1/n_out    | Xavier
tanh             | Var = 1/n_in    | Var = 1/n_out    | Xavier
sigmoid          | Var = 1/n_in    | Var = 1/n_out    | Xavier
ReLU             | Var = 2/n_in    | Var = 2/n_out    | He
Leaky ReLU       | Var ‚âà 2/n_in    | Var ‚âà 2/n_out    | He
ELU              | Var ‚âà 1.5/n_in  | Var ‚âà 1.5/n_out  | He or Xavier
SELU             | Special         | Special          | LeCun*

*LeCun Normal: W ~ N(0, 1/n_in)
```

**Modern PyTorch/TensorFlow Defaults:**
```python
import torch.nn as nn

# Linear layer
nn.Linear(n_in, n_out)
# Default: Xavier uniform (Glorot)

# Conv2D layer
nn.Conv2d(in_channels, out_channels, kernel_size)
# Default: He (Kaiming) uniform for ReLU

# LSTM/GRU
nn.LSTM(input_size, hidden_size)
# Default: Xavier uniform (Glorot)
```

**Complete Initialization Example:**
```python
class NeuralNetwork:
    """Neural network with proper initialization"""

    def __init__(self, layers, activation='relu'):
        """
        Args:
            layers: [n_input, n_hidden1, n_hidden2, ..., n_output]
            activation: 'relu', 'tanh', 'sigmoid'
        """
        self.weights = []
        self.biases = []

        for i in range(len(layers) - 1):
            n_in, n_out = layers[i], layers[i+1]

            # Initialize weights
            if activation == 'relu':
                # He initialization
                W = np.random.randn(n_out, n_in) * np.sqrt(2.0 / n_in)
            elif activation in ['tanh', 'sigmoid']:
                # Xavier initialization
                W = np.random.randn(n_out, n_in) * np.sqrt(2.0 / (n_in + n_out))
            else:
                # Default: small random
                W = np.random.randn(n_out, n_in) * 0.01

            # Initialize biases to zero (common practice)
            b = np.zeros((n_out, 1))

            self.weights.append(W)
            self.biases.append(b)

    def forward(self, x, activation='relu'):
        """Forward pass with specified activation"""
        a = x
        for W, b in zip(self.weights[:-1], self.biases[:-1]):
            z = W @ a + b
            if activation == 'relu':
                a = np.maximum(0, z)
            elif activation == 'tanh':
                a = np.tanh(z)
            elif activation == 'sigmoid':
                a = 1 / (1 + np.exp(-z))

        # Output layer (no activation for regression, or apply softmax for classification)
        z = self.weights[-1] @ a + self.biases[-1]
        return z

# Example usage
model = NeuralNetwork([784, 512, 256, 10], activation='relu')
print(f"Layer 1 weights std: {model.weights[0].std():.4f}")
print(f"Expected std: {np.sqrt(2.0/784):.4f}")
```

### Advanced Initialization Strategies

**1. LSUV (Layer-Sequential Unit-Variance, 2016):**
```python
def lsuv_init(model, data_sample):
    """
    Initialize weights then adjust to unit variance

    1. Initialize with orthogonal matrices
    2. Forward pass with sample data
    3. Scale weights to make output variance = 1
    4. Repeat for each layer
    """
    x = data_sample

    for layer in model.layers:
        # Initialize with orthogonal matrix
        W = np.linalg.qr(np.random.randn(layer.n_out, layer.n_in))[0]
        layer.W = W

        # Forward pass
        z = layer.forward(x)

        # Adjust to unit variance
        std = z.std()
        layer.W = layer.W / std

        x = layer.activation(z)
```

**2. Fixup Initialization (2019):**
```
For very deep networks (ResNets):
- Initialize most layers with He/Xavier
- Scale residual branches by 1/‚àöL (L = depth)
- No batch normalization needed!
```

**3. Batch Normalization Alternative:**
```
Instead of careful initialization:
- Use Batch Normalization after each layer
- BN normalizes activations to mean=0, std=1
- Makes network more robust to initialization
- Trade-off: BN adds computation and complexity
```

### Theoretical Guarantees

**Theorem (He et al., 2015):**
```
For ReLU networks with He initialization:
- Forward signal does not vanish or explode
- Backward gradient does not vanish or explode
- Enables training of networks with 30+ layers

Mathematically:
E[||y^(l)||¬≤] = E[||x^(0)||¬≤]  (forward)
E[||‚àÇL/‚àÇx^(0)||¬≤] = E[||‚àÇL/‚àÇy^(L)||¬≤]  (backward)

where l = layer index, L = total layers
```

**Condition for Gradient Flow:**
```
For stable training:

Forward: œÉ¬≤_out = œÉ¬≤_in  (variance preservation)
Backward: œÉ¬≤_grad = constant across layers

This requires: œÉ¬≤_w = O(1/n_in)

Violation leads to:
- œÉ¬≤_w too large ‚Üí exploding gradients
- œÉ¬≤_w too small ‚Üí vanishing gradients
```

### Summary and Best Practices

**Quick Reference:**
```python
import torch.nn as nn

# For ReLU/Leaky ReLU (most common):
nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='relu')
nn.init.zeros_(layer.bias)

# For tanh/sigmoid:
nn.init.xavier_normal_(layer.weight)
nn.init.zeros_(layer.bias)

# For LSTM/GRU:
nn.init.orthogonal_(layer.weight_ih)
nn.init.orthogonal_(layer.weight_hh)
nn.init.zeros_(layer.bias)

# General tip: Biases usually initialized to zero
# Exception: LSTM forget gate bias can be initialized to 1
```

**Key Insights:**
```
1. **Never initialize all weights to same value** (breaks symmetry)

2. **Match initialization to activation:**
   - ReLU family ‚Üí He initialization
   - tanh/sigmoid ‚Üí Xavier initialization

3. **Consider network depth:**
   - Very deep networks (>50 layers): Use Fixup or normalization layers
   - Moderate depth (10-30): He/Xavier sufficient

4. **Empirical tuning:**
   - Monitor activation/gradient statistics during training
   - Activation std should stay ‚âà1 across layers
   - Gradient norm should not explode or vanish

5. **Modern best practice:**
   - Use He/Xavier + Batch/Layer Normalization
   - This combination is very robust
```

---

## üîë Key Takeaways

**For Linear Algebra**:
- Vectors and matrices represent data and transformations
- Matrix multiplication is the core operation in neural networks
- Eigenvalues/eigenvectors are central to PCA and spectral methods
- SVD is a swiss army knife for dimensionality reduction and matrix factorization

**For Calculus**:
- Derivatives measure rates of change
- Gradients point in direction of steepest increase
- Chain rule enables backpropagation
- Optimization is gradient descent on steroids

**For Probability**:
- Bayes' theorem is fundamental for inference
- MLE connects optimization to statistics
- Normal distribution appears everywhere
- Conditional probability models dependencies

**For Statistics**:
- Descriptive statistics summarize data
- Hypothesis testing quantifies uncertainty
- Correlation ‚â† causation
- Always visualize before analyzing

**For Optimization**:
- Convex problems have unique global minima
- Neural networks are non-convex (local minima exist)
- Second-order methods converge faster but are expensive
- Constrained optimization uses Lagrange multipliers

**For Information Theory**:
- Entropy quantifies uncertainty
- Cross-entropy is the ML loss function
- KL divergence measures distribution difference
- Mutual information measures dependence

---

## üìö Further Reading

**Books**:
1. **Deisenroth, M. P., Faisal, A. A., & Ong, C. S.** (2020). *Mathematics for Machine Learning*. Cambridge University Press. Available free at: https://mml-book.github.io/
2. **Goodfellow, I., Bengio, Y., & Courville, A.** (2016). *Deep Learning*. MIT Press. Available at: https://www.deeplearningbook.org/
3. **Bishop, C. M.** (2006). *Pattern Recognition and Machine Learning*. Springer.
4. **Hastie, T., Tibshirani, R., & Friedman, J.** (2009). *The Elements of Statistical Learning* (2nd ed.). Springer. Available free at: https://hastie.su.stanford.edu/ElemStatLearn/
5. **Strang, G.** (2016). *Introduction to Linear Algebra* (5th ed.). Wellesley-Cambridge Press.
6. **Boyd, S., & Vandenberghe, L.** (2004). *Convex Optimization*. Cambridge University Press. Available free at: https://web.stanford.edu/~boyd/cvxbook/

**Seminal Papers**:
1. **Rumelhart, D. E., Hinton, G. E., & Williams, R. J.** (1986). "Learning representations by back-propagating errors." *Nature*, 323(6088), 533-536.
2. **Robbins, H., & Monro, S.** (1951). "A stochastic approximation method." *The Annals of Mathematical Statistics*, 22(3), 400-407.
3. **Kingma, D. P., & Ba, J.** (2015). "Adam: A method for stochastic optimization." *ICLR 2015*. arXiv:1412.6980

**Online Resources**:
- **3Blue1Brown** (YouTube): Visual explanations of linear algebra and calculus
  - "Essence of Linear Algebra" series
  - "Essence of Calculus" series
- **Khan Academy**: Foundations of probability and statistics
- **MIT OpenCourseWare**:
  - 18.06 Linear Algebra (Gilbert Strang)
  - 18.01 Single Variable Calculus
- **Stanford CS229**: Machine Learning (Andrew Ng)
  - Linear Algebra and Calculus review notes

**Practice**:
- Implement algorithms from scratch using only NumPy
- Derive gradients by hand before using automatic differentiation
- Visualize mathematical concepts with matplotlib
- Solve problems on Project Euler and Brilliant.org
- Work through exercises in the books listed above

---

## üìñ References

Key concepts and formulations in this guide are based on:

- **Linear Algebra**: Strang (2016), Deisenroth et al. (2020, Chapter 2)
- **Calculus & Optimization**: Boyd & Vandenberghe (2004), Goodfellow et al. (2016, Chapter 4)
- **Probability Theory**: Bishop (2006, Chapter 1-2), Deisenroth et al. (2020, Chapter 6)
- **Statistics**: Hastie et al. (2009, Chapter 2), James et al. (2021)
- **Information Theory**: Cover & Thomas (2006), Goodfellow et al. (2016, Chapter 3)
- **Backpropagation**: Rumelhart et al. (1986), Goodfellow et al. (2016, Chapter 6)
- **Gradient Descent**: Robbins & Monro (1951), Bottou (2010)

---

*Master these mathematical foundations and you'll have deep understanding of how and why ML algorithms work!*
