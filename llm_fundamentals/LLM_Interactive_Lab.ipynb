{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¤– Large Language Models (LLMs) - Complete Guide\n",
    "\n",
    "## Understanding the AI Revolution: From GPT to Claude\n",
    "\n",
    "**What you'll learn:**\n",
    "- What LLMs are and how they work\n",
    "- The architecture behind models like GPT, Claude, and LLaMA\n",
    "- Transformers and attention mechanisms\n",
    "- Training, fine-tuning, and prompt engineering\n",
    "- Hands-on: Build a mini language model\n",
    "- Using LLM APIs (OpenAI, Anthropic, etc.)\n",
    "\n",
    "**Prerequisites:**\n",
    "- Basic Python\n",
    "- Understanding of neural networks (helpful but not required)\n",
    "\n",
    "**Time:** 90-120 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š Table of Contents\n",
    "\n",
    "1. [Introduction to LLMs](#intro)\n",
    "2. [How LLMs Work](#how)\n",
    "3. [The Transformer Architecture](#transformer)\n",
    "4. [Attention Mechanism](#attention)\n",
    "5. [Training Process](#training)\n",
    "6. [Tokenization](#tokenization)\n",
    "7. [Prompting & Context](#prompting)\n",
    "8. [Fine-tuning vs RAG](#finetuning)\n",
    "9. [Building a Mini LLM](#miniLLM)\n",
    "10. [Using LLM APIs](#apis)\n",
    "11. [Advanced Topics](#advanced)\n",
    "12. [Exercises](#exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## 1. ğŸ¯ Introduction to LLMs\n",
    "\n",
    "### What is a Large Language Model?\n",
    "\n",
    "An **LLM** is a neural network trained on massive amounts of text to:\n",
    "- Understand language patterns\n",
    "- Generate human-like text\n",
    "- Complete tasks through natural language\n",
    "\n",
    "### Popular LLMs\n",
    "\n",
    "| Model | Company | Size | Best For |\n",
    "|-------|---------|------|----------|\n",
    "| **GPT-4** | OpenAI | ~1.7T params | General purpose, reasoning |\n",
    "| **Claude 3** | Anthropic | Unknown | Safety, long context |\n",
    "| **Gemini** | Google | ~1.5T params | Multimodal tasks |\n",
    "| **LLaMA 3** | Meta | 8B-70B params | Open source, fine-tuning |\n",
    "| **Mistral** | Mistral AI | 7B-8x7B params | Efficient, open |\n",
    "\n",
    "### Evolution Timeline\n",
    "\n",
    "```\n",
    "2017: Transformer Architecture (\"Attention is All You Need\")\n",
    "   â†“\n",
    "2018: GPT-1 (117M params) - First large-scale pretraining\n",
    "   â†“\n",
    "2019: GPT-2 (1.5B params) - \"Too dangerous to release\"\n",
    "   â†“\n",
    "2020: GPT-3 (175B params) - Emergent capabilities\n",
    "   â†“\n",
    "2022: ChatGPT - Conversational AI goes mainstream\n",
    "   â†“\n",
    "2023: GPT-4, Claude 3, Gemini - Multimodal, longer context\n",
    "   â†“\n",
    "2024: Open source explosion (LLaMA 3, Mistral, Phi-3)\n",
    "```\n",
    "\n",
    "### Key Capabilities\n",
    "\n",
    "âœ… **Text Generation**: Write essays, code, poetry\n",
    "\n",
    "âœ… **Question Answering**: General knowledge queries\n",
    "\n",
    "âœ… **Summarization**: Condense long documents\n",
    "\n",
    "âœ… **Translation**: Between languages\n",
    "\n",
    "âœ… **Code Generation**: Write and debug code\n",
    "\n",
    "âœ… **Reasoning**: Solve problems step-by-step\n",
    "\n",
    "âœ… **Conversation**: Natural dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "!pip install -q torch transformers tiktoken matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Libraries loaded successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='how'></a>\n",
    "## 2. ğŸ§  How LLMs Work\n",
    "\n",
    "### The Big Picture\n",
    "\n",
    "```\n",
    "Input Text â†’ Tokenization â†’ Embeddings â†’ Transformer Layers â†’ Output Probabilities\n",
    "\n",
    "\"Hello world\"  â†’  [15496, 995]  â†’  [vectors]  â†’  [processing]  â†’  Next token: \"!\"\n",
    "```\n",
    "\n",
    "### Core Components\n",
    "\n",
    "1. **Tokenization**: Break text into tokens\n",
    "   ```\n",
    "   \"Hello world\" â†’ [\"Hello\", \" world\"]\n",
    "   ```\n",
    "\n",
    "2. **Embeddings**: Convert tokens to vectors\n",
    "   ```\n",
    "   \"Hello\" â†’ [0.23, -0.45, 0.67, ...] (768 numbers)\n",
    "   ```\n",
    "\n",
    "3. **Transformer Layers**: Process sequences with attention\n",
    "   - Self-attention: Which words relate to each other?\n",
    "   - Feed-forward: Transform representations\n",
    "\n",
    "4. **Prediction**: Probability distribution over vocabulary\n",
    "   ```\n",
    "   Next token probabilities:\n",
    "   \"!\" â†’ 0.45\n",
    "   \".\" â†’ 0.30\n",
    "   \",\" â†’ 0.15\n",
    "   ```\n",
    "\n",
    "### Autoregressive Generation\n",
    "\n",
    "LLMs generate text **one token at a time**, using previous tokens as context:\n",
    "\n",
    "```\n",
    "Prompt: \"The capital of France is\"\n",
    "\n",
    "Step 1: \"The capital of France is\" â†’ predict \"Paris\" (90% confidence)\n",
    "Step 2: \"The capital of France is Paris\" â†’ predict \",\" (70% confidence)  \n",
    "Step 3: \"The capital of France is Paris,\" â†’ predict \"which\" (60% confidence)\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Load a small GPT-2 model\n",
    "print(\"Loading GPT-2 (small) model...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "print(\"âœ… Model loaded!\\n\")\n",
    "\n",
    "# Model info\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model: GPT-2 (small)\")\n",
    "print(f\"Parameters: {num_params:,}\")\n",
    "print(f\"Layers: {model.config.n_layer}\")\n",
    "print(f\"Hidden size: {model.config.n_embd}\")\n",
    "print(f\"Attention heads: {model.config.n_head}\")\n",
    "print(f\"Vocabulary size: {model.config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Simple text generation\n",
    "def generate_text(prompt, max_length=50, temperature=0.7):\n",
    "    \"\"\"Generate text using GPT-2\"\"\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test it\n",
    "prompts = [\n",
    "    \"Artificial intelligence is\",\n",
    "    \"The future of technology\",\n",
    "    \"Machine learning can help\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ¤– GPT-2 Text Generation Demo\\n\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated = generate_text(prompt, max_length=40)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {generated}\\n\")\n",
    "    print(\"-\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transformer'></a>\n",
    "## 3. ğŸ—ï¸ The Transformer Architecture\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         Input Text                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚\n",
    "             â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚      Tokenization                  â”‚\n",
    "â”‚  \"Hello\" â†’ [15496]                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚\n",
    "             â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Token Embeddings                 â”‚\n",
    "â”‚   [15496] â†’ [0.2, -0.5, ...]       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚\n",
    "             â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Positional Encoding              â”‚\n",
    "â”‚   Add position information         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚\n",
    "             â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Transformer Block 1              â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚\n",
    "â”‚   â”‚  Multi-Head Attention â”‚        â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚\n",
    "â”‚              â”‚                     â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚\n",
    "â”‚   â”‚  Feed Forward Network â”‚        â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚\n",
    "             â–¼\n",
    "     [Repeat 12-96 times]\n",
    "             â”‚\n",
    "             â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Output Layer                     â”‚\n",
    "â”‚   Probability over vocabulary      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚\n",
    "             â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Next Token Prediction            â”‚\n",
    "â”‚   \"world\" (85% confidence)         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Key Innovations\n",
    "\n",
    "1. **Self-Attention**: Every token can \"look at\" every other token\n",
    "2. **Parallel Processing**: Unlike RNNs, can process all tokens simultaneously\n",
    "3. **Positional Encoding**: Knows word order without recurrence\n",
    "4. **Layer Normalization**: Stable training\n",
    "5. **Residual Connections**: Gradient flow\n",
    "\n",
    "### Model Sizes\n",
    "\n",
    "| Model | Layers | Hidden Size | Attention Heads | Parameters |\n",
    "|-------|--------|-------------|-----------------|------------|\n",
    "| GPT-2 Small | 12 | 768 | 12 | 117M |\n",
    "| GPT-2 Medium | 24 | 1024 | 16 | 345M |\n",
    "| GPT-2 Large | 36 | 1280 | 20 | 774M |\n",
    "| GPT-3 | 96 | 12288 | 96 | 175B |\n",
    "| GPT-4 | ~120 | ~18000 | ~140 | ~1.7T |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Transformer Block Implementation\n",
    "class SimpleTransformerBlock(nn.Module):\n",
    "    \"\"\"A simplified transformer block for educational purposes\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-head self-attention\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Self-attention with residual connection\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        x = self.ln1(x + attn_out)\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.ln2(x + ffn_out)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create a simple transformer block\n",
    "block = SimpleTransformerBlock(embed_dim=512, num_heads=8, ff_dim=2048)\n",
    "\n",
    "# Test with random input\n",
    "batch_size, seq_len, embed_dim = 2, 10, 512\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "output = block(x)\n",
    "\n",
    "print(\"âœ… Simple Transformer Block\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in block.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='attention'></a>\n",
    "## 4. ğŸ‘ï¸ Attention Mechanism\n",
    "\n",
    "### What is Attention?\n",
    "\n",
    "**Attention** lets the model focus on relevant parts of the input:\n",
    "\n",
    "```\n",
    "Sentence: \"The cat sat on the mat\"\n",
    "\n",
    "Processing \"sat\":\n",
    "  - High attention to \"cat\" (subject)\n",
    "  - High attention to \"mat\" (location)\n",
    "  - Low attention to \"the\" (less important)\n",
    "```\n",
    "\n",
    "### Self-Attention Formula\n",
    "\n",
    "```\n",
    "Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V\n",
    "\n",
    "Where:\n",
    "  Q = Queries (what we're looking for)\n",
    "  K = Keys (what we can match against)\n",
    "  V = Values (what we retrieve)\n",
    "  d_k = dimension of keys (for scaling)\n",
    "```\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "Instead of one attention mechanism, use multiple \"heads\":\n",
    "\n",
    "```\n",
    "Head 1: Focuses on syntactic relationships\n",
    "Head 2: Focuses on semantic meaning\n",
    "Head 3: Focuses on long-range dependencies\n",
    "...\n",
    "Head 8: Focuses on positional patterns\n",
    "\n",
    "â†’ Concatenate all heads â†’ Learn richer representations\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention patterns\n",
    "def visualize_attention(text, layer=0, head=0):\n",
    "    \"\"\"Visualize attention weights from GPT-2\"\"\"\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    \n",
    "    # Get attention weights\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_attentions=True)\n",
    "        attention = outputs.attentions[layer][0, head].numpy()\n",
    "    \n",
    "    # Get tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(attention, cmap='viridis')\n",
    "    plt.colorbar(label='Attention Weight')\n",
    "    plt.xticks(range(len(tokens)), tokens, rotation=90)\n",
    "    plt.yticks(range(len(tokens)), tokens)\n",
    "    plt.xlabel('Key Tokens')\n",
    "    plt.ylabel('Query Tokens')\n",
    "    plt.title(f'Attention Pattern (Layer {layer}, Head {head})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example\n",
    "text = \"The cat sat on the mat\"\n",
    "print(f\"Text: {text}\\n\")\n",
    "visualize_attention(text, layer=0, head=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tokenization'></a>\n",
    "## 5. ğŸ”¤ Tokenization\n",
    "\n",
    "### What is Tokenization?\n",
    "\n",
    "Breaking text into **tokens** (sub-word units):\n",
    "\n",
    "```\n",
    "Word-level: \"Hello world\" â†’ [\"Hello\", \"world\"]\n",
    "Character-level: \"Hello\" â†’ [\"H\", \"e\", \"l\", \"l\", \"o\"]\n",
    "Subword (BPE): \"HelloWorld\" â†’ [\"Hello\", \"World\"]\n",
    "```\n",
    "\n",
    "### Why Subword Tokenization?\n",
    "\n",
    "**Benefits:**\n",
    "- âœ… Handles unknown words (\"tokenization\" â†’ \"token\" + \"ization\")\n",
    "- âœ… Smaller vocabulary size\n",
    "- âœ… Better for multilingual models\n",
    "\n",
    "### Popular Algorithms\n",
    "\n",
    "1. **BPE** (Byte Pair Encoding) - GPT models\n",
    "2. **WordPiece** - BERT\n",
    "3. **SentencePiece** - T5, LLaMA\n",
    "4. **Unigram** - Alternative to BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization examples\n",
    "examples = [\n",
    "    \"Hello, world!\",\n",
    "    \"Tokenization is important\",\n",
    "    \"GPT-4 can understand complex patterns\",\n",
    "    \"ğŸ¤– AI is amazing!\",\n",
    "    \"programming\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ”¤ Tokenization Examples\\n\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "for text in examples:\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.encode(text)\n",
    "    \n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Token IDs: {token_ids}\")\n",
    "    print(f\"Count: {len(tokens)} tokens\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='training'></a>\n",
    "## 6. ğŸ‹ï¸ Training Process\n",
    "\n",
    "### Three Stages of LLM Training\n",
    "\n",
    "```\n",
    "1. PRE-TRAINING (Months, $$$$$)\n",
    "   â””â”€â”€ Train on massive text corpus (internet, books, code)\n",
    "   â””â”€â”€ Learn: grammar, facts, reasoning patterns\n",
    "   â””â”€â”€ Objective: Predict next token\n",
    "   â””â”€â”€ Cost: ~$100M for GPT-4 scale\n",
    "\n",
    "2. SUPERVISED FINE-TUNING (Days-Weeks, $$$)\n",
    "   â””â”€â”€ Train on high-quality Q&A pairs\n",
    "   â””â”€â”€ Learn: Follow instructions, helpful responses\n",
    "   â””â”€â”€ Cost: ~$100K\n",
    "\n",
    "3. RLHF (Weeks, $$)\n",
    "   â””â”€â”€ Reinforcement Learning from Human Feedback\n",
    "   â””â”€â”€ Learn: Align with human preferences\n",
    "   â””â”€â”€ Makes model helpful, harmless, honest\n",
    "```\n",
    "\n",
    "### Training Data Scale\n",
    "\n",
    "| Model | Training Tokens | Training Data |\n",
    "|-------|----------------|---------------|\n",
    "| GPT-2 | 10B | 40GB text |\n",
    "| GPT-3 | 300B | ~500GB text |\n",
    "| GPT-4 | ~13T | ~10TB text |\n",
    "| LLaMA 2 | 2T | ~2TB text |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='miniLLM'></a>\n",
    "## 9. ğŸ”¨ Building a Mini Language Model\n",
    "\n",
    "Let's build a tiny LLM from scratch to understand the concepts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Mini-LLM Implementation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MiniLLM(nn.Module):\n",
    "    \"\"\"A minimal language model for character-level text generation\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=128, num_heads=4, num_layers=2, max_len=128):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Token embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Positional embeddings\n",
    "        self.position_embedding = nn.Embedding(max_len, embed_dim)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SimpleTransformerBlock(embed_dim, num_heads, embed_dim * 4)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output layer\n",
    "        self.ln_f = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # Token + position embeddings\n",
    "        tok_emb = self.token_embedding(idx)  # (B, T, embed_dim)\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=idx.device))  # (T, embed_dim)\n",
    "        x = tok_emb + pos_emb  # (B, T, embed_dim)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Output\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)  # (B, T, vocab_size)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
    "        \"\"\"Generate new tokens autoregressively\"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop context if too long\n",
    "            idx_cond = idx if idx.size(1) <= self.max_len else idx[:, -self.max_len:]\n",
    "            \n",
    "            # Get predictions\n",
    "            logits = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature  # Last token\n",
    "            \n",
    "            # Sample\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "        \n",
    "        return idx\n",
    "\n",
    "print(\"âœ… MiniLLM class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on a tiny dataset (Shakespeare-style)\n",
    "text = \"\"\"To be or not to be, that is the question.\n",
    "Whether 'tis nobler in the mind to suffer\n",
    "The slings and arrows of outrageous fortune,\n",
    "Or to take arms against a sea of troubles.\"\"\"\n",
    "\n",
    "# Create character-level vocabulary\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Encode text\n",
    "encode = lambda s: [char_to_idx[c] for c in s]\n",
    "decode = lambda l: ''.join([idx_to_char[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(f\"Text length: {len(text)} characters\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"First 50 characters: {text[:50]}\")\n",
    "print(f\"Encoded: {data[:50].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and test model\n",
    "model_mini = MiniLLM(vocab_size=vocab_size, embed_dim=64, num_heads=4, num_layers=2)\n",
    "\n",
    "print(\"ğŸ¤– MiniLLM Architecture:\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model_mini.parameters()):,}\")\n",
    "print(f\"Vocabulary: {vocab_size} characters\")\n",
    "\n",
    "# Generate before training (random)\n",
    "context = torch.tensor([encode(\"To be\")], dtype=torch.long)\n",
    "with torch.no_grad():\n",
    "    generated = model_mini.generate(context, max_new_tokens=50, temperature=1.0)\n",
    "    \n",
    "print(\"\\nğŸ² Before training (random):\")\n",
    "print(decode(generated[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='apis'></a>\n",
    "## 10. ğŸŒ Using LLM APIs\n",
    "\n",
    "### Popular LLM APIs\n",
    "\n",
    "| Provider | Model | Pricing | Best For |\n",
    "|----------|-------|---------|----------|\n",
    "| OpenAI | GPT-4 | $0.03/1K tokens | General purpose |\n",
    "| Anthropic | Claude 3 | $0.015/1K tokens | Long context |\n",
    "| Google | Gemini | Free tier | Multimodal |\n",
    "| Cohere | Command | $0.001/1K tokens | Cost-effective |\n",
    "| Together AI | LLaMA 3 | $0.0006/1K tokens | Open source |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using OpenAI API (requires API key)\n",
    "# Uncomment to use\n",
    "\n",
    "# import openai\n",
    "# import os\n",
    "\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# def chat_with_gpt(prompt, model=\"gpt-4\"):\n",
    "#     response = openai.ChatCompletion.create(\n",
    "#         model=model,\n",
    "#         messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#         temperature=0.7,\n",
    "#         max_tokens=150\n",
    "#     )\n",
    "#     return response.choices[0].message.content\n",
    "\n",
    "# # Test\n",
    "# answer = chat_with_gpt(\"Explain what a large language model is in simple terms\")\n",
    "# print(answer)\n",
    "\n",
    "print(\"ğŸ’¡ To use LLM APIs:\")\n",
    "print(\"1. Get API key from provider\")\n",
    "print(\"2. Set environment variable: export OPENAI_API_KEY='your-key'\")\n",
    "print(\"3. Uncomment and run the code above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‰ Conclusion\n",
    "\n",
    "You've learned:\n",
    "\n",
    "âœ… What LLMs are and how they work\n",
    "\n",
    "âœ… Transformer architecture and attention\n",
    "\n",
    "âœ… Tokenization and embeddings\n",
    "\n",
    "âœ… Training process (pre-training, fine-tuning, RLHF)\n",
    "\n",
    "âœ… Built a mini LLM from scratch\n",
    "\n",
    "âœ… How to use LLM APIs\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Fine-tune a model on custom data\n",
    "2. Build applications with LLM APIs\n",
    "3. Learn prompt engineering\n",
    "4. Explore RAG (Retrieval Augmented Generation)\n",
    "5. Study model efficiency (quantization, distillation)\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Attention is All You Need](https://arxiv.org/abs/1706.03762) - Original Transformer paper\n",
    "- [GPT-3 Paper](https://arxiv.org/abs/2005.14165)\n",
    "- [Hugging Face Course](https://huggingface.co/course)\n",
    "- [Karpathy's Neural Networks](https://karpathy.ai/zero-to-hero.html)\n",
    "\n",
    "**Happy learning! ğŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
