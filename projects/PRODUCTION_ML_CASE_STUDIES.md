# Production ML Case Studies: Real-World Applications

**Complete end-to-end ML solutions for real-world business problems**

> ðŸ¢ **5 Industry Case Studies** | ðŸ’¼ **Production Architecture** | ðŸ“Š **Business Impact** | ðŸš€ **Deployment Strategies**

---

## Table of Contents

1. [Case Study 1: E-Commerce Product Recommendation System](#case-study-1-e-commerce-recommendation)
2. [Case Study 2: Financial Fraud Detection](#case-study-2-fraud-detection)
3. [Case Study 3: Predictive Maintenance for Manufacturing](#case-study-3-predictive-maintenance)
4. [Case Study 4: Customer Churn Prediction](#case-study-4-customer-churn)
5. [Case Study 5: Real-Time Content Moderation](#case-study-5-content-moderation)

Each case study includes:
- âœ… Business problem definition
- âœ… Data architecture
- âœ… Model selection and implementation
- âœ… Production system design
- âœ… Monitoring and maintenance
- âœ… Business metrics and ROI
- âœ… Lessons learned

---

## Case Study 1: E-Commerce Product Recommendation System

### 1.1 Business Context

**Company:** Medium-sized e-commerce platform (5M monthly active users, 100K products)

**Business Problem:**
- Low conversion rate (2.5%)
- Poor user engagement (avg 3 mins session time)
- High cart abandonment (70%)
- Difficulty discovering relevant products

**Success Metrics:**
- Increase click-through rate (CTR) by 20%
- Increase conversion rate by 15%
- Increase average order value by 10%
- Reduce cart abandonment by 15%

**Timeline:** 3 months from inception to production

**Team:** 2 ML engineers, 1 backend engineer, 1 product manager

---

### 1.2 Solution Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     User Interaction Layer                   â”‚
â”‚  (Web/Mobile App â†’ API Gateway â†’ Load Balancer)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Real-time Serving Layer                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Feature Storeâ”‚  â”‚ Model Server â”‚  â”‚ A/B Testing  â”‚      â”‚
â”‚  â”‚   (Redis)    â”‚  â”‚  (TensorFlow â”‚  â”‚   Service    â”‚      â”‚
â”‚  â”‚              â”‚  â”‚   Serving)   â”‚  â”‚              â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Offline Training Pipeline                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Data Pipelineâ”‚  â”‚Model Trainingâ”‚  â”‚  Evaluation  â”‚      â”‚
â”‚  â”‚  (Airflow)   â”‚  â”‚  (PyTorch)   â”‚  â”‚   Pipeline   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Layer                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  User Events â”‚  â”‚  Product DB  â”‚  â”‚  Historical  â”‚      â”‚
â”‚  â”‚  (Kafka)     â”‚  â”‚ (PostgreSQL) â”‚  â”‚  Data (S3)   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 1.3 Data Architecture

**Data Sources:**

1. **User Interaction Events** (streaming via Kafka)
```json
{
  "event_type": "product_view",
  "user_id": "user_12345",
  "product_id": "prod_67890",
  "timestamp": "2025-10-19T10:30:00Z",
  "session_id": "session_abc123",
  "device_type": "mobile",
  "context": {
    "category": "electronics",
    "price": 299.99,
    "search_query": "wireless headphones"
  }
}
```

2. **Product Catalog** (PostgreSQL)
```sql
CREATE TABLE products (
    product_id VARCHAR(50) PRIMARY KEY,
    name VARCHAR(255),
    category VARCHAR(100),
    subcategory VARCHAR(100),
    brand VARCHAR(100),
    price DECIMAL(10, 2),
    stock_quantity INTEGER,
    features JSONB,
    created_at TIMESTAMP,
    updated_at TIMESTAMP
);
```

3. **User Profiles**
```sql
CREATE TABLE users (
    user_id VARCHAR(50) PRIMARY KEY,
    demographics JSONB,
    preferences JSONB,
    lifetime_value DECIMAL(10, 2),
    segment VARCHAR(50),
    created_at TIMESTAMP
);
```

**Feature Engineering Pipeline:**

```python
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

class FeatureEngineer:
    """
    Feature engineering for recommendation system.
    """

    def __init__(self, lookback_days=30):
        self.lookback_days = lookback_days

    def create_user_features(self, user_events, user_profile):
        """
        Create user-level features.

        Features:
        - Engagement metrics (clicks, views, purchases)
        - Category preferences
        - Price sensitivity
        - Temporal patterns (time of day, day of week)
        - Session behavior
        """
        features = {}

        # Engagement metrics (last 30 days)
        recent_events = user_events[
            user_events['timestamp'] >= datetime.now() - timedelta(days=self.lookback_days)
        ]

        features['total_views'] = len(recent_events[recent_events['event_type'] == 'view'])
        features['total_clicks'] = len(recent_events[recent_events['event_type'] == 'click'])
        features['total_purchases'] = len(recent_events[recent_events['event_type'] == 'purchase'])
        features['total_add_to_cart'] = len(recent_events[recent_events['event_type'] == 'add_to_cart'])

        # Conversion rate
        features['click_through_rate'] = (
            features['total_clicks'] / max(features['total_views'], 1)
        )
        features['purchase_rate'] = (
            features['total_purchases'] / max(features['total_clicks'], 1)
        )

        # Category preferences (top 5 categories by interaction)
        category_counts = recent_events.groupby('category').size()
        top_categories = category_counts.nlargest(5)

        for i, (category, count) in enumerate(top_categories.items()):
            features[f'top_category_{i+1}'] = category
            features[f'top_category_{i+1}_count'] = count

        # Price sensitivity
        if features['total_purchases'] > 0:
            purchase_prices = recent_events[
                recent_events['event_type'] == 'purchase'
            ]['price']
            features['avg_purchase_price'] = purchase_prices.mean()
            features['median_purchase_price'] = purchase_prices.median()
            features['price_std'] = purchase_prices.std()
        else:
            features['avg_purchase_price'] = 0
            features['median_purchase_price'] = 0
            features['price_std'] = 0

        # Temporal patterns
        hour_distribution = recent_events['timestamp'].dt.hour.value_counts(normalize=True)
        features['peak_hour'] = hour_distribution.idxmax()
        features['hour_concentration'] = hour_distribution.max()

        dow_distribution = recent_events['timestamp'].dt.dayofweek.value_counts(normalize=True)
        features['peak_day'] = dow_distribution.idxmax()

        # Session behavior
        sessions = recent_events.groupby('session_id')
        features['avg_session_length'] = sessions.size().mean()
        features['avg_products_per_session'] = sessions['product_id'].nunique().mean()

        # Recency
        if len(recent_events) > 0:
            features['days_since_last_visit'] = (
                datetime.now() - recent_events['timestamp'].max()
            ).days
        else:
            features['days_since_last_visit'] = 999

        # User profile features
        features['user_segment'] = user_profile.get('segment', 'unknown')
        features['lifetime_value'] = user_profile.get('lifetime_value', 0)
        features['account_age_days'] = (
            datetime.now() - pd.to_datetime(user_profile['created_at'])
        ).days

        return features

    def create_product_features(self, product, product_stats):
        """
        Create product-level features.

        Features:
        - Product attributes (category, brand, price)
        - Popularity metrics
        - Engagement rates
        - Seasonal trends
        """
        features = {}

        # Product attributes
        features['category'] = product['category']
        features['subcategory'] = product['subcategory']
        features['brand'] = product['brand']
        features['price'] = product['price']
        features['log_price'] = np.log1p(product['price'])
        features['in_stock'] = 1 if product['stock_quantity'] > 0 else 0

        # Popularity metrics (from stats table)
        features['total_views_30d'] = product_stats.get('views_30d', 0)
        features['total_purchases_30d'] = product_stats.get('purchases_30d', 0)
        features['conversion_rate'] = (
            product_stats.get('purchases_30d', 0) /
            max(product_stats.get('views_30d', 1), 1)
        )

        # Velocity (trending)
        features['views_7d'] = product_stats.get('views_7d', 0)
        features['views_1d'] = product_stats.get('views_1d', 0)
        features['trending_score'] = (
            features['views_7d'] / max(features['total_views_30d'], 1)
        )

        # Revenue
        features['revenue_30d'] = (
            product_stats.get('purchases_30d', 0) * product['price']
        )

        # Product age
        features['product_age_days'] = (
            datetime.now() - pd.to_datetime(product['created_at'])
        ).days

        return features

    def create_interaction_features(self, user_features, product_features):
        """
        Create user-product interaction features.
        """
        features = {}

        # Price affinity
        features['price_diff_from_avg'] = (
            product_features['price'] - user_features['avg_purchase_price']
        )
        features['price_ratio_to_avg'] = (
            product_features['price'] / max(user_features['avg_purchase_price'], 1)
        )

        # Category match
        for i in range(1, 6):
            cat_key = f'top_category_{i}'
            if cat_key in user_features:
                if user_features[cat_key] == product_features['category']:
                    features[f'category_match_rank_{i}'] = 1
                else:
                    features[f'category_match_rank_{i}'] = 0

        # Engagement likelihood (user engagement * product popularity)
        features['engagement_score'] = (
            user_features['click_through_rate'] *
            product_features['conversion_rate']
        )

        return features
```

---

### 1.4 Model Architecture

**Hybrid Recommendation System:**

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class HybridRecommender(nn.Module):
    """
    Hybrid recommendation model combining:
    1. Collaborative Filtering (Matrix Factorization)
    2. Content-Based (features)
    3. Context-Aware (session, time)
    """

    def __init__(self, num_users, num_products, embedding_dim=128,
                 content_dim=50, hidden_dims=[256, 128, 64]):
        super(HybridRecommender, self).__init__()

        # === Collaborative Filtering Component ===
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        self.product_embedding = nn.Embedding(num_products, embedding_dim)

        # === Content-Based Component ===
        # Product content encoder
        self.product_content_encoder = nn.Sequential(
            nn.Linear(content_dim, 128),
            nn.ReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.3),
            nn.Linear(128, embedding_dim)
        )

        # User preference encoder
        self.user_preference_encoder = nn.Sequential(
            nn.Linear(content_dim, 128),
            nn.ReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.3),
            nn.Linear(128, embedding_dim)
        )

        # === Context-Aware Component ===
        # Time and session features
        self.context_encoder = nn.Sequential(
            nn.Linear(20, 64),  # hour, day, session length, etc.
            nn.ReLU(),
            nn.Linear(64, 32)
        )

        # === Fusion Network ===
        # Combine all components
        fusion_input_dim = embedding_dim * 3 + 32  # CF + content + context

        layers = []
        prev_dim = fusion_input_dim

        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU(),
                nn.BatchNorm1d(hidden_dim),
                nn.Dropout(0.3)
            ])
            prev_dim = hidden_dim

        layers.append(nn.Linear(prev_dim, 1))  # Final prediction

        self.fusion_network = nn.Sequential(*layers)

        # Initialize weights
        self._init_weights()

    def _init_weights(self):
        """Initialize embeddings and weights."""
        nn.init.normal_(self.user_embedding.weight, std=0.01)
        nn.init.normal_(self.product_embedding.weight, std=0.01)

        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)

    def forward(self, user_ids, product_ids, user_content, product_content, context):
        """
        Forward pass.

        Args:
            user_ids: (batch,) - user indices
            product_ids: (batch,) - product indices
            user_content: (batch, content_dim) - user content features
            product_content: (batch, content_dim) - product content features
            context: (batch, 20) - contextual features

        Returns:
            scores: (batch, 1) - predicted interaction scores
        """
        # Collaborative filtering embeddings
        user_emb = self.user_embedding(user_ids)  # (batch, embedding_dim)
        product_emb = self.product_embedding(product_ids)  # (batch, embedding_dim)
        cf_features = user_emb * product_emb  # Element-wise product

        # Content-based features
        user_content_emb = self.user_preference_encoder(user_content)
        product_content_emb = self.product_content_encoder(product_content)
        content_features = user_content_emb * product_content_emb

        # Context features
        context_features = self.context_encoder(context)

        # Concatenate all features
        combined = torch.cat([
            cf_features,
            content_features,
            context_features
        ], dim=1)

        # Final prediction
        scores = self.fusion_network(combined)

        return scores

    def predict_top_k(self, user_id, candidate_products, user_content,
                      products_content, context, k=10):
        """
        Predict top-k products for a user.

        Args:
            user_id: Single user ID
            candidate_products: List of product IDs to rank
            user_content: User content features
            products_content: Product content features for candidates
            context: Context features
            k: Number of recommendations

        Returns:
            top_k_products: List of (product_id, score) tuples
        """
        self.eval()

        with torch.no_grad():
            # Repeat user features for all candidates
            batch_size = len(candidate_products)
            user_ids = torch.full((batch_size,), user_id, dtype=torch.long)
            product_ids = torch.tensor(candidate_products, dtype=torch.long)

            # Expand user features
            user_content_batch = user_content.unsqueeze(0).expand(batch_size, -1)
            context_batch = context.unsqueeze(0).expand(batch_size, -1)

            # Predict scores
            scores = self(user_ids, product_ids, user_content_batch,
                         products_content, context_batch)

            # Get top-k
            top_k_scores, top_k_indices = torch.topk(scores.squeeze(), k)

            top_k_products = [
                (candidate_products[idx], score.item())
                for idx, score in zip(top_k_indices, top_k_scores)
            ]

            return top_k_products
```

---

### 1.5 Training Pipeline

```python
import torch
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np

class RecommendationDataset(Dataset):
    """
    Dataset for training recommendation model.
    """

    def __init__(self, interactions, user_features, product_features,
                 context_features, negative_samples=4):
        """
        Args:
            interactions: DataFrame with (user_id, product_id, label)
            user_features: Dict mapping user_id to features
            product_features: Dict mapping product_id to features
            context_features: Dict mapping interaction_id to context
            negative_samples: Number of negative samples per positive
        """
        self.interactions = interactions
        self.user_features = user_features
        self.product_features = product_features
        self.context_features = context_features
        self.negative_samples = negative_samples

        # Build product pool for negative sampling
        self.all_products = set(product_features.keys())

        # Create samples (positive + negatives)
        self.samples = self._create_samples()

    def _create_samples(self):
        """Create training samples with negative sampling."""
        samples = []

        for idx, row in self.interactions.iterrows():
            user_id = row['user_id']
            product_id = row['product_id']
            label = row['label']  # 1 for positive interaction

            # Positive sample
            samples.append({
                'user_id': user_id,
                'product_id': product_id,
                'label': label
            })

            # Negative samples (if positive interaction)
            if label == 1:
                user_products = set(self.interactions[
                    self.interactions['user_id'] == user_id
                ]['product_id'])

                neg_pool = list(self.all_products - user_products)
                neg_samples = np.random.choice(
                    neg_pool,
                    size=min(self.negative_samples, len(neg_pool)),
                    replace=False
                )

                for neg_product in neg_samples:
                    samples.append({
                        'user_id': user_id,
                        'product_id': neg_product,
                        'label': 0
                    })

        return samples

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]

        user_id = sample['user_id']
        product_id = sample['product_id']
        label = sample['label']

        # Get features
        user_content = torch.FloatTensor(self.user_features[user_id])
        product_content = torch.FloatTensor(self.product_features[product_id])
        context = torch.FloatTensor(self.context_features.get(idx, np.zeros(20)))

        return {
            'user_id': user_id,
            'product_id': product_id,
            'user_content': user_content,
            'product_content': product_content,
            'context': context,
            'label': torch.FloatTensor([label])
        }


def train_recommender(model, train_loader, val_loader, config):
    """
    Train recommendation model.
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    # Loss and optimizer
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.5, patience=3
    )

    best_ndcg = 0.0

    for epoch in range(config['epochs']):
        # Training
        model.train()
        train_loss = 0.0

        for batch in train_loader:
            user_ids = batch['user_id'].to(device)
            product_ids = batch['product_id'].to(device)
            user_content = batch['user_content'].to(device)
            product_content = batch['product_content'].to(device)
            context = batch['context'].to(device)
            labels = batch['label'].to(device)

            # Forward pass
            scores = model(user_ids, product_ids, user_content,
                          product_content, context)
            loss = criterion(scores, labels)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            train_loss += loss.item()

        avg_train_loss = train_loss / len(train_loader)

        # Validation
        val_metrics = evaluate_recommender(model, val_loader, device)

        print(f"Epoch {epoch+1}/{config['epochs']}")
        print(f"Train Loss: {avg_train_loss:.4f}")
        print(f"Val NDCG@10: {val_metrics['ndcg@10']:.4f}")
        print(f"Val Precision@10: {val_metrics['precision@10']:.4f}")
        print(f"Val Recall@10: {val_metrics['recall@10']:.4f}")

        # Learning rate scheduling
        scheduler.step(val_metrics['ndcg@10'])

        # Save best model
        if val_metrics['ndcg@10'] > best_ndcg:
            best_ndcg = val_metrics['ndcg@10']
            torch.save(model.state_dict(), 'best_recommender.pth')
            print(f"âœ“ Saved best model (NDCG@10: {best_ndcg:.4f})")


def evaluate_recommender(model, data_loader, device, k=10):
    """
    Evaluate recommendation model.

    Metrics:
    - NDCG@k: Normalized Discounted Cumulative Gain
    - Precision@k: Fraction of relevant items in top-k
    - Recall@k: Fraction of relevant items retrieved
    """
    model.eval()

    ndcg_scores = []
    precision_scores = []
    recall_scores = []

    with torch.no_grad():
        for batch in data_loader:
            # ... evaluation logic
            pass

    return {
        'ndcg@10': np.mean(ndcg_scores),
        'precision@10': np.mean(precision_scores),
        'recall@10': np.mean(recall_scores)
    }
```

---

### 1.6 Production Deployment

**Serving Architecture:**

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import redis
import torch
import json
from typing import List, Dict

app = FastAPI()

# Initialize Redis for caching
redis_client = redis.Redis(host='localhost', port=6379, db=0)

# Load model
model = HybridRecommender(num_users=5000000, num_products=100000)
model.load_state_dict(torch.load('best_recommender.pth'))
model.eval()

class RecommendationRequest(BaseModel):
    user_id: int
    context: Dict
    num_recommendations: int = 10
    filter_purchased: bool = True

class RecommendationResponse(BaseModel):
    user_id: int
    recommendations: List[Dict]
    latency_ms: float
    from_cache: bool

@app.post("/recommend", response_model=RecommendationResponse)
async def get_recommendations(request: RecommendationRequest):
    """
    Get personalized product recommendations.

    SLA: < 100ms p99 latency
    """
    import time
    start_time = time.time()

    # Check cache
    cache_key = f"rec:{request.user_id}:{request.num_recommendations}"
    cached = redis_client.get(cache_key)

    if cached:
        recommendations = json.loads(cached)
        latency_ms = (time.time() - start_time) * 1000
        return RecommendationResponse(
            user_id=request.user_id,
            recommendations=recommendations,
            latency_ms=latency_ms,
            from_cache=True
        )

    # Generate recommendations
    try:
        # Get candidate products (pre-filtered by business rules)
        candidates = get_candidate_products(request.user_id, request.filter_purchased)

        # Get features
        user_features = get_user_features(request.user_id)
        product_features = get_product_features(candidates)
        context_features = encode_context(request.context)

        # Predict
        recommendations = model.predict_top_k(
            user_id=request.user_id,
            candidate_products=candidates,
            user_content=user_features,
            products_content=product_features,
            context=context_features,
            k=request.num_recommendations
        )

        # Format response
        formatted_recs = [
            {
                'product_id': prod_id,
                'score': float(score),
                'product_info': get_product_info(prod_id)
            }
            for prod_id, score in recommendations
        ]

        # Cache for 1 hour
        redis_client.setex(cache_key, 3600, json.dumps(formatted_recs))

        latency_ms = (time.time() - start_time) * 1000

        return RecommendationResponse(
            user_id=request.user_id,
            recommendations=formatted_recs,
            latency_ms=latency_ms,
            from_cache=False
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# Monitoring endpoint
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "redis_connected": redis_client.ping()
    }
```

---

### 1.7 Monitoring and Metrics

**Key Metrics to Track:**

```python
class RecommendationMetrics:
    """
    Production monitoring for recommendation system.
    """

    def __init__(self):
        self.metrics = {
            # Business metrics
            'ctr': [],  # Click-through rate
            'conversion_rate': [],
            'revenue_per_recommendation': [],

            # ML metrics
            'avg_prediction_score': [],
            'score_distribution': [],

            # System metrics
            'latency_p50': [],
            'latency_p95': [],
            'latency_p99': [],
            'cache_hit_rate': [],
            'error_rate': []
        }

    def log_recommendation_shown(self, user_id, product_ids, scores):
        """Log when recommendations are shown to user."""
        self.metrics['avg_prediction_score'].append(np.mean(scores))

    def log_recommendation_clicked(self, user_id, product_id, position):
        """Log when user clicks a recommendation."""
        # Calculate CTR for this position
        pass

    def log_purchase(self, user_id, product_id, revenue, from_recommendation):
        """Log purchase event."""
        if from_recommendation:
            self.metrics['revenue_per_recommendation'].append(revenue)

    def log_latency(self, latency_ms):
        """Log serving latency."""
        self.metrics['latency_p99'].append(latency_ms)

    def calculate_metrics(self, window='1h'):
        """
        Calculate aggregated metrics.
        """
        return {
            'ctr': self._calculate_ctr(window),
            'conversion_rate': self._calculate_conversion_rate(window),
            'avg_latency_p99': np.percentile(self.metrics['latency_p99'], 99),
            'cache_hit_rate': self._calculate_cache_hit_rate(window)
        }
```

---

### 1.8 Results and Business Impact

**A/B Test Results (4 weeks):**

| Metric | Control | Treatment | Improvement |
|--------|---------|-----------|-------------|
| CTR | 2.1% | 2.8% | **+33%** âœ… |
| Conversion Rate | 2.5% | 3.1% | **+24%** âœ… |
| AOV | $45 | $51 | **+13%** âœ… |
| Revenue per User | $1.13 | $1.58 | **+40%** âœ… |
| Session Time | 3.2 min | 4.5 min | **+41%** |

**Technical Performance:**

- p99 Latency: **78ms** (target: <100ms) âœ…
- Model Size: **450MB** (compressed)
- Training Time: **6 hours** on 4x V100 GPUs
- Cache Hit Rate: **85%**
- Uptime: **99.95%**

**Business Impact:**

- **Additional Revenue**: $2.3M/month
- **ROI**: 15x (implementation cost vs. revenue increase)
- **User Engagement**: +40% session time, +25% pages per session

---

### 1.9 Lessons Learned

**What Worked Well:**

1. âœ… **Hybrid approach** combining CF + content + context significantly outperformed individual methods
2. âœ… **Negative sampling** during training improved discrimination between good and bad recommendations
3. âœ… **Redis caching** reduced latency and database load by 85%
4. âœ… **A/B testing framework** enabled data-driven decisions
5. âœ… **Feature store** simplified feature engineering and ensured consistency

**Challenges:**

1. âš ï¸ **Cold start problem** for new users/products
   - **Solution**: Fallback to popularity-based recommendations + quick onboarding quiz

2. âš ï¸ **Model staleness** - user preferences change
   - **Solution**: Retrain daily, use online learning for critical users

3. âš ï¸ **Diversity vs. relevance tradeoff**
   - **Solution**: DPP (Determinantal Point Process) for diversification

4. âš ï¸ **Filter bubble** - users only see similar products
   - **Solution**: Exploration bonus (Îµ-greedy) + serendipity injection

5. âš ï¸ **Seasonal variations**
   - **Solution**: Time-aware features + seasonal model variants

**Best Practices:**

1. ðŸ“Š **Start with strong baselines** (popularity, collaborative filtering)
2. ðŸ”„ **Iterate quickly** with A/B tests
3. ðŸ“ˆ **Monitor business metrics**, not just ML metrics
4. ðŸ›¡ï¸ **Build fallbacks** for model failures
5. ðŸ” **Explain recommendations** to build user trust
6. âš¡ **Optimize for latency** from day one
7. ðŸ“Š **Track everything** - you can't improve what you don't measure

---

**[Continue to Case Study 2: Financial Fraud Detection...]**
