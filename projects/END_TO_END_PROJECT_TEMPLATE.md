# End-to-End Production ML Project Template

**Complete blueprint for building production-ready ML systems from scratch**

> üéØ **Portfolio-Ready** | üèóÔ∏è **Production-Grade** | üìä **Full Pipeline** | üöÄ **Deployable** | üìà **Monitored**

---

## Project: E-Commerce Product Recommendation System

**Goal:** Build a complete production ML system that recommends products to users based on their behavior.

**Target:** Deploy a real-time API serving personalized recommendations with <100ms latency, handling 1000+ requests/second.

---

## Table of Contents

1. [Project Structure](#project-structure)
2. [Setup & Prerequisites](#setup-prerequisites)
3. [Phase 1: Data Pipeline](#phase-1-data-pipeline)
4. [Phase 2: Feature Engineering](#phase-2-feature-engineering)
5. [Phase 3: Model Development](#phase-3-model-development)
6. [Phase 4: Model Serving](#phase-4-model-serving)
7. [Phase 5: Monitoring & Operations](#phase-5-monitoring-operations)
8. [Phase 6: Deployment](#phase-6-deployment)
9. [Testing Strategy](#testing-strategy)
10. [Portfolio Presentation](#portfolio-presentation)

---

## 1. Project Structure

```
recommendation-system/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                    # Raw event data
‚îÇ   ‚îú‚îÄ‚îÄ processed/              # Processed features
‚îÇ   ‚îî‚îÄ‚îÄ synthetic_generator.py  # Generate synthetic data
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ingestion.py        # Data ingestion from sources
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ preprocessing.py    # Data cleaning
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ features/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ feature_store.py    # Feast feature definitions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ user_features.py    # User feature engineering
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ product_features.py # Product feature engineering
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ collaborative_filtering.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ content_based.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ hybrid.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ serving/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api.py              # FastAPI serving
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ predictor.py        # Model inference
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ monitoring/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ metrics.py          # Prometheus metrics
‚îÇ       ‚îî‚îÄ‚îÄ drift_detection.py  # Data drift monitoring
‚îÇ
‚îú‚îÄ‚îÄ pipelines/
‚îÇ   ‚îú‚îÄ‚îÄ training_pipeline.py    # Airflow DAG for training
‚îÇ   ‚îî‚îÄ‚îÄ feature_pipeline.py     # Feature computation pipeline
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_EDA.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 02_Feature_Engineering.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 03_Model_Development.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 04_Model_Evaluation.ipynb
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_features.py
‚îÇ   ‚îú‚îÄ‚îÄ test_models.py
‚îÇ   ‚îî‚îÄ‚îÄ test_api.py
‚îÇ
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ feature_store.yaml
‚îÇ   ‚îú‚îÄ‚îÄ model_config.yaml
‚îÇ   ‚îî‚îÄ‚îÄ prometheus.yml
‚îÇ
‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.api
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.training
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml
‚îÇ
‚îú‚îÄ‚îÄ k8s/
‚îÇ   ‚îú‚îÄ‚îÄ deployment.yaml
‚îÇ   ‚îî‚îÄ‚îÄ service.yaml
‚îÇ
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îú‚îÄ‚îÄ ci.yml
‚îÇ       ‚îî‚îÄ‚îÄ cd.yml
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ setup.py
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ Makefile
```

---

## 2. Setup & Prerequisites

### Installation

```bash
# Clone template
git clone https://github.com/yourusername/recommendation-system
cd recommendation-system

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install package in development mode
pip install -e .
```

### Requirements

```txt
# requirements.txt

# Core ML
pandas>=2.0.0
numpy>=1.24.0
scikit-learn>=1.3.0
torch>=2.0.0

# Feature Store
feast>=0.30.0

# API & Serving
fastapi>=0.100.0
uvicorn>=0.23.0
pydantic>=2.0.0

# Monitoring
prometheus-client>=0.17.0
evidently>=0.4.0

# Data Processing
pyarrow>=12.0.0

# Streaming (optional)
kafka-python>=2.0.0
redis>=4.5.0

# Orchestration
apache-airflow>=2.6.0  # Optional

# Testing
pytest>=7.4.0
pytest-cov>=4.1.0

# Utilities
python-dotenv>=1.0.0
click>=8.1.0
```

---

## 3. Phase 1: Data Pipeline

### Generate Synthetic Data

```python
# data/synthetic_generator.py
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import random

class SyntheticDataGenerator:
    """Generate realistic e-commerce event data"""

    def __init__(self, n_users=10000, n_products=1000, n_days=90):
        self.n_users = n_users
        self.n_products = n_products
        self.n_days = n_days
        self.start_date = datetime.now() - timedelta(days=n_days)

    def generate_events(self):
        """Generate user-product interaction events"""
        events = []

        # Product categories
        categories = ['Electronics', 'Clothing', 'Books', 'Home', 'Sports']

        # Products
        products = pd.DataFrame({
            'product_id': range(self.n_products),
            'category': np.random.choice(categories, self.n_products),
            'price': np.random.lognormal(mean=3, sigma=1, size=self.n_products),
            'rating': np.random.uniform(3, 5, self.n_products)
        })

        # Generate events
        for day in range(self.n_days):
            date = self.start_date + timedelta(days=day)

            # Active users per day (varies)
            n_active = int(self.n_users * np.random.uniform(0.05, 0.15))
            active_users = np.random.choice(self.n_users, n_active, replace=False)

            for user_id in active_users:
                # Each user generates 1-10 events
                n_events = np.random.randint(1, 11)

                for _ in range(n_events):
                    event_type = np.random.choice(
                        ['view', 'click', 'add_to_cart', 'purchase'],
                        p=[0.6, 0.25, 0.1, 0.05]
                    )

                    product_id = np.random.choice(self.n_products)

                    event = {
                        'timestamp': date + timedelta(
                            hours=np.random.randint(0, 24),
                            minutes=np.random.randint(0, 60)
                        ),
                        'user_id': int(user_id),
                        'product_id': int(product_id),
                        'event_type': event_type,
                        'category': products.loc[product_id, 'category'],
                        'price': products.loc[product_id, 'price'],
                    }

                    events.append(event)

        return pd.DataFrame(events), products

    def save_data(self, output_dir='data/raw'):
        """Generate and save data"""
        events_df, products_df = self.generate_events()

        events_df.to_parquet(f'{output_dir}/user_events.parquet', index=False)
        products_df.to_parquet(f'{output_dir}/products.parquet', index=False)

        print(f"Generated {len(events_df):,} events for {self.n_users:,} users")
        print(f"Generated {len(products_df):,} products")

        return events_df, products_df

# Usage
if __name__ == "__main__":
    generator = SyntheticDataGenerator(n_users=10000, n_products=1000, n_days=90)
    events_df, products_df = generator.save_data()
```

---

## 4. Phase 2: Feature Engineering

### Define Features in Feast

```python
# src/features/feature_store.py
from feast import Entity, FeatureView, Field, FileSource
from feast.types import Int64, Float64, String
from datetime import timedelta

# Entities
user = Entity(
    name="user",
    join_keys=["user_id"],
    description="User entity"
)

product = Entity(
    name="product",
    join_keys=["product_id"],
    description="Product entity"
)

# User features source
user_features_source = FileSource(
    path="data/processed/user_features.parquet",
    timestamp_field="event_timestamp",
)

# User feature view
user_features_fv = FeatureView(
    name="user_features",
    entities=[user],
    ttl=timedelta(days=30),
    schema=[
        Field(name="user_id", dtype=Int64),
        Field(name="total_views", dtype=Int64),
        Field(name="total_purchases", dtype=Int64),
        Field(name="avg_purchase_value", dtype=Float64),
        Field(name="days_since_last_purchase", dtype=Int64),
        Field(name="favorite_category", dtype=String),
        Field(name="event_timestamp", dtype=Int64),
    ],
    online=True,
    source=user_features_source,
    tags={"team": "ml"}
)

# Product features source
product_features_source = FileSource(
    path="data/processed/product_features.parquet",
    timestamp_field="event_timestamp",
)

# Product feature view
product_features_fv = FeatureView(
    name="product_features",
    entities=[product],
    ttl=timedelta(days=7),
    schema=[
        Field(name="product_id", dtype=Int64),
        Field(name="total_views", dtype=Int64),
        Field(name="total_purchases", dtype=Int64),
        Field(name="purchase_rate", dtype=Float64),
        Field(name="avg_rating", dtype=Float64),
        Field(name="category", dtype=String),
        Field(name="event_timestamp", dtype=Int64),
    ],
    online=True,
    source=product_features_source,
    tags={"team": "ml"}
)
```

### Compute Features

```python
# src/features/user_features.py
import pandas as pd
from datetime import datetime, timedelta

def compute_user_features(events_df):
    """Compute user behavioral features"""

    # Filter to last 30 days
    cutoff_date = datetime.now() - timedelta(days=30)
    recent_events = events_df[events_df['timestamp'] >= cutoff_date]

    # Aggregate features
    user_features = recent_events.groupby('user_id').agg({
        'event_type': 'count',  # total_views
        'timestamp': 'max',
        'price': 'mean',
        'category': lambda x: x.mode()[0] if len(x) > 0 else 'Unknown'
    }).rename(columns={
        'event_type': 'total_views',
        'timestamp': 'last_event_time',
        'price': 'avg_viewed_price',
        'category': 'favorite_category'
    })

    # Purchase-specific features
    purchases = recent_events[recent_events['event_type'] == 'purchase']
    purchase_features = purchases.groupby('user_id').agg({
        'event_type': 'count',
        'price': 'mean',
        'timestamp': 'max'
    }).rename(columns={
        'event_type': 'total_purchases',
        'price': 'avg_purchase_value',
        'timestamp': 'last_purchase_time'
    })

    # Merge
    user_features = user_features.join(purchase_features, how='left').fillna(0)

    # Days since last purchase
    user_features['days_since_last_purchase'] = (
        datetime.now() - user_features['last_purchase_time']
    ).dt.days

    # Add timestamp for Feast
    user_features['event_timestamp'] = int(datetime.now().timestamp())

    return user_features.reset_index()

# Usage
events_df = pd.read_parquet('data/raw/user_events.parquet')
user_features = compute_user_features(events_df)
user_features.to_parquet('data/processed/user_features.parquet', index=False)
```

---

## 5. Phase 3: Model Development

### Collaborative Filtering Model

```python
# src/models/collaborative_filtering.py
import numpy as np
from sklearn.decomposition import NMF
import torch
import torch.nn as nn

class MatrixFactorization(nn.Module):
    """Neural collaborative filtering"""

    def __init__(self, n_users, n_items, n_factors=50):
        super().__init__()

        self.user_factors = nn.Embedding(n_users, n_factors)
        self.item_factors = nn.Embedding(n_items, n_factors)

        # Initialize
        self.user_factors.weight.data.uniform_(-0.05, 0.05)
        self.item_factors.weight.data.uniform_(-0.05, 0.05)

    def forward(self, user, item):
        user_embedding = self.user_factors(user)
        item_embedding = self.item_factors(item)

        # Dot product
        return (user_embedding * item_embedding).sum(1)

def train_collaborative_filtering(interactions_df, n_epochs=20):
    """Train collaborative filtering model"""

    # Prepare data
    user_ids = interactions_df['user_id'].values
    product_ids = interactions_df['product_id'].values
    ratings = interactions_df['rating'].values  # implicit feedback

    # Model
    n_users = user_ids.max() + 1
    n_items = product_ids.max() + 1

    model = MatrixFactorization(n_users, n_items, n_factors=50)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    criterion = nn.MSELoss()

    # Training
    for epoch in range(n_epochs):
        # Convert to tensors
        user_tensor = torch.LongTensor(user_ids)
        item_tensor = torch.LongTensor(product_ids)
        rating_tensor = torch.FloatTensor(ratings)

        # Forward pass
        predictions = model(user_tensor, item_tensor)
        loss = criterion(predictions, rating_tensor)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if (epoch + 1) % 5 == 0:
            print(f"Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.4f}")

    return model

def get_recommendations(model, user_id, n_recommendations=10, exclude_items=None):
    """Get top-N recommendations for a user"""

    model.eval()

    with torch.no_grad():
        # Score all items for this user
        user_tensor = torch.LongTensor([user_id] * model.item_factors.num_embeddings)
        item_tensor = torch.LongTensor(range(model.item_factors.num_embeddings))

        scores = model(user_tensor, item_tensor).numpy()

        # Exclude already interacted items
        if exclude_items:
            scores[exclude_items] = -np.inf

        # Top-N
        top_indices = np.argsort(scores)[-n_recommendations:][::-1]

    return top_indices, scores[top_indices]
```

### Model Training Script

```python
# src/models/train.py
import pandas as pd
import torch
from src.models.collaborative_filtering import train_collaborative_filtering
import mlflow

def main():
    # Load data
    events = pd.read_parquet('data/raw/user_events.parquet')

    # Create implicit ratings (view=1, click=2, cart=3, purchase=5)
    rating_map = {'view': 1, 'click': 2, 'add_to_cart': 3, 'purchase': 5}
    events['rating'] = events['event_type'].map(rating_map)

    # Aggregate to user-item pairs
    interactions = events.groupby(['user_id', 'product_id'])['rating'].max().reset_index()

    # Split train/test
    from sklearn.model_selection import train_test_split
    train_df, test_df = train_test_split(interactions, test_size=0.2, random_state=42)

    # Train with MLflow tracking
    mlflow.start_run()

    mlflow.log_params({
        "n_factors": 50,
        "n_epochs": 20,
        "learning_rate": 0.01
    })

    model = train_collaborative_filtering(train_df, n_epochs=20)

    # Evaluate
    # ... evaluation code ...

    # Save model
    torch.save(model.state_dict(), 'models/collaborative_filtering.pth')
    mlflow.pytorch.log_model(model, "model")

    mlflow.end_run()

if __name__ == "__main__":
    main()
```

---

## 6. Phase 4: Model Serving

### FastAPI Serving

```python
# src/serving/api.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
from src.models.collaborative_filtering import MatrixFactorization, get_recommendations
from feast import FeatureStore
import prometheus_client as prom
import time

app = FastAPI(title="Recommendation API")

# Load model
model = MatrixFactorization(n_users=10000, n_items=1000, n_factors=50)
model.load_state_dict(torch.load('models/collaborative_filtering.pth'))
model.eval()

# Feature store
store = FeatureStore(repo_path=".")

# Prometheus metrics
request_latency = prom.Histogram('request_latency_seconds', 'Request latency')
request_count = prom.Counter('requests_total', 'Total requests')

class RecommendationRequest(BaseModel):
    user_id: int
    n_recommendations: int = 10

class RecommendationResponse(BaseModel):
    user_id: int
    recommendations: list[dict]
    latency_ms: float

@app.get("/")
async def root():
    return {"message": "Recommendation API", "version": "1.0.0"}

@app.post("/recommend", response_model=RecommendationResponse)
async def recommend(request: RecommendationRequest):
    start_time = time.time()
    request_count.inc()

    try:
        # Get features from feature store
        features = store.get_online_features(
            features=[
                "user_features:total_purchases",
                "user_features:favorite_category"
            ],
            entity_rows=[{"user_id": request.user_id}]
        ).to_dict()

        # Get recommendations from model
        product_ids, scores = get_recommendations(
            model,
            request.user_id,
            n_recommendations=request.n_recommendations
        )

        # Format response
        recommendations = [
            {"product_id": int(pid), "score": float(score)}
            for pid, score in zip(product_ids, scores)
        ]

        latency_ms = (time.time() - start_time) * 1000
        request_latency.observe(time.time() - start_time)

        return RecommendationResponse(
            user_id=request.user_id,
            recommendations=recommendations,
            latency_ms=latency_ms
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/metrics")
async def metrics():
    return prom.generate_latest()

@app.get("/health")
async def health():
    return {"status": "healthy"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

---

## 7. Phase 5: Monitoring & Operations

### Drift Detection

```python
# src/monitoring/drift_detection.py
from evidently.report import Report
from evidently.metric_preset import DataDriftPreset
import pandas as pd
from datetime import datetime, timedelta

class DriftMonitor:
    """Monitor data drift in production"""

    def __init__(self, reference_data):
        self.reference_data = reference_data

    def check_drift(self, current_data):
        """Check for data drift"""

        report = Report(metrics=[DataDriftPreset()])

        report.run(
            reference_data=self.reference_data,
            current_data=current_data
        )

        # Save report
        report.save_html(f"reports/drift_report_{datetime.now().strftime('%Y%m%d')}.html")

        # Check if drift detected
        result = report.as_dict()
        drift_detected = result['metrics'][0]['result']['dataset_drift']

        if drift_detected:
            print("‚ö†Ô∏è  Data drift detected! Triggering alert...")
            # Send alert to Slack/PagerDuty
            self.send_alert("Data drift detected")

        return drift_detected

    def send_alert(self, message):
        # Integration with alerting system
        pass

# Usage (run daily)
if __name__ == "__main__":
    # Load reference data (last month)
    reference = pd.read_parquet('data/processed/user_features_reference.parquet')

    # Load current data (last day)
    current = pd.read_parquet('data/processed/user_features_current.parquet')

    monitor = DriftMonitor(reference)
    drift_detected = monitor.check_drift(current)
```

---

## 8. Phase 6: Deployment

### Docker Configuration

```dockerfile
# docker/Dockerfile.api
FROM python:3.10-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY src/ ./src/
COPY models/ ./models/
COPY config/ ./config/

# Expose ports
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=3s \
  CMD curl -f http://localhost:8000/health || exit 1

# Run application
CMD ["uvicorn", "src.serving.api:app", "--host", "0.0.0.0", "--port", "8000"]
```

```yaml
# docker/docker-compose.yml
version: '3.8'

services:
  api:
    build:
      context: ..
      dockerfile: docker/Dockerfile.api
    ports:
      - "8000:8000"
    environment:
      - MODEL_PATH=/app/models/collaborative_filtering.pth
    volumes:
      - ../models:/app/models
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 3s
      retries: 3

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ../config/prometheus.yml:/etc/prometheus/prometheus.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
```

### Kubernetes Deployment

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: recommendation-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: recommendation-api
  template:
    metadata:
      labels:
        app: recommendation-api
    spec:
      containers:
      - name: api
        image: your-registry/recommendation-api:latest
        ports:
        - containerPort: 8000
        env:
        - name: MODEL_PATH
          value: /models/collaborative_filtering.pth
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: recommendation-api-service
spec:
  type: LoadBalancer
  selector:
    app: recommendation-api
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
```

---

## 9. Testing Strategy

### Unit Tests

```python
# tests/test_features.py
import pytest
import pandas as pd
from src.features.user_features import compute_user_features

def test_compute_user_features():
    """Test user feature computation"""

    # Sample data
    events = pd.DataFrame({
        'user_id': [1, 1, 2],
        'product_id': [10, 20, 10],
        'event_type': ['view', 'purchase', 'view'],
        'timestamp': pd.to_datetime(['2025-10-01', '2025-10-02', '2025-10-03']),
        'price': [100.0, 150.0, 100.0],
        'category': ['Electronics', 'Electronics', 'Books']
    })

    # Compute features
    features = compute_user_features(events)

    # Assertions
    assert len(features) == 2  # 2 users
    assert 'total_views' in features.columns
    assert features[features['user_id'] == 1]['total_purchases'].iloc[0] == 1
```

```python
# tests/test_api.py
from fastapi.testclient import TestClient
from src.serving.api import app

client = TestClient(app)

def test_recommend():
    """Test recommendation endpoint"""

    response = client.post("/recommend", json={
        "user_id": 1,
        "n_recommendations": 5
    })

    assert response.status_code == 200
    data = response.json()
    assert 'recommendations' in data
    assert len(data['recommendations']) == 5
    assert data['latency_ms'] < 200  # Latency SLA

def test_health():
    """Test health endpoint"""
    response = client.get("/health")
    assert response.status_code == 200
    assert response.json()["status"] == "healthy"
```

### CI/CD Pipeline

```yaml
# .github/workflows/ci.yml
name: CI

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v2

    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest pytest-cov

    - name: Run tests
      run: |
        pytest tests/ --cov=src --cov-report=xml

    - name: Upload coverage
      uses: codecov/codecov-action@v2

  lint:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v2

    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.10'

    - name: Lint with flake8
      run: |
        pip install flake8
        flake8 src/ --max-line-length=120

  build:
    runs-on: ubuntu-latest
    needs: [test, lint]

    steps:
    - uses: actions/checkout@v2

    - name: Build Docker image
      run: |
        docker build -f docker/Dockerfile.api -t recommendation-api:latest .

    - name: Run Docker container
      run: |
        docker run -d -p 8000:8000 recommendation-api:latest
        sleep 10
        curl http://localhost:8000/health
```

---

## 10. Portfolio Presentation

### Project README Template

```markdown
# E-Commerce Product Recommendation System

**Production-ready ML system for personalized product recommendations**

![System Architecture](docs/architecture.png)

## üéØ Project Overview

Built a complete end-to-end ML system that serves personalized product recommendations to 10K+ users with <100ms latency.

**Key Achievements:**
- ‚ö° **95ms p99 latency** (target: <100ms)
- üöÄ **1200 QPS throughput** (target: 1000 QPS)
- üìà **23% improvement** in click-through rate vs baseline
- üîÑ **Real-time** feature updates with streaming pipeline
- üìä **Production monitoring** with automated drift detection

## üèóÔ∏è Architecture

[Include architecture diagram here]

**Tech Stack:**
- **ML:** PyTorch, scikit-learn
- **Feature Store:** Feast
- **Serving:** FastAPI, Docker, Kubernetes
- **Monitoring:** Prometheus, Grafana, Evidently AI
- **CI/CD:** GitHub Actions

## üìä Results

| Metric | Baseline | Our Model | Improvement |
|--------|----------|-----------|-------------|
| CTR | 3.2% | 3.9% | +23% |
| Latency (p99) | - | 95ms | ‚úì |
| Throughput | - | 1200 QPS | ‚úì |

## üöÄ Quick Start

\`\`\`bash
# Clone repository
git clone https://github.com/yourusername/recommendation-system

# Setup
make install

# Generate synthetic data
python data/synthetic_generator.py

# Train model
python src/models/train.py

# Run API
make serve

# Test
curl -X POST http://localhost:8000/recommend \\
  -H "Content-Type: application/json" \\
  -d '{"user_id": 1, "n_recommendations": 10}'
\`\`\`

## üìÅ Project Structure

[Project structure from above]

## üß™ Testing

\`\`\`bash
# Run all tests
pytest tests/ --cov=src

# Run specific test
pytest tests/test_api.py -v
\`\`\`

Coverage: 85%

## üìà Monitoring

- **Prometheus:** http://localhost:9090
- **Grafana:** http://localhost:3000
- **Drift Reports:** `reports/`

## üîÑ CI/CD

- Automated testing on every PR
- Docker image built on merge to main
- Deployment to staging ‚Üí production with approval

## üìù Future Improvements

1. A/B testing framework for model comparison
2. Real-time model updates with online learning
3. Multi-armed bandit for exploration/exploitation
4. Distributed training for faster iterations
5. Advanced features (session context, temporal patterns)

## ü§ù Contributing

Pull requests welcome! See [CONTRIBUTING.md](CONTRIBUTING.md)

## üìÑ License

MIT License

---

**Author:** Your Name
**LinkedIn:** [Your Profile](https://linkedin.com/in/yourprofile)
**Portfolio:** [Your Website](https://yourwebsite.com)
```

---

## Summary

This template provides a **complete production ML project** that you can build, deploy, and showcase in your portfolio.

**What You've Built:**
- ‚úÖ End-to-end ML pipeline (data ‚Üí features ‚Üí model ‚Üí serving)
- ‚úÖ Production-grade API with monitoring
- ‚úÖ Feature store integration
- ‚úÖ Automated testing and CI/CD
- ‚úÖ Docker and Kubernetes deployment
- ‚úÖ Drift detection and monitoring

**Portfolio Value:**
- Demonstrates production ML skills
- Shows end-to-end system thinking
- Includes monitoring and operations
- Has deployable code (Docker, K8s)
- Well-documented and tested

**Time to Complete:** 2-3 weeks full-time

**Next Steps:**
1. Generate synthetic data
2. Build feature pipeline
3. Train baseline model
4. Deploy API locally
5. Add monitoring
6. Deploy to cloud (AWS/GCP)
7. Document in portfolio

---

**Generated with Claude Code**

*Last Updated: 2025-10-25*
