{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç RAG (Retrieval Augmented Generation) - Interactive Lab\n",
    "\n",
    "## Learn how to build AI systems that can search and understand massive document repositories!\n",
    "\n",
    "**What you'll learn:**\n",
    "- How RAG solves the challenge of connecting AI to large document collections\n",
    "- Vector embeddings and semantic search\n",
    "- Different chunking strategies and when to use them\n",
    "- Building production-ready RAG systems\n",
    "- Hands-on implementation with real documents\n",
    "\n",
    "**Prerequisites:**\n",
    "- Basic Python knowledge\n",
    "- Understanding of machine learning concepts (helpful but not required)\n",
    "\n",
    "**Time to complete:** 60-90 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Table of Contents\n",
    "\n",
    "1. [Introduction to RAG](#intro)\n",
    "2. [Setup and Dependencies](#setup)\n",
    "3. [Loading Documents](#loading)\n",
    "4. [Chunking Strategies](#chunking)\n",
    "5. [Vector Embeddings](#embeddings)\n",
    "6. [Vector Databases](#vectordb)\n",
    "7. [Semantic Search](#search)\n",
    "8. [Complete RAG Pipeline](#pipeline)\n",
    "9. [Advanced Topics](#advanced)\n",
    "10. [Exercises](#exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## 1. üéØ Introduction to RAG\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Imagine you have:\n",
    "- **1000s of company documents** (policies, reports, documentation)\n",
    "- **Questions** that need answers from these documents\n",
    "- **AI assistants** (like ChatGPT) that can answer questions BUT...\n",
    "\n",
    "‚ùå ChatGPT doesn't know about YOUR specific documents\n",
    "\n",
    "‚ùå Can't feed 1000s of pages into a single prompt (context limits)\n",
    "\n",
    "‚ùå Documents change frequently\n",
    "\n",
    "### The Solution: RAG\n",
    "\n",
    "**RAG = Retrieval Augmented Generation**\n",
    "\n",
    "Think of it as giving AI a \"search engine\" for your documents:\n",
    "\n",
    "```\n",
    "User Question ‚Üí [RAG System] ‚Üí Relevant Docs ‚Üí AI ‚Üí Answer\n",
    "```\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "1. **üìÑ Index Phase** (done once):\n",
    "   - Split documents into chunks\n",
    "   - Convert chunks to vectors (embeddings)\n",
    "   - Store in vector database\n",
    "\n",
    "2. **üîç Query Phase** (every question):\n",
    "   - Convert question to vector\n",
    "   - Find similar document chunks (semantic search)\n",
    "   - Feed relevant chunks + question to AI\n",
    "   - Get accurate, grounded answer\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "- **Customer Support**: Answer questions from product documentation\n",
    "- **Legal**: Search through contracts and case law\n",
    "- **Healthcare**: Query medical research papers\n",
    "- **Enterprise**: Internal knowledge management systems\n",
    "- **Education**: Intelligent tutoring systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## 2. üõ†Ô∏è Setup and Dependencies\n",
    "\n",
    "Let's install the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install -q sentence-transformers faiss-cpu chromadb nltk tiktoken openai anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For text processing\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# For embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# For vector databases\n",
    "import faiss\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='loading'></a>\n",
    "## 3. üìÑ Loading Documents\n",
    "\n",
    "We'll work with realistic company documents:\n",
    "- Employee policies\n",
    "- Product documentation  \n",
    "- Sales reports\n",
    "\n",
    "These represent typical enterprise knowledge bases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document loader class\n",
    "class DocumentLoader:\n",
    "    \"\"\"Load and manage documents from various sources\"\"\"\n",
    "    \n",
    "    def __init__(self, documents_dir: str):\n",
    "        self.documents_dir = documents_dir\n",
    "        self.documents = {}\n",
    "    \n",
    "    def load_txt_files(self) -> Dict[str, str]:\n",
    "        \"\"\"Load all .txt files from the documents directory\"\"\"\n",
    "        for filename in os.listdir(self.documents_dir):\n",
    "            if filename.endswith('.txt'):\n",
    "                filepath = os.path.join(self.documents_dir, filename)\n",
    "                with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                    self.documents[filename] = f.read()\n",
    "        \n",
    "        print(f\"‚úÖ Loaded {len(self.documents)} documents\")\n",
    "        return self.documents\n",
    "    \n",
    "    def get_document_stats(self) -> pd.DataFrame:\n",
    "        \"\"\"Get statistics about loaded documents\"\"\"\n",
    "        stats = []\n",
    "        for name, content in self.documents.items():\n",
    "            stats.append({\n",
    "                'Document': name,\n",
    "                'Characters': len(content),\n",
    "                'Words': len(content.split()),\n",
    "                'Lines': len(content.split('\\n'))\n",
    "            })\n",
    "        return pd.DataFrame(stats)\n",
    "\n",
    "# Load our sample documents\n",
    "loader = DocumentLoader('data/sample_documents')\n",
    "documents = loader.load_txt_files()\n",
    "\n",
    "# Display stats\n",
    "print(\"\\nüìä Document Statistics:\")\n",
    "print(loader.get_document_stats().to_string(index=False))\n",
    "\n",
    "# Preview first document\n",
    "print(\"\\nüìÑ Preview of first document:\")\n",
    "first_doc = list(documents.values())[0]\n",
    "print(first_doc[:500] + \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='chunking'></a>\n",
    "## 4. ‚úÇÔ∏è Chunking Strategies\n",
    "\n",
    "### Why Chunk Documents?\n",
    "\n",
    "Documents are often too large to process at once. We need to split them into **chunks**:\n",
    "\n",
    "- **Too small**: Loses context (\"the company\" - which company?)\n",
    "- **Too large**: Too much irrelevant info, exceeds AI context limits\n",
    "- **Just right**: Balances context and precision\n",
    "\n",
    "### Three Chunking Strategies\n",
    "\n",
    "We'll implement and compare:\n",
    "\n",
    "1. **Fixed-Size Chunking**: Split every N characters/tokens\n",
    "2. **Sentence-Based Chunking**: Group complete sentences\n",
    "3. **Semantic Chunking**: Split at topic boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextChunker:\n",
    "    \"\"\"Different strategies for chunking text documents\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def fixed_size_chunking(\n",
    "        text: str, \n",
    "        chunk_size: int = 500, \n",
    "        overlap: int = 50\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Split text into fixed-size chunks with overlap\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to chunk\n",
    "            chunk_size: Number of characters per chunk\n",
    "            overlap: Number of overlapping characters between chunks\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(text):\n",
    "            end = start + chunk_size\n",
    "            chunk = text[start:end]\n",
    "            \n",
    "            # Try to break at word boundary\n",
    "            if end < len(text):\n",
    "                last_space = chunk.rfind(' ')\n",
    "                if last_space != -1:\n",
    "                    chunk = chunk[:last_space]\n",
    "            \n",
    "            chunks.append(chunk.strip())\n",
    "            start = end - overlap\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    @staticmethod\n",
    "    def sentence_based_chunking(\n",
    "        text: str, \n",
    "        sentences_per_chunk: int = 5\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Group sentences into chunks\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to chunk\n",
    "            sentences_per_chunk: Number of sentences per chunk\n",
    "        \"\"\"\n",
    "        sentences = sent_tokenize(text)\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(sentences), sentences_per_chunk):\n",
    "            chunk = ' '.join(sentences[i:i + sentences_per_chunk])\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    @staticmethod\n",
    "    def semantic_chunking(\n",
    "        text: str,\n",
    "        section_pattern: str = r'\\n\\n+|SECTION \\d+'\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Split text at semantic boundaries (sections, paragraphs)\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to chunk\n",
    "            section_pattern: Regex pattern to identify sections\n",
    "        \"\"\"\n",
    "        # Split on section markers or double newlines\n",
    "        chunks = re.split(section_pattern, text)\n",
    "        \n",
    "        # Filter empty chunks and very short ones\n",
    "        chunks = [c.strip() for c in chunks if len(c.strip()) > 100]\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# Test different chunking strategies\n",
    "print(\"üî¨ Testing Chunking Strategies\\n\")\n",
    "\n",
    "sample_doc = documents['company_policies.txt']\n",
    "chunker = TextChunker()\n",
    "\n",
    "# Strategy 1: Fixed-size\n",
    "fixed_chunks = chunker.fixed_size_chunking(sample_doc, chunk_size=500, overlap=50)\n",
    "print(f\"‚úÇÔ∏è Fixed-size chunking: {len(fixed_chunks)} chunks\")\n",
    "print(f\"   Average length: {np.mean([len(c) for c in fixed_chunks]):.0f} chars\\n\")\n",
    "\n",
    "# Strategy 2: Sentence-based\n",
    "sentence_chunks = chunker.sentence_based_chunking(sample_doc, sentences_per_chunk=5)\n",
    "print(f\"üìù Sentence-based chunking: {len(sentence_chunks)} chunks\")\n",
    "print(f\"   Average length: {np.mean([len(c) for c in sentence_chunks]):.0f} chars\\n\")\n",
    "\n",
    "# Strategy 3: Semantic\n",
    "semantic_chunks = chunker.semantic_chunking(sample_doc)\n",
    "print(f\"üéØ Semantic chunking: {len(semantic_chunks)} chunks\")\n",
    "print(f\"   Average length: {np.mean([len(c) for c in semantic_chunks]):.0f} chars\\n\")\n",
    "\n",
    "# Preview a chunk from each strategy\n",
    "print(\"\\nüìã Sample chunks:\\n\")\n",
    "print(\"Fixed-size chunk:\")\n",
    "print(fixed_chunks[0][:200] + \"...\\n\")\n",
    "print(\"\\nSentence-based chunk:\")\n",
    "print(sentence_chunks[0][:200] + \"...\\n\")\n",
    "print(\"\\nSemantic chunk:\")\n",
    "print(semantic_chunks[0][:200] + \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì Chunking Strategy Comparison\n",
    "\n",
    "| Strategy | Pros | Cons | Best For |\n",
    "|----------|------|------|----------|\n",
    "| **Fixed-size** | Simple, predictable size | May break sentences mid-way | Uniform processing, simple documents |\n",
    "| **Sentence-based** | Preserves sentence integrity | Variable chunk sizes | Q&A systems, natural language |\n",
    "| **Semantic** | Preserves topic coherence | Requires good structure | Structured docs, technical docs |\n",
    "\n",
    "**Pro Tip**: Start with sentence-based chunking - it's a good balance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='embeddings'></a>\n",
    "## 5. üß† Vector Embeddings\n",
    "\n",
    "### What are Embeddings?\n",
    "\n",
    "**Embeddings** convert text into numbers (vectors) that capture **meaning**:\n",
    "\n",
    "```\n",
    "\"remote work policy\" ‚Üí [0.23, -0.45, 0.67, ...] (384 numbers)\n",
    "\"working from home\"  ‚Üí [0.21, -0.43, 0.69, ...] (similar numbers!)\n",
    "```\n",
    "\n",
    "**Key insight**: Similar meaning = similar vectors!\n",
    "\n",
    "This enables **semantic search**: Find documents by meaning, not just keywords.\n",
    "\n",
    "### Popular Embedding Models\n",
    "\n",
    "- **sentence-transformers**: Fast, runs locally\n",
    "- **OpenAI ada-002**: High quality, API-based\n",
    "- **Cohere**: Great for multilingual\n",
    "\n",
    "We'll use `sentence-transformers` - it's free and fast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingGenerator:\n",
    "    \"\"\"Generate vector embeddings for text\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        \"\"\"Initialize embedding model\n",
    "        \n",
    "        Popular models:\n",
    "        - all-MiniLM-L6-v2: Fast, 384 dimensions (default)\n",
    "        - all-mpnet-base-v2: Higher quality, 768 dimensions\n",
    "        - multi-qa-mpnet-base-dot-v1: Optimized for Q&A\n",
    "        \"\"\"\n",
    "        print(f\"Loading embedding model: {model_name}...\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        print(f\"‚úÖ Model loaded! Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "    \n",
    "    def embed_texts(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for a list of texts\"\"\"\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        return embeddings\n",
    "    \n",
    "    def embed_single(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Generate embedding for a single text\"\"\"\n",
    "        return self.model.encode([text])[0]\n",
    "\n",
    "# Initialize embedding generator\n",
    "embedding_gen = EmbeddingGenerator()\n",
    "\n",
    "# Generate embeddings for our chunks\n",
    "print(\"\\nüîÑ Generating embeddings for document chunks...\")\n",
    "chunks_to_embed = sentence_chunks[:10]  # Use first 10 chunks for demo\n",
    "chunk_embeddings = embedding_gen.embed_texts(chunks_to_embed)\n",
    "\n",
    "print(f\"\\n‚úÖ Generated {len(chunk_embeddings)} embeddings\")\n",
    "print(f\"   Shape: {chunk_embeddings.shape}\")\n",
    "print(f\"   Each chunk is represented by {chunk_embeddings.shape[1]} numbers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embeddings in 2D using PCA\n",
    "print(\"\\nüìä Visualizing Embeddings\\n\")\n",
    "\n",
    "# Reduce dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(chunk_embeddings)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], s=100, alpha=0.6)\n",
    "\n",
    "# Add labels for each point\n",
    "for i, (x, y) in enumerate(embeddings_2d):\n",
    "    label = chunks_to_embed[i][:50] + \"...\"  # First 50 chars\n",
    "    plt.annotate(f\"Chunk {i}\", (x, y), fontsize=9, alpha=0.7)\n",
    "\n",
    "plt.title(\"Document Chunks in 2D Embedding Space\\n(Similar chunks are closer together)\", fontsize=14)\n",
    "plt.xlabel(\"PCA Dimension 1\")\n",
    "plt.ylabel(\"PCA Dimension 2\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Interpretation: Chunks with similar content appear closer in this space!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Experiment: Semantic Similarity\n",
    "\n",
    "Let's test if embeddings truly capture meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Test semantic similarity\n",
    "test_queries = [\n",
    "    \"remote work from home\",\n",
    "    \"salary and compensation\",\n",
    "    \"vacation time off\"\n",
    "]\n",
    "\n",
    "print(\"üîç Testing Semantic Similarity\\n\")\n",
    "print(\"We'll find which chunks are most similar to each query:\\n\")\n",
    "\n",
    "for query in test_queries:\n",
    "    # Embed query\n",
    "    query_embedding = embedding_gen.embed_single(query)\n",
    "    \n",
    "    # Calculate similarity with all chunks\n",
    "    similarities = cosine_similarity([query_embedding], chunk_embeddings)[0]\n",
    "    \n",
    "    # Get top match\n",
    "    top_idx = np.argmax(similarities)\n",
    "    top_score = similarities[top_idx]\n",
    "    \n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"Best match (score: {top_score:.3f}):\")\n",
    "    print(f\"{chunks_to_embed[top_idx][:200]}...\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vectordb'></a>\n",
    "## 6. üóÑÔ∏è Vector Databases\n",
    "\n",
    "### Why Vector Databases?\n",
    "\n",
    "Once you have embeddings, you need **fast similarity search**:\n",
    "\n",
    "- Traditional DB: \"Find WHERE id = 123\" ‚úÖ\n",
    "- Vector DB: \"Find most similar to [0.23, -0.45, ...]\" ‚úÖ\n",
    "\n",
    "**Challenge**: Comparing millions of high-dimensional vectors is slow!\n",
    "\n",
    "**Solution**: Vector databases use clever algorithms (HNSW, IVF) for fast approximate search.\n",
    "\n",
    "### Popular Vector Databases\n",
    "\n",
    "1. **FAISS** (Facebook AI): Fastest, in-memory, local\n",
    "2. **ChromaDB**: Easy to use, persistent storage\n",
    "3. **Pinecone**: Managed cloud service\n",
    "4. **Weaviate**: Full-featured, production-grade\n",
    "\n",
    "We'll demo both FAISS and ChromaDB!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: FAISS (Fast, In-Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FAISSVectorStore:\n",
    "    \"\"\"Vector store using FAISS for fast similarity search\"\"\"\n",
    "    \n",
    "    def __init__(self, dimension: int):\n",
    "        \"\"\"Initialize FAISS index\n",
    "        \n",
    "        Args:\n",
    "            dimension: Embedding dimension (e.g., 384 for MiniLM)\n",
    "        \"\"\"\n",
    "        # Create a flat (exact) index for small datasets\n",
    "        self.index = faiss.IndexFlatL2(dimension)\n",
    "        self.chunks = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_documents(self, chunks: List[str], embeddings: np.ndarray, metadata: List[Dict] = None):\n",
    "        \"\"\"Add documents to the index\"\"\"\n",
    "        # FAISS requires float32\n",
    "        embeddings = embeddings.astype('float32')\n",
    "        \n",
    "        self.index.add(embeddings)\n",
    "        self.chunks.extend(chunks)\n",
    "        \n",
    "        if metadata:\n",
    "            self.metadata.extend(metadata)\n",
    "        else:\n",
    "            self.metadata.extend([{}] * len(chunks))\n",
    "        \n",
    "        print(f\"‚úÖ Added {len(chunks)} documents. Total: {self.index.ntotal}\")\n",
    "    \n",
    "    def search(self, query_embedding: np.ndarray, k: int = 5) -> List[Tuple[str, float, Dict]]:\n",
    "        \"\"\"Search for most similar documents\n",
    "        \n",
    "        Args:\n",
    "            query_embedding: Query vector\n",
    "            k: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of (chunk, distance, metadata) tuples\n",
    "        \"\"\"\n",
    "        query_embedding = query_embedding.astype('float32').reshape(1, -1)\n",
    "        \n",
    "        # Search\n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "        \n",
    "        results = []\n",
    "        for dist, idx in zip(distances[0], indices[0]):\n",
    "            results.append((\n",
    "                self.chunks[idx],\n",
    "                float(dist),\n",
    "                self.metadata[idx]\n",
    "            ))\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Create FAISS vector store\n",
    "print(\"üèóÔ∏è Building FAISS Vector Store\\n\")\n",
    "\n",
    "vector_store = FAISSVectorStore(dimension=384)\n",
    "\n",
    "# Add our embedded chunks\n",
    "metadata = [{'chunk_id': i, 'source': 'company_policies.txt'} for i in range(len(chunks_to_embed))]\n",
    "vector_store.add_documents(chunks_to_embed, chunk_embeddings, metadata)\n",
    "\n",
    "print(f\"\\nüìä Vector store stats:\")\n",
    "print(f\"   Total documents: {vector_store.index.ntotal}\")\n",
    "print(f\"   Dimension: {vector_store.index.d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test FAISS search\n",
    "print(\"\\nüîç Testing FAISS Search\\n\")\n",
    "\n",
    "query = \"What is the remote work policy?\"\n",
    "query_embedding = embedding_gen.embed_single(query)\n",
    "\n",
    "results = vector_store.search(query_embedding, k=3)\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Top 3 Results:\\n\")\n",
    "\n",
    "for i, (chunk, distance, meta) in enumerate(results, 1):\n",
    "    print(f\"{i}. [Distance: {distance:.4f}] Source: {meta['source']}\")\n",
    "    print(f\"   {chunk[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: ChromaDB (Persistent, Easy to Use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChromaVectorStore:\n",
    "    \"\"\"Vector store using ChromaDB\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"documents\", persist_directory: str = \"./chroma_db\"):\n",
    "        \"\"\"Initialize ChromaDB\"\"\"\n",
    "        self.client = chromadb.Client(Settings(\n",
    "            chroma_db_impl=\"duckdb+parquet\",\n",
    "            persist_directory=persist_directory\n",
    "        ))\n",
    "        \n",
    "        # Create or get collection\n",
    "        self.collection = self.client.get_or_create_collection(name=collection_name)\n",
    "        print(f\"‚úÖ ChromaDB collection '{collection_name}' ready\")\n",
    "    \n",
    "    def add_documents(self, chunks: List[str], embeddings: np.ndarray = None, metadata: List[Dict] = None):\n",
    "        \"\"\"Add documents to ChromaDB\"\"\"\n",
    "        ids = [f\"doc_{i}\" for i in range(len(chunks))]\n",
    "        \n",
    "        if embeddings is not None:\n",
    "            embeddings = embeddings.tolist()\n",
    "        \n",
    "        self.collection.add(\n",
    "            documents=chunks,\n",
    "            embeddings=embeddings,\n",
    "            metadatas=metadata,\n",
    "            ids=ids\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Added {len(chunks)} documents to ChromaDB\")\n",
    "    \n",
    "    def search(self, query_embedding: np.ndarray, k: int = 5) -> List[Tuple[str, float, Dict]]:\n",
    "        \"\"\"Search for similar documents\"\"\"\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding.tolist()],\n",
    "            n_results=k\n",
    "        )\n",
    "        \n",
    "        output = []\n",
    "        for doc, dist, meta in zip(\n",
    "            results['documents'][0],\n",
    "            results['distances'][0],\n",
    "            results['metadatas'][0]\n",
    "        ):\n",
    "            output.append((doc, dist, meta))\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Create ChromaDB vector store\n",
    "print(\"\\nüèóÔ∏è Building ChromaDB Vector Store\\n\")\n",
    "\n",
    "chroma_store = ChromaVectorStore(collection_name=\"company_docs\")\n",
    "chroma_store.add_documents(chunks_to_embed, chunk_embeddings, metadata)\n",
    "\n",
    "# Test ChromaDB search\n",
    "print(\"\\nüîç Testing ChromaDB Search\\n\")\n",
    "\n",
    "query = \"How much PTO do employees get?\"\n",
    "query_embedding = embedding_gen.embed_single(query)\n",
    "\n",
    "results = chroma_store.search(query_embedding, k=3)\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Top 3 Results:\\n\")\n",
    "\n",
    "for i, (chunk, distance, meta) in enumerate(results, 1):\n",
    "    print(f\"{i}. [Distance: {distance:.4f}]\")\n",
    "    print(f\"   {chunk[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ FAISS vs ChromaDB Comparison\n",
    "\n",
    "| Feature | FAISS | ChromaDB |\n",
    "|---------|-------|----------|\n",
    "| **Speed** | ‚ö° Fastest | üöÄ Fast |\n",
    "| **Persistence** | In-memory (can save) | Built-in persistence |\n",
    "| **Ease of use** | More code | Very simple |\n",
    "| **Filtering** | Manual | Built-in metadata filters |\n",
    "| **Best for** | Maximum speed, large scale | Quick prototypes, small-medium scale |\n",
    "\n",
    "**Recommendation**: \n",
    "- Prototyping? Use **ChromaDB**\n",
    "- Production with millions of docs? Use **FAISS** or **Pinecone**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='search'></a>\n",
    "## 7. üîé Semantic Search\n",
    "\n",
    "Now let's build a complete semantic search system!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSearchEngine:\n",
    "    \"\"\"Complete semantic search system\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model: EmbeddingGenerator, vector_store):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.vector_store = vector_store\n",
    "    \n",
    "    def index_documents(self, documents: Dict[str, str], chunking_strategy: str = 'sentence'):\n",
    "        \"\"\"Index all documents\"\"\"\n",
    "        print(\"üìö Indexing documents...\\n\")\n",
    "        \n",
    "        all_chunks = []\n",
    "        all_metadata = []\n",
    "        chunker = TextChunker()\n",
    "        \n",
    "        for doc_name, content in documents.items():\n",
    "            print(f\"Processing: {doc_name}\")\n",
    "            \n",
    "            # Chunk document\n",
    "            if chunking_strategy == 'sentence':\n",
    "                chunks = chunker.sentence_based_chunking(content)\n",
    "            elif chunking_strategy == 'fixed':\n",
    "                chunks = chunker.fixed_size_chunking(content)\n",
    "            else:\n",
    "                chunks = chunker.semantic_chunking(content)\n",
    "            \n",
    "            # Create metadata\n",
    "            metadata = [{'source': doc_name, 'chunk_id': i} for i in range(len(chunks))]\n",
    "            \n",
    "            all_chunks.extend(chunks)\n",
    "            all_metadata.extend(metadata)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        print(f\"\\nüîÑ Generating embeddings for {len(all_chunks)} chunks...\")\n",
    "        embeddings = self.embedding_model.embed_texts(all_chunks)\n",
    "        \n",
    "        # Add to vector store\n",
    "        self.vector_store.add_documents(all_chunks, embeddings, all_metadata)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Indexing complete! {len(all_chunks)} chunks indexed.\")\n",
    "    \n",
    "    def search(self, query: str, k: int = 5, verbose: bool = True) -> List[Dict]:\n",
    "        \"\"\"Search for relevant documents\"\"\"\n",
    "        # Embed query\n",
    "        query_embedding = self.embedding_model.embed_single(query)\n",
    "        \n",
    "        # Search\n",
    "        results = self.vector_store.search(query_embedding, k=k)\n",
    "        \n",
    "        # Format results\n",
    "        formatted_results = []\n",
    "        for chunk, score, metadata in results:\n",
    "            formatted_results.append({\n",
    "                'text': chunk,\n",
    "                'score': score,\n",
    "                'source': metadata.get('source', 'unknown'),\n",
    "                'chunk_id': metadata.get('chunk_id', -1)\n",
    "            })\n",
    "        \n",
    "        if verbose:\n",
    "            self._display_results(query, formatted_results)\n",
    "        \n",
    "        return formatted_results\n",
    "    \n",
    "    def _display_results(self, query: str, results: List[Dict]):\n",
    "        \"\"\"Pretty print search results\"\"\"\n",
    "        print(f\"\\nüîç Query: '{query}'\\n\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"\\n{i}. [{result['source']}] Score: {result['score']:.4f}\")\n",
    "            print(\"-\" * 80)\n",
    "            print(result['text'][:300] + \"...\" if len(result['text']) > 300 else result['text'])\n",
    "            print()\n",
    "\n",
    "# Build complete search engine\n",
    "print(\"üöÄ Building Semantic Search Engine\\n\")\n",
    "\n",
    "# Create fresh vector store for all documents\n",
    "search_vector_store = FAISSVectorStore(dimension=384)\n",
    "search_engine = SemanticSearchEngine(embedding_gen, search_vector_store)\n",
    "\n",
    "# Index all documents\n",
    "search_engine.index_documents(documents, chunking_strategy='sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test search with various queries\n",
    "test_queries = [\n",
    "    \"What is the remote work policy?\",\n",
    "    \"How do I install the software on Mac?\",\n",
    "    \"What were the Q4 sales numbers?\",\n",
    "    \"Tell me about parental leave benefits\",\n",
    "    \"How do I fix sync issues?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üß™ SEMANTIC SEARCH DEMO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for query in test_queries:\n",
    "    results = search_engine.search(query, k=2)\n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pipeline'></a>\n",
    "## 8. üèóÔ∏è Complete RAG Pipeline\n",
    "\n",
    "Now let's build the full RAG system: **Retrieval + Generation**\n",
    "\n",
    "We'll integrate with an LLM to generate answers based on retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGSystem:\n",
    "    \"\"\"Complete Retrieval Augmented Generation system\"\"\"\n",
    "    \n",
    "    def __init__(self, search_engine: SemanticSearchEngine, llm_provider: str = 'mock'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            search_engine: Semantic search engine for retrieval\n",
    "            llm_provider: 'openai', 'anthropic', or 'mock' for demo\n",
    "        \"\"\"\n",
    "        self.search_engine = search_engine\n",
    "        self.llm_provider = llm_provider\n",
    "    \n",
    "    def query(self, question: str, k: int = 3, verbose: bool = True) -> Dict:\n",
    "        \"\"\"Process a question through the RAG pipeline\"\"\"\n",
    "        \n",
    "        # Step 1: Retrieve relevant documents\n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"üì• QUESTION: {question}\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "            print(\"üîç Step 1: Retrieving relevant documents...\\n\")\n",
    "        \n",
    "        relevant_docs = self.search_engine.search(question, k=k, verbose=False)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"‚úÖ Found {len(relevant_docs)} relevant chunks\\n\")\n",
    "            for i, doc in enumerate(relevant_docs, 1):\n",
    "                print(f\"{i}. [{doc['source']}] (score: {doc['score']:.4f})\")\n",
    "            print()\n",
    "        \n",
    "        # Step 2: Build context\n",
    "        context = self._build_context(relevant_docs)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"üìù Step 2: Building context for LLM...\")\n",
    "            print(f\"   Context length: {len(context)} characters\\n\")\n",
    "        \n",
    "        # Step 3: Generate answer\n",
    "        if verbose:\n",
    "            print(\"ü§ñ Step 3: Generating answer...\\n\")\n",
    "        \n",
    "        answer = self._generate_answer(question, context)\n",
    "        \n",
    "        # Display results\n",
    "        if verbose:\n",
    "            print(f\"{'='*80}\")\n",
    "            print(\"üí° ANSWER:\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "            print(answer)\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(\"üìö SOURCES:\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "            sources = list(set([doc['source'] for doc in relevant_docs]))\n",
    "            for source in sources:\n",
    "                print(f\"  - {source}\")\n",
    "            print()\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': relevant_docs,\n",
    "            'context': context\n",
    "        }\n",
    "    \n",
    "    def _build_context(self, docs: List[Dict]) -> str:\n",
    "        \"\"\"Build context string from retrieved documents\"\"\"\n",
    "        context_parts = []\n",
    "        for i, doc in enumerate(docs, 1):\n",
    "            context_parts.append(f\"[Source {i}: {doc['source']}]\\n{doc['text']}\")\n",
    "        return \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    def _generate_answer(self, question: str, context: str) -> str:\n",
    "        \"\"\"Generate answer using LLM\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"Answer the question based on the context below. If the answer cannot be found in the context, say \"I don't have enough information to answer that.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        if self.llm_provider == 'mock':\n",
    "            # Mock response for demo (in real system, call OpenAI/Anthropic)\n",
    "            return self._mock_llm_response(question, context)\n",
    "        \n",
    "        # Real LLM integration would go here:\n",
    "        # elif self.llm_provider == 'openai':\n",
    "        #     return self._call_openai(prompt)\n",
    "        # elif self.llm_provider == 'anthropic':\n",
    "        #     return self._call_anthropic(prompt)\n",
    "    \n",
    "    def _mock_llm_response(self, question: str, context: str) -> str:\n",
    "        \"\"\"Generate a mock response (for demo purposes)\"\"\"\n",
    "        # In a real system, this would call an actual LLM\n",
    "        # For now, we'll extract relevant parts of the context\n",
    "        \n",
    "        # Simple extraction: return first 500 chars of most relevant chunk\n",
    "        first_chunk = context.split(\"\\n\\n\")[0]\n",
    "        \n",
    "        return f\"\"\"Based on the company documentation:\n",
    "\n",
    "{first_chunk[:400]}...\n",
    "\n",
    "[NOTE: This is a mock response. In production, connect to OpenAI/Anthropic for actual AI-generated answers.]\"\"\"\n",
    "\n",
    "# Create RAG system\n",
    "print(\"\\nüèóÔ∏è Building Complete RAG System\\n\")\n",
    "rag_system = RAGSystem(search_engine, llm_provider='mock')\n",
    "print(\"‚úÖ RAG System ready!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RAG system with sample questions\n",
    "sample_questions = [\n",
    "    \"What is the company's remote work policy?\",\n",
    "    \"How do I troubleshoot sync issues with CloudSync?\",\n",
    "    \"What were the top performing regions in Q4?\"\n",
    "]\n",
    "\n",
    "for question in sample_questions:\n",
    "    result = rag_system.query(question, k=2)\n",
    "    print(\"\\n\" + \"*\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîå Connecting to Real LLMs\n",
    "\n",
    "To use real AI for answer generation, uncomment and configure:\n",
    "\n",
    "#### OpenAI Integration:\n",
    "```python\n",
    "import openai\n",
    "\n",
    "def _call_openai(self, prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "```\n",
    "\n",
    "#### Anthropic (Claude) Integration:\n",
    "```python\n",
    "import anthropic\n",
    "\n",
    "def _call_anthropic(self, prompt):\n",
    "    client = anthropic.Anthropic(api_key=os.environ['ANTHROPIC_API_KEY'])\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-opus-20240229\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    return response.content[0].text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='advanced'></a>\n",
    "## 9. üöÄ Advanced Topics\n",
    "\n",
    "### Hybrid Search (Keyword + Semantic)\n",
    "\n",
    "Combine traditional keyword search with semantic search for best results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridSearch:\n",
    "    \"\"\"Combine keyword and semantic search\"\"\"\n",
    "    \n",
    "    def __init__(self, search_engine: SemanticSearchEngine):\n",
    "        self.search_engine = search_engine\n",
    "        self.chunks = []\n",
    "    \n",
    "    def keyword_search(self, query: str, chunks: List[str], k: int = 5) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Simple keyword-based search using TF-IDF\"\"\"\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        \n",
    "        # Create TF-IDF vectors\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(chunks + [query])\n",
    "        \n",
    "        # Calculate similarity\n",
    "        query_vector = tfidf_matrix[-1]\n",
    "        doc_vectors = tfidf_matrix[:-1]\n",
    "        similarities = cosine_similarity(query_vector, doc_vectors)[0]\n",
    "        \n",
    "        # Get top k\n",
    "        top_indices = np.argsort(similarities)[::-1][:k]\n",
    "        \n",
    "        return [(chunks[i], similarities[i]) for i in top_indices]\n",
    "    \n",
    "    def hybrid_search(self, query: str, chunks: List[str], k: int = 5, \n",
    "                     semantic_weight: float = 0.7) -> List[str]:\n",
    "        \"\"\"Combine keyword and semantic search\n",
    "        \n",
    "        Args:\n",
    "            semantic_weight: Weight for semantic search (0-1)\n",
    "                            keyword_weight = 1 - semantic_weight\n",
    "        \"\"\"\n",
    "        # Get semantic results\n",
    "        semantic_results = self.search_engine.search(query, k=k*2, verbose=False)\n",
    "        \n",
    "        # Get keyword results  \n",
    "        keyword_results = self.keyword_search(query, chunks, k=k*2)\n",
    "        \n",
    "        # Combine scores (normalize first)\n",
    "        combined_scores = {}\n",
    "        \n",
    "        # Add semantic scores\n",
    "        for result in semantic_results:\n",
    "            text = result['text']\n",
    "            # Convert distance to similarity (inverse)\n",
    "            score = 1 / (1 + result['score'])\n",
    "            combined_scores[text] = score * semantic_weight\n",
    "        \n",
    "        # Add keyword scores\n",
    "        keyword_weight = 1 - semantic_weight\n",
    "        for text, score in keyword_results:\n",
    "            if text in combined_scores:\n",
    "                combined_scores[text] += score * keyword_weight\n",
    "            else:\n",
    "                combined_scores[text] = score * keyword_weight\n",
    "        \n",
    "        # Sort by combined score\n",
    "        sorted_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return [text for text, score in sorted_results[:k]]\n",
    "\n",
    "print(\"üî¨ Hybrid Search Demo\\n\")\n",
    "print(\"Combining keyword matching with semantic understanding...\\n\")\n",
    "\n",
    "hybrid = HybridSearch(search_engine)\n",
    "\n",
    "# Test query\n",
    "query = \"PTO vacation days\"\n",
    "all_chunks = [doc['text'] for doc in search_engine.search(\"*\", k=100, verbose=False)]\n",
    "\n",
    "results = hybrid.hybrid_search(query, all_chunks[:50], k=3, semantic_weight=0.7)\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Top 3 Hybrid Results:\\n\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. {result[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-ranking for Better Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reranker:\n",
    "    \"\"\"Re-rank search results for better relevance\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'cross-encoder/ms-marco-MiniLM-L-6-v2'):\n",
    "        \"\"\"Initialize cross-encoder for re-ranking\"\"\"\n",
    "        from sentence_transformers import CrossEncoder\n",
    "        print(f\"Loading re-ranking model: {model_name}...\")\n",
    "        self.model = CrossEncoder(model_name)\n",
    "        print(\"‚úÖ Re-ranker ready!\")\n",
    "    \n",
    "    def rerank(self, query: str, documents: List[str], top_k: int = 5) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Re-rank documents by relevance to query\"\"\"\n",
    "        # Create query-document pairs\n",
    "        pairs = [[query, doc] for doc in documents]\n",
    "        \n",
    "        # Score all pairs\n",
    "        scores = self.model.predict(pairs)\n",
    "        \n",
    "        # Sort by score\n",
    "        scored_docs = list(zip(documents, scores))\n",
    "        scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return scored_docs[:top_k]\n",
    "\n",
    "# Demo re-ranking\n",
    "print(\"\\nüéØ Re-ranking Demo\\n\")\n",
    "print(\"Re-ranking improves precision by using more sophisticated relevance scoring...\\n\")\n",
    "\n",
    "reranker = Reranker()\n",
    "\n",
    "query = \"What are the system requirements?\"\n",
    "candidates = [doc['text'] for doc in search_engine.search(query, k=10, verbose=False)]\n",
    "\n",
    "reranked = reranker.rerank(query, candidates, top_k=3)\n",
    "\n",
    "print(f\"\\nQuery: '{query}'\\n\")\n",
    "print(\"Top 3 Re-ranked Results:\\n\")\n",
    "for i, (doc, score) in enumerate(reranked, 1):\n",
    "    print(f\"{i}. [Score: {score:.4f}]\")\n",
    "    print(f\"   {doc[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercises'></a>\n",
    "## 10. üéì Exercises\n",
    "\n",
    "Try these challenges to deepen your understanding:\n",
    "\n",
    "### Exercise 1: Custom Chunking Strategy\n",
    "Create a chunking strategy that splits on section headers (e.g., \"SECTION 1:\", \"###\")\n",
    "\n",
    "### Exercise 2: Add Metadata Filtering\n",
    "Extend the search to filter by document source (e.g., only search in \"product_documentation.txt\")\n",
    "\n",
    "### Exercise 3: Evaluation Metrics\n",
    "Implement precision@k and recall@k to measure search quality\n",
    "\n",
    "### Exercise 4: Multi-query RAG\n",
    "Generate multiple variations of the user's question and retrieve docs for each\n",
    "\n",
    "### Exercise 5: Add Your Own Documents\n",
    "Add your own text files to the `data/sample_documents` folder and index them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Custom Chunking Strategy\n",
    "# YOUR CODE HERE\n",
    "\n",
    "def custom_chunking(text: str) -> List[str]:\n",
    "    \"\"\"Split text on section headers\"\"\"\n",
    "    # Hint: Use regex to find patterns like \"SECTION 1:\", \"###\", etc.\n",
    "    pass\n",
    "\n",
    "# Test your chunking strategy\n",
    "# chunks = custom_chunking(documents['company_policies.txt'])\n",
    "# print(f\"Created {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Metadata Filtering\n",
    "# YOUR CODE HERE\n",
    "\n",
    "def search_with_filter(search_engine, query: str, source_filter: str = None):\n",
    "    \"\"\"Search with optional source filtering\"\"\"\n",
    "    # Hint: Retrieve more results than needed, then filter by metadata\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "# results = search_with_filter(search_engine, \"sales\", source_filter=\"sales_reports.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've built a complete RAG system from scratch! You now understand:\n",
    "\n",
    "‚úÖ What RAG is and why it's important\n",
    "\n",
    "‚úÖ Different document chunking strategies\n",
    "\n",
    "‚úÖ How vector embeddings capture semantic meaning\n",
    "\n",
    "‚úÖ Vector databases for fast similarity search\n",
    "\n",
    "‚úÖ Building a complete RAG pipeline\n",
    "\n",
    "‚úÖ Advanced techniques (hybrid search, re-ranking)\n",
    "\n",
    "### üìö Next Steps\n",
    "\n",
    "1. **Connect to real LLMs**: Integrate OpenAI or Anthropic APIs\n",
    "2. **Scale up**: Try with larger document collections\n",
    "3. **Production deployment**: Use managed services like Pinecone or Weaviate\n",
    "4. **Add chat history**: Build a conversational RAG system\n",
    "5. **Fine-tune**: Experiment with different embedding models\n",
    "\n",
    "### üîó Resources\n",
    "\n",
    "- [LangChain RAG Tutorial](https://python.langchain.com/docs/use_cases/question_answering/)\n",
    "- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)\n",
    "- [FAISS Documentation](https://github.com/facebookresearch/faiss/wiki)\n",
    "- [Sentence Transformers](https://www.sbert.net/)\n",
    "\n",
    "Happy building! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
