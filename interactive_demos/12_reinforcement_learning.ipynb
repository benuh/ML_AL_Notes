{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: From Fundamentals to Deep RL\n",
    "\n",
    "**Complete Guide to Reinforcement Learning**\n",
    "\n",
    "Learn RL from first principles to modern deep RL algorithms like DQN, Policy Gradients, and PPO.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to Reinforcement Learning](#1-introduction)\n",
    "2. [Markov Decision Processes (MDPs)](#2-markov-decision-processes)\n",
    "3. [Classical RL: Q-Learning](#3-classical-rl-q-learning)\n",
    "4. [Deep Q-Networks (DQN)](#4-deep-q-networks-dqn)\n",
    "5. [Policy Gradient Methods](#5-policy-gradient-methods)\n",
    "6. [Actor-Critic Methods](#6-actor-critic-methods)\n",
    "7. [Proximal Policy Optimization (PPO)](#7-proximal-policy-optimization)\n",
    "8. [Advanced Topics](#8-advanced-topics)\n",
    "9. [Interview Questions](#9-interview-questions)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install gymnasium torch numpy matplotlib imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import gymnasium as gym\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Matplotlib settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction to Reinforcement Learning\n",
    "\n",
    "### What is Reinforcement Learning?\n",
    "\n",
    "**Reinforcement Learning (RL)** is learning by interaction with an environment to maximize cumulative reward.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Agent**: The learner/decision maker\n",
    "- **Environment**: What the agent interacts with\n",
    "- **State** ($s_t$): Current situation of the agent\n",
    "- **Action** ($a_t$): Decision made by agent\n",
    "- **Reward** ($r_t$): Feedback from environment\n",
    "- **Policy** ($\\pi$): Strategy for choosing actions\n",
    "\n",
    "**RL vs Other ML:**\n",
    "- **Supervised Learning**: Learn from labeled examples\n",
    "- **Unsupervised Learning**: Find patterns in data\n",
    "- **Reinforcement Learning**: Learn from rewards through trial and error\n",
    "\n",
    "### The RL Framework\n",
    "\n",
    "```\n",
    "Agent                    Environment\n",
    "  |                           |\n",
    "  |  ------ action a_t ---->  |\n",
    "  |                           |\n",
    "  |  <--- state s_t+1 ------  |\n",
    "  |  <--- reward r_t+1 -----  |\n",
    "```\n",
    "\n",
    "**Goal**: Learn policy $\\pi$ that maximizes expected cumulative reward:\n",
    "\n",
    "$$G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + ... = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}$$\n",
    "\n",
    "where $\\gamma \\in [0, 1]$ is the discount factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Grid World Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGridWorld:\n",
    "    \"\"\"\n",
    "    Simple 4x4 grid world environment\n",
    "    \n",
    "    Agent starts at (0, 0) and tries to reach goal at (3, 3)\n",
    "    Actions: up, down, left, right\n",
    "    Reward: -1 per step, +10 at goal\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, size=4):\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset to start state\"\"\"\n",
    "        self.agent_pos = [0, 0]\n",
    "        self.goal_pos = [self.size - 1, self.size - 1]\n",
    "        return tuple(self.agent_pos)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take action and return (next_state, reward, done)\n",
    "        \n",
    "        Actions: 0=up, 1=down, 2=left, 3=right\n",
    "        \"\"\"\n",
    "        # Update position based on action\n",
    "        if action == 0:  # up\n",
    "            self.agent_pos[0] = max(0, self.agent_pos[0] - 1)\n",
    "        elif action == 1:  # down\n",
    "            self.agent_pos[0] = min(self.size - 1, self.agent_pos[0] + 1)\n",
    "        elif action == 2:  # left\n",
    "            self.agent_pos[1] = max(0, self.agent_pos[1] - 1)\n",
    "        elif action == 3:  # right\n",
    "            self.agent_pos[1] = min(self.size - 1, self.agent_pos[1] + 1)\n",
    "        \n",
    "        # Compute reward\n",
    "        if self.agent_pos == self.goal_pos:\n",
    "            reward = 10.0\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -1.0\n",
    "            done = False\n",
    "        \n",
    "        return tuple(self.agent_pos), reward, done\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Visualize grid world\"\"\"\n",
    "        grid = np.zeros((self.size, self.size))\n",
    "        grid[self.agent_pos[0], self.agent_pos[1]] = 1  # Agent\n",
    "        grid[self.goal_pos[0], self.goal_pos[1]] = 2    # Goal\n",
    "        \n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.imshow(grid, cmap='viridis', vmin=0, vmax=2)\n",
    "        plt.colorbar(ticks=[0, 1, 2], label='0=Empty, 1=Agent, 2=Goal')\n",
    "        plt.title('Grid World')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# Test the environment\n",
    "env = SimpleGridWorld(size=4)\n",
    "print(\"Initial state:\", env.reset())\n",
    "env.render()\n",
    "\n",
    "# Take some random actions\n",
    "print(\"\\nTaking random actions:\")\n",
    "for i in range(5):\n",
    "    action = np.random.randint(0, 4)\n",
    "    state, reward, done = env.step(action)\n",
    "    print(f\"Action: {action}, State: {state}, Reward: {reward}, Done: {done}\")\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Markov Decision Processes (MDPs)\n",
    "\n",
    "### Mathematical Framework\n",
    "\n",
    "An MDP is defined by tuple $(S, A, P, R, \\gamma)$:\n",
    "- $S$: Set of states\n",
    "- $A$: Set of actions\n",
    "- $P(s'|s,a)$: Transition probability\n",
    "- $R(s,a,s')$: Reward function\n",
    "- $\\gamma$: Discount factor\n",
    "\n",
    "**Markov Property**: Future depends only on current state, not history:\n",
    "\n",
    "$$P(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, ...) = P(s_{t+1}|s_t, a_t)$$\n",
    "\n",
    "### Key Functions\n",
    "\n",
    "**State-Value Function** $V^\\pi(s)$: Expected return starting from state $s$ under policy $\\pi$\n",
    "\n",
    "$$V^\\pi(s) = E_\\pi[G_t | s_t = s] = E_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1} \\bigg| s_t = s\\right]$$\n",
    "\n",
    "**Action-Value Function** $Q^\\pi(s,a)$: Expected return starting from state $s$, taking action $a$, then following policy $\\pi$\n",
    "\n",
    "$$Q^\\pi(s,a) = E_\\pi[G_t | s_t = s, a_t = a]$$\n",
    "\n",
    "### Bellman Equations\n",
    "\n",
    "**Bellman Expectation Equation**:\n",
    "\n",
    "$$V^\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V^\\pi(s')]$$\n",
    "\n",
    "$$Q^\\pi(s,a) = \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma \\sum_{a'} \\pi(a'|s') Q^\\pi(s',a')]$$\n",
    "\n",
    "**Bellman Optimality Equation**:\n",
    "\n",
    "$$V^*(s) = \\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V^*(s')]$$\n",
    "\n",
    "$$Q^*(s,a) = \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma \\max_{a'} Q^*(s',a')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIteration:\n",
    "    \"\"\"\n",
    "    Value Iteration algorithm for solving MDPs\n",
    "    \n",
    "    Iteratively updates value function using Bellman optimality equation\n",
    "    until convergence.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, gamma=0.99, theta=1e-6):\n",
    "        self.env = env\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.theta = theta  # Convergence threshold\n",
    "        \n",
    "        # Initialize value function\n",
    "        self.V = np.zeros((env.size, env.size))\n",
    "        \n",
    "    def solve(self, max_iterations=1000):\n",
    "        \"\"\"\n",
    "        Run value iteration until convergence\n",
    "        \"\"\"\n",
    "        for iteration in range(max_iterations):\n",
    "            delta = 0\n",
    "            \n",
    "            # Update value for each state\n",
    "            for i in range(self.env.size):\n",
    "                for j in range(self.env.size):\n",
    "                    if [i, j] == self.env.goal_pos:\n",
    "                        continue  # Goal state has value 0\n",
    "                    \n",
    "                    v = self.V[i, j]\n",
    "                    \n",
    "                    # Compute max over actions\n",
    "                    action_values = []\n",
    "                    for action in range(4):  # 4 actions\n",
    "                        # Simulate taking action\n",
    "                        self.env.agent_pos = [i, j]\n",
    "                        next_state, reward, done = self.env.step(action)\n",
    "                        \n",
    "                        # Bellman update\n",
    "                        value = reward + self.gamma * self.V[next_state[0], next_state[1]]\n",
    "                        action_values.append(value)\n",
    "                    \n",
    "                    # Update value function\n",
    "                    self.V[i, j] = max(action_values)\n",
    "                    \n",
    "                    # Track change\n",
    "                    delta = max(delta, abs(v - self.V[i, j]))\n",
    "            \n",
    "            # Check convergence\n",
    "            if delta < self.theta:\n",
    "                print(f\"Converged after {iteration + 1} iterations\")\n",
    "                break\n",
    "        \n",
    "        return self.V\n",
    "    \n",
    "    def extract_policy(self):\n",
    "        \"\"\"\n",
    "        Extract optimal policy from value function\n",
    "        \"\"\"\n",
    "        policy = np.zeros((self.env.size, self.env.size), dtype=int)\n",
    "        \n",
    "        for i in range(self.env.size):\n",
    "            for j in range(self.env.size):\n",
    "                if [i, j] == self.env.goal_pos:\n",
    "                    continue\n",
    "                \n",
    "                # Find best action\n",
    "                action_values = []\n",
    "                for action in range(4):\n",
    "                    self.env.agent_pos = [i, j]\n",
    "                    next_state, reward, done = self.env.step(action)\n",
    "                    value = reward + self.gamma * self.V[next_state[0], next_state[1]]\n",
    "                    action_values.append(value)\n",
    "                \n",
    "                policy[i, j] = np.argmax(action_values)\n",
    "        \n",
    "        return policy\n",
    "    \n",
    "    def visualize(self, policy):\n",
    "        \"\"\"Visualize value function and policy\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Value function\n",
    "        im1 = ax1.imshow(self.V, cmap='viridis')\n",
    "        ax1.set_title('Value Function V(s)')\n",
    "        plt.colorbar(im1, ax=ax1)\n",
    "        \n",
    "        # Add values as text\n",
    "        for i in range(self.env.size):\n",
    "            for j in range(self.env.size):\n",
    "                ax1.text(j, i, f'{self.V[i, j]:.1f}',\n",
    "                        ha='center', va='center', color='white')\n",
    "        \n",
    "        # Policy\n",
    "        im2 = ax2.imshow(policy, cmap='tab10', vmin=0, vmax=3)\n",
    "        ax2.set_title('Optimal Policy')\n",
    "        plt.colorbar(im2, ax=ax2, ticks=[0, 1, 2, 3],\n",
    "                    label='0=up, 1=down, 2=left, 3=right')\n",
    "        \n",
    "        # Add arrows\n",
    "        arrows = ['↑', '↓', '←', '→']\n",
    "        for i in range(self.env.size):\n",
    "            for j in range(self.env.size):\n",
    "                if [i, j] != self.env.goal_pos:\n",
    "                    ax2.text(j, i, arrows[policy[i, j]],\n",
    "                            ha='center', va='center', fontsize=20, color='white')\n",
    "                else:\n",
    "                    ax2.text(j, i, 'G', ha='center', va='center',\n",
    "                            fontsize=20, color='red', weight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Solve grid world with value iteration\n",
    "env = SimpleGridWorld(size=4)\n",
    "vi = ValueIteration(env, gamma=0.99)\n",
    "\n",
    "print(\"Running Value Iteration...\")\n",
    "V = vi.solve()\n",
    "\n",
    "print(\"\\nExtracting optimal policy...\")\n",
    "policy = vi.extract_policy()\n",
    "\n",
    "print(\"\\nVisualization:\")\n",
    "vi.visualize(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Classical RL: Q-Learning\n",
    "\n",
    "### Q-Learning Algorithm\n",
    "\n",
    "**Q-Learning** is a model-free, off-policy TD control algorithm.\n",
    "\n",
    "**Update Rule**:\n",
    "\n",
    "$$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha [r_{t+1} + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t)]$$\n",
    "\n",
    "where:\n",
    "- $\\alpha$: Learning rate\n",
    "- $r_{t+1}$: Reward received\n",
    "- $\\gamma$: Discount factor\n",
    "- TD Error: $\\delta_t = r_{t+1} + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t)$\n",
    "\n",
    "**Key Features**:\n",
    "- **Model-free**: Doesn't need transition probabilities\n",
    "- **Off-policy**: Learns optimal policy while following exploratory policy\n",
    "- **Guaranteed convergence** (with appropriate learning rate schedule)\n",
    "\n",
    "### Exploration vs Exploitation\n",
    "\n",
    "**ε-greedy Policy**:\n",
    "- With probability $\\epsilon$: explore (random action)\n",
    "- With probability $1-\\epsilon$: exploit (best action)\n",
    "\n",
    "$$a_t = \\begin{cases}\n",
    "\\text{random action} & \\text{with probability } \\epsilon \\\\\n",
    "\\arg\\max_a Q(s_t, a) & \\text{with probability } 1-\\epsilon\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    Q-Learning agent for discrete state-action spaces\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, learning_rate=0.1,\n",
    "                 gamma=0.99, epsilon=1.0, epsilon_decay=0.995,\n",
    "                 epsilon_min=0.01):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        # Q-table: state -> action values\n",
    "        self.Q = {}\n",
    "    \n",
    "    def get_q_value(self, state, action):\n",
    "        \"\"\"Get Q-value for state-action pair\"\"\"\n",
    "        if state not in self.Q:\n",
    "            self.Q[state] = np.zeros(self.action_size)\n",
    "        return self.Q[state][action]\n",
    "    \n",
    "    def get_max_q_value(self, state):\n",
    "        \"\"\"Get maximum Q-value for state\"\"\"\n",
    "        if state not in self.Q:\n",
    "            self.Q[state] = np.zeros(self.action_size)\n",
    "        return np.max(self.Q[state])\n",
    "    \n",
    "    def select_action(self, state, training=True):\n",
    "        \"\"\"\n",
    "        Select action using ε-greedy policy\n",
    "        \"\"\"\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            # Explore: random action\n",
    "            return np.random.randint(self.action_size)\n",
    "        else:\n",
    "            # Exploit: best action\n",
    "            if state not in self.Q:\n",
    "                self.Q[state] = np.zeros(self.action_size)\n",
    "            return np.argmax(self.Q[state])\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Q-Learning update\n",
    "        \"\"\"\n",
    "        # Current Q-value\n",
    "        current_q = self.get_q_value(state, action)\n",
    "        \n",
    "        # TD target\n",
    "        if done:\n",
    "            td_target = reward\n",
    "        else:\n",
    "            td_target = reward + self.gamma * self.get_max_q_value(next_state)\n",
    "        \n",
    "        # TD error\n",
    "        td_error = td_target - current_q\n",
    "        \n",
    "        # Update Q-value\n",
    "        if state not in self.Q:\n",
    "            self.Q[state] = np.zeros(self.action_size)\n",
    "        self.Q[state][action] += self.lr * td_error\n",
    "        \n",
    "        return td_error\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay exploration rate\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "def train_q_learning(env, agent, num_episodes=500):\n",
    "    \"\"\"\n",
    "    Train Q-Learning agent\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        # Episode loop\n",
    "        while steps < 100:  # Max steps per episode\n",
    "            # Select action\n",
    "            action = agent.select_action(state, training=True)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # Update Q-value\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Decay epsilon\n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        # Track metrics\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-50:])\n",
    "            avg_length = np.mean(episode_lengths[-50:])\n",
    "            print(f\"Episode {episode + 1}/{num_episodes}, \"\n",
    "                  f\"Avg Reward: {avg_reward:.2f}, \"\n",
    "                  f\"Avg Length: {avg_length:.1f}, \"\n",
    "                  f\"Epsilon: {agent.epsilon:.3f}\")\n",
    "    \n",
    "    return episode_rewards, episode_lengths\n",
    "\n",
    "# Train Q-Learning agent\n",
    "env = SimpleGridWorld(size=4)\n",
    "agent = QLearningAgent(state_size=(4, 4), action_size=4,\n",
    "                      learning_rate=0.1, gamma=0.99,\n",
    "                      epsilon=1.0, epsilon_decay=0.995)\n",
    "\n",
    "print(\"Training Q-Learning agent...\\n\")\n",
    "rewards, lengths = train_q_learning(env, agent, num_episodes=500)\n",
    "\n",
    "# Plot learning curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Rewards\n",
    "ax1.plot(rewards, alpha=0.3, label='Episode Reward')\n",
    "ax1.plot(np.convolve(rewards, np.ones(50)/50, mode='valid'),\n",
    "         label='Moving Average (50 episodes)', linewidth=2)\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Total Reward')\n",
    "ax1.set_title('Q-Learning: Rewards over Time')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Episode lengths\n",
    "ax2.plot(lengths, alpha=0.3, label='Episode Length')\n",
    "ax2.plot(np.convolve(lengths, np.ones(50)/50, mode='valid'),\n",
    "         label='Moving Average (50 episodes)', linewidth=2)\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Steps to Goal')\n",
    "ax2.set_title('Q-Learning: Episode Length over Time')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test trained agent\n",
    "print(\"\\nTesting trained agent...\")\n",
    "state = env.reset()\n",
    "env.render()\n",
    "\n",
    "for step in range(20):\n",
    "    action = agent.select_action(state, training=False)\n",
    "    state, reward, done = env.step(action)\n",
    "    print(f\"Step {step + 1}: Action={action}, State={state}, Reward={reward}\")\n",
    "    \n",
    "    if done:\n",
    "        print(\"\\nGoal reached!\")\n",
    "        env.render()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Deep Q-Networks (DQN)\n",
    "\n",
    "### From Q-Learning to Deep Q-Learning\n",
    "\n",
    "**Problem with Q-Learning**: Cannot handle large/continuous state spaces (infinite table!)\n",
    "\n",
    "**Solution**: Approximate Q-function with neural network:\n",
    "\n",
    "$$Q(s, a; \\theta) \\approx Q^*(s, a)$$\n",
    "\n",
    "### DQN Innovations\n",
    "\n",
    "**1. Experience Replay**\n",
    "- Store transitions $(s, a, r, s')$ in replay buffer\n",
    "- Sample random minibatches for training\n",
    "- Breaks correlation between consecutive samples\n",
    "\n",
    "**2. Target Network**\n",
    "- Use separate network for computing targets\n",
    "- Update target network slowly (every C steps)\n",
    "- Stabilizes learning\n",
    "\n",
    "**Loss Function**:\n",
    "\n",
    "$$L(\\theta) = E_{(s,a,r,s') \\sim D}\\left[(r + \\gamma \\max_{a'} Q(s', a'; \\theta^-) - Q(s, a; \\theta))^2\\right]$$\n",
    "\n",
    "where $\\theta^-$ are target network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Experience Replay Buffer\n",
    "    \n",
    "    Stores transitions and samples random minibatches for training\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.Transition = namedtuple('Transition',\n",
    "                                    ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add transition to buffer\"\"\"\n",
    "        self.buffer.append(self.Transition(state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample random minibatch\"\"\"\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        # Transpose batch\n",
    "        batch = self.Transition(*zip(*transitions))\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(np.array(batch.state))\n",
    "        actions = torch.LongTensor(batch.action)\n",
    "        rewards = torch.FloatTensor(batch.reward)\n",
    "        next_states = torch.FloatTensor(np.array(batch.next_state))\n",
    "        dones = torch.FloatTensor(batch.done)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Q-Network\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"Forward pass: state -> Q-values for all actions\"\"\"\n",
    "        return self.network(state)\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    DQN Agent with experience replay and target network\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, learning_rate=1e-3,\n",
    "                 gamma=0.99, epsilon=1.0, epsilon_decay=0.995,\n",
    "                 epsilon_min=0.01, buffer_size=10000, batch_size=64,\n",
    "                 target_update=10):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        \n",
    "        # Q-network and target network\n",
    "        self.q_network = DQN(state_dim, action_dim)\n",
    "        self.target_network = DQN(state_dim, action_dim)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.target_network.eval()  # Target network in eval mode\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "        \n",
    "        # Training step counter\n",
    "        self.steps = 0\n",
    "    \n",
    "    def select_action(self, state, training=True):\n",
    "        \"\"\"\n",
    "        ε-greedy action selection\n",
    "        \"\"\"\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                q_values = self.q_network(state_tensor)\n",
    "                return q_values.argmax(dim=1).item()\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store transition in replay buffer\"\"\"\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        DQN update step\n",
    "        \"\"\"\n",
    "        # Need enough samples in buffer\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Sample minibatch\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Compute current Q-values\n",
    "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Compute target Q-values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(current_q_values, target_q_values)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network\n",
    "        self.steps += 1\n",
    "        if self.steps % self.target_update == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay exploration rate\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "# Create CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "print(f\"CartPole Environment:\")\n",
    "print(f\"State dimension: {state_dim}\")\n",
    "print(f\"Action dimension: {action_dim}\")\n",
    "print(f\"\\nState: [cart position, cart velocity, pole angle, pole angular velocity]\")\n",
    "print(f\"Actions: 0 = push left, 1 = push right\")\n",
    "print(f\"\\nGoal: Keep pole upright for as long as possible (max 500 steps)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(env, agent, num_episodes=500):\n",
    "    \"\"\"\n",
    "    Train DQN agent\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    losses = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        episode_losses = []\n",
    "        \n",
    "        # Episode loop\n",
    "        for step in range(500):  # Max 500 steps\n",
    "            # Select action\n",
    "            action = agent.select_action(state, training=True)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Store transition\n",
    "            agent.store_transition(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Update network\n",
    "            loss = agent.update()\n",
    "            if loss is not None:\n",
    "                episode_losses.append(loss)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Decay epsilon\n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        # Track metrics\n",
    "        episode_rewards.append(total_reward)\n",
    "        if episode_losses:\n",
    "            losses.append(np.mean(episode_losses))\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-50:])\n",
    "            print(f\"Episode {episode + 1}/{num_episodes}, \"\n",
    "                  f\"Avg Reward: {avg_reward:.1f}, \"\n",
    "                  f\"Epsilon: {agent.epsilon:.3f}, \"\n",
    "                  f\"Buffer Size: {len(agent.replay_buffer)}\")\n",
    "    \n",
    "    return episode_rewards, losses\n",
    "\n",
    "# Create and train DQN agent\n",
    "agent = DQNAgent(state_dim=state_dim, action_dim=action_dim,\n",
    "                learning_rate=1e-3, gamma=0.99,\n",
    "                epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01,\n",
    "                buffer_size=10000, batch_size=64, target_update=10)\n",
    "\n",
    "print(\"\\nTraining DQN agent on CartPole...\\n\")\n",
    "rewards, losses = train_dqn(env, agent, num_episodes=500)\n",
    "\n",
    "# Plot results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Rewards\n",
    "ax1.plot(rewards, alpha=0.3, label='Episode Reward')\n",
    "if len(rewards) >= 50:\n",
    "    ax1.plot(np.convolve(rewards, np.ones(50)/50, mode='valid'),\n",
    "             label='Moving Average (50 episodes)', linewidth=2)\n",
    "ax1.axhline(y=195, color='r', linestyle='--', label='Solved Threshold (195)')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Total Reward')\n",
    "ax1.set_title('DQN: Rewards over Time')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Loss\n",
    "if losses:\n",
    "    ax2.plot(losses, alpha=0.6, label='Training Loss')\n",
    "    if len(losses) >= 50:\n",
    "        ax2.plot(np.convolve(losses, np.ones(50)/50, mode='valid'),\n",
    "                 label='Moving Average (50 episodes)', linewidth=2)\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_title('DQN: Training Loss over Time')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test trained agent\n",
    "print(\"\\nTesting trained DQN agent...\")\n",
    "test_rewards = []\n",
    "for i in range(10):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(500):\n",
    "        action = agent.select_action(state, training=False)\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    test_rewards.append(total_reward)\n",
    "    print(f\"Test Episode {i + 1}: Reward = {total_reward:.0f}\")\n",
    "\n",
    "print(f\"\\nAverage Test Reward: {np.mean(test_rewards):.1f} ± {np.std(test_rewards):.1f}\")\n",
    "print(f\"CartPole is 'solved' when average reward > 195 over 100 episodes\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Policy Gradient Methods\n",
    "\n",
    "### From Value-Based to Policy-Based RL\n",
    "\n",
    "**Value-based** (Q-Learning, DQN): Learn value function, derive policy\n",
    "\n",
    "**Policy-based**: Directly learn policy $\\pi_\\theta(a|s)$\n",
    "\n",
    "### REINFORCE Algorithm\n",
    "\n",
    "**Objective**: Maximize expected return\n",
    "\n",
    "$$J(\\theta) = E_{\\tau \\sim \\pi_\\theta}[G_0] = E_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^T \\gamma^t r_t\\right]$$\n",
    "\n",
    "**Policy Gradient Theorem**:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = E_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) G_t\\right]$$\n",
    "\n",
    "**Interpretation**: \n",
    "- Increase probability of actions that lead to high returns\n",
    "- Decrease probability of actions that lead to low returns\n",
    "\n",
    "**Update Rule**:\n",
    "\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) G_t$$\n",
    "\n",
    "### Advantages\n",
    "\n",
    "✅ Can handle continuous action spaces  \n",
    "✅ Can learn stochastic policies  \n",
    "✅ Better convergence properties  \n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "❌ High variance  \n",
    "❌ Sample inefficient  \n",
    "❌ Can get stuck in local optima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy network for REINFORCE\n",
    "    \n",
    "    Outputs probability distribution over actions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1)  # Output probabilities\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"Forward pass: state -> action probabilities\"\"\"\n",
    "        return self.network(state)\n",
    "\n",
    "class REINFORCEAgent:\n",
    "    \"\"\"\n",
    "    REINFORCE (Monte Carlo Policy Gradient) agent\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, learning_rate=1e-3, gamma=0.99):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Policy network\n",
    "        self.policy = PolicyNetwork(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Episode memory\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Sample action from policy\n",
    "        \"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action_probs = self.policy(state_tensor)\n",
    "        \n",
    "        # Sample action from distribution\n",
    "        m = torch.distributions.Categorical(action_probs)\n",
    "        action = m.sample()\n",
    "        \n",
    "        # Store log probability\n",
    "        self.log_probs.append(m.log_prob(action))\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def store_reward(self, reward):\n",
    "        \"\"\"Store reward\"\"\"\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        REINFORCE update at end of episode\n",
    "        \"\"\"\n",
    "        # Compute returns (discounted cumulative rewards)\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for reward in reversed(self.rewards):\n",
    "            G = reward + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        \n",
    "        # Normalize returns (variance reduction)\n",
    "        returns = torch.FloatTensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
    "        \n",
    "        # Compute policy loss\n",
    "        policy_loss = []\n",
    "        for log_prob, G in zip(self.log_probs, returns):\n",
    "            policy_loss.append(-log_prob * G)\n",
    "        \n",
    "        policy_loss = torch.stack(policy_loss).sum()\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Clear episode memory\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        return policy_loss.item()\n",
    "\n",
    "def train_reinforce(env, agent, num_episodes=1000):\n",
    "    \"\"\"\n",
    "    Train REINFORCE agent\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    losses = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        # Collect episode\n",
    "        for step in range(500):  # Max 500 steps\n",
    "            # Select action\n",
    "            action = agent.select_action(state)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Store reward\n",
    "            agent.store_reward(reward)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Update policy at end of episode\n",
    "        loss = agent.update()\n",
    "        \n",
    "        # Track metrics\n",
    "        episode_rewards.append(total_reward)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-50:])\n",
    "            print(f\"Episode {episode + 1}/{num_episodes}, \"\n",
    "                  f\"Avg Reward: {avg_reward:.1f}\")\n",
    "    \n",
    "    return episode_rewards, losses\n",
    "\n",
    "# Create and train REINFORCE agent\n",
    "env = gym.make('CartPole-v1')\n",
    "agent = REINFORCEAgent(state_dim=state_dim, action_dim=action_dim,\n",
    "                      learning_rate=1e-3, gamma=0.99)\n",
    "\n",
    "print(\"Training REINFORCE agent on CartPole...\\n\")\n",
    "rewards, losses = train_reinforce(env, agent, num_episodes=1000)\n",
    "\n",
    "# Plot results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Rewards\n",
    "ax1.plot(rewards, alpha=0.3, label='Episode Reward')\n",
    "if len(rewards) >= 50:\n",
    "    ax1.plot(np.convolve(rewards, np.ones(50)/50, mode='valid'),\n",
    "             label='Moving Average (50 episodes)', linewidth=2)\n",
    "ax1.axhline(y=195, color='r', linestyle='--', label='Solved Threshold (195)')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Total Reward')\n",
    "ax1.set_title('REINFORCE: Rewards over Time')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Loss\n",
    "ax2.plot(losses, alpha=0.3, label='Policy Loss')\n",
    "if len(losses) >= 50:\n",
    "    ax2.plot(np.convolve(losses, np.ones(50)/50, mode='valid'),\n",
    "             label='Moving Average (50 episodes)', linewidth=2)\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('REINFORCE: Policy Loss over Time')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test trained agent\n",
    "print(\"\\nTesting trained REINFORCE agent...\")\n",
    "test_rewards = []\n",
    "for i in range(10):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(500):\n",
    "        action = agent.select_action(state)\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    # Clear episode memory\n",
    "    agent.log_probs = []\n",
    "    agent.rewards = []\n",
    "    \n",
    "    test_rewards.append(total_reward)\n",
    "    print(f\"Test Episode {i + 1}: Reward = {total_reward:.0f}\")\n",
    "\n",
    "print(f\"\\nAverage Test Reward: {np.mean(test_rewards):.1f} ± {np.std(test_rewards):.1f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Actor-Critic Methods\n",
    "\n",
    "### Combining Value-Based and Policy-Based RL\n",
    "\n",
    "**Actor-Critic** = Policy Gradient + Value Function\n",
    "\n",
    "**Actor** (Policy): $\\pi_\\theta(a|s)$ - decides which action to take\n",
    "\n",
    "**Critic** (Value Function): $V_\\phi(s)$ or $Q_\\phi(s,a)$ - evaluates actions\n",
    "\n",
    "### Advantage Actor-Critic (A2C)\n",
    "\n",
    "**Advantage Function**:\n",
    "\n",
    "$$A(s,a) = Q(s,a) - V(s)$$\n",
    "\n",
    "Measures how much better action $a$ is compared to average.\n",
    "\n",
    "**Policy Gradient with Advantage**:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = E[\\nabla_\\theta \\log \\pi_\\theta(a|s) A(s,a)]$$\n",
    "\n",
    "**TD-Error as Advantage**:\n",
    "\n",
    "$$A(s,a) \\approx \\delta = r + \\gamma V(s') - V(s)$$\n",
    "\n",
    "### Benefits\n",
    "\n",
    "✅ Lower variance than REINFORCE  \n",
    "✅ More sample efficient  \n",
    "✅ Online learning (update every step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor-Critic network\n",
    "    \n",
    "    Shared feature extractor with separate heads for:\n",
    "    - Actor: action probabilities\n",
    "    - Critic: state value\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        # Shared layers\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Actor head\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Critic head\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Returns: (action_probs, state_value)\n",
    "        \"\"\"\n",
    "        features = self.shared(state)\n",
    "        action_probs = self.actor(features)\n",
    "        state_value = self.critic(features)\n",
    "        return action_probs, state_value\n",
    "\n",
    "class A2CAgent:\n",
    "    \"\"\"\n",
    "    Advantage Actor-Critic (A2C) agent\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, learning_rate=1e-3, gamma=0.99):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Actor-Critic network\n",
    "        self.ac_network = ActorCritic(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.ac_network.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Select action from policy\n",
    "        \"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action_probs, _ = self.ac_network(state_tensor)\n",
    "        \n",
    "        # Sample action\n",
    "        m = torch.distributions.Categorical(action_probs)\n",
    "        action = m.sample()\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        A2C update (online, every step)\n",
    "        \"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "        \n",
    "        # Get action probs and values\n",
    "        action_probs, state_value = self.ac_network(state_tensor)\n",
    "        _, next_state_value = self.ac_network(next_state_tensor)\n",
    "        \n",
    "        # Compute TD target and advantage\n",
    "        if done:\n",
    "            td_target = reward\n",
    "        else:\n",
    "            td_target = reward + self.gamma * next_state_value.item()\n",
    "        \n",
    "        advantage = td_target - state_value.item()\n",
    "        \n",
    "        # Actor loss (policy gradient with advantage)\n",
    "        m = torch.distributions.Categorical(action_probs)\n",
    "        log_prob = m.log_prob(torch.tensor(action))\n",
    "        actor_loss = -log_prob * advantage\n",
    "        \n",
    "        # Critic loss (MSE between value and TD target)\n",
    "        critic_loss = F.mse_loss(state_value, torch.tensor([[td_target]]))\n",
    "        \n",
    "        # Total loss\n",
    "        loss = actor_loss + critic_loss\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "def train_a2c(env, agent, num_episodes=1000):\n",
    "    \"\"\"\n",
    "    Train A2C agent\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    losses = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        episode_losses = []\n",
    "        \n",
    "        # Episode loop\n",
    "        for step in range(500):  # Max 500 steps\n",
    "            # Select action\n",
    "            action = agent.select_action(state)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Update (online)\n",
    "            loss = agent.update(state, action, reward, next_state, done)\n",
    "            episode_losses.append(loss)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Track metrics\n",
    "        episode_rewards.append(total_reward)\n",
    "        losses.append(np.mean(episode_losses))\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-50:])\n",
    "            print(f\"Episode {episode + 1}/{num_episodes}, \"\n",
    "                  f\"Avg Reward: {avg_reward:.1f}\")\n",
    "    \n",
    "    return episode_rewards, losses\n",
    "\n",
    "# Create and train A2C agent\n",
    "env = gym.make('CartPole-v1')\n",
    "agent = A2CAgent(state_dim=state_dim, action_dim=action_dim,\n",
    "                learning_rate=1e-3, gamma=0.99)\n",
    "\n",
    "print(\"Training A2C agent on CartPole...\\n\")\n",
    "rewards, losses = train_a2c(env, agent, num_episodes=500)\n",
    "\n",
    "# Plot results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Rewards\n",
    "ax1.plot(rewards, alpha=0.3, label='Episode Reward')\n",
    "if len(rewards) >= 50:\n",
    "    ax1.plot(np.convolve(rewards, np.ones(50)/50, mode='valid'),\n",
    "             label='Moving Average (50 episodes)', linewidth=2)\n",
    "ax1.axhline(y=195, color='r', linestyle='--', label='Solved Threshold (195)')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Total Reward')\n",
    "ax1.set_title('A2C: Rewards over Time')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Loss\n",
    "ax2.plot(losses, alpha=0.3, label='Total Loss')\n",
    "if len(losses) >= 50:\n",
    "    ax2.plot(np.convolve(losses, np.ones(50)/50, mode='valid'),\n",
    "             label='Moving Average (50 episodes)', linewidth=2)\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('A2C: Loss over Time')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Proximal Policy Optimization (PPO)\n",
    "\n",
    "### The Challenge: Policy Gradient Instability\n",
    "\n",
    "Problem: Large policy updates can cause performance collapse\n",
    "\n",
    "### PPO Solution: Clipped Surrogate Objective\n",
    "\n",
    "**Probability Ratio**:\n",
    "\n",
    "$$r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$$\n",
    "\n",
    "**Clipped Objective**:\n",
    "\n",
    "$$L^{CLIP}(\\theta) = E_t[\\min(r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t)]$$\n",
    "\n",
    "where $\\epsilon$ is typically 0.2.\n",
    "\n",
    "**Interpretation**:\n",
    "- If advantage is positive: clip ratio to $[1, 1+\\epsilon]$ (don't increase prob too much)\n",
    "- If advantage is negative: clip ratio to $[1-\\epsilon, 1]$ (don't decrease prob too much)\n",
    "\n",
    "### Why PPO is Popular\n",
    "\n",
    "✅ Stable training  \n",
    "✅ Sample efficient  \n",
    "✅ Works well with continuous actions  \n",
    "✅ State-of-the-art performance  \n",
    "✅ Easy to implement  \n",
    "\n",
    "**Used in**: OpenAI Five (Dota 2), DeepMind's AlphaStar (StarCraft II)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Advanced Topics\n",
    "\n",
    "### 8.1 Multi-Armed Bandits\n",
    "\n",
    "**Simplest RL problem**: No states, just actions and rewards\n",
    "\n",
    "**Exploration Strategies**:\n",
    "- ε-greedy\n",
    "- Upper Confidence Bound (UCB)\n",
    "- Thompson Sampling\n",
    "\n",
    "### 8.2 Model-Based RL\n",
    "\n",
    "**Learn environment model**: $P(s'|s,a)$ and $R(s,a)$\n",
    "\n",
    "**Then plan** using the model\n",
    "\n",
    "**Advantages**:\n",
    "- More sample efficient\n",
    "- Can plan ahead\n",
    "\n",
    "### 8.3 Offline RL\n",
    "\n",
    "Learn from fixed dataset (no environment interaction)\n",
    "\n",
    "**Applications**: Healthcare, robotics, autonomous driving\n",
    "\n",
    "### 8.4 Multi-Agent RL\n",
    "\n",
    "Multiple agents learning simultaneously\n",
    "\n",
    "**Challenges**:\n",
    "- Non-stationary environment\n",
    "- Credit assignment\n",
    "- Communication\n",
    "\n",
    "### 8.5 Hierarchical RL\n",
    "\n",
    "Learn hierarchical policies (high-level goals → low-level actions)\n",
    "\n",
    "**Approaches**:\n",
    "- Options framework\n",
    "- Feudal RL\n",
    "- HIRO\n",
    "\n",
    "### 8.6 Inverse RL\n",
    "\n",
    "Learn reward function from expert demonstrations\n",
    "\n",
    "**Applications**: Imitation learning, apprenticeship learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Interview Questions\n",
    "\n",
    "### Fundamentals\n",
    "\n",
    "**Q1: What is the difference between supervised learning and reinforcement learning?**\n",
    "\n",
    "**A**: \n",
    "- **Supervised Learning**: Learn from labeled examples $(x, y)$. Goal: minimize prediction error.\n",
    "- **Reinforcement Learning**: Learn from rewards through trial and error. Goal: maximize cumulative reward.\n",
    "\n",
    "Key differences:\n",
    "- SL: Direct feedback (correct answer)\n",
    "- RL: Delayed feedback (reward after sequence of actions)\n",
    "- SL: i.i.d. data\n",
    "- RL: Sequential, correlated data\n",
    "\n",
    "---\n",
    "\n",
    "**Q2: Explain the exploration-exploitation tradeoff.**\n",
    "\n",
    "**A**: \n",
    "- **Exploitation**: Use current knowledge to maximize reward (greedy)\n",
    "- **Exploration**: Try new actions to gain more information\n",
    "\n",
    "Too much exploitation: May miss better options  \n",
    "Too much exploration: Waste time on suboptimal actions\n",
    "\n",
    "Solutions: ε-greedy, UCB, Thompson Sampling\n",
    "\n",
    "---\n",
    "\n",
    "**Q3: What is the discount factor γ and why is it important?**\n",
    "\n",
    "**A**: Discount factor $\\gamma \\in [0, 1]$ determines how much we value future rewards:\n",
    "\n",
    "$$G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + ...$$\n",
    "\n",
    "- $\\gamma = 0$: Only care about immediate reward (myopic)\n",
    "- $\\gamma = 1$: All future rewards equally important\n",
    "- $\\gamma = 0.99$ (typical): Future rewards slightly discounted\n",
    "\n",
    "Why discount?\n",
    "1. Uncertainty increases with time\n",
    "2. Mathematical convergence\n",
    "3. Finite horizon episodes\n",
    "\n",
    "---\n",
    "\n",
    "**Q4: What is the Bellman equation?**\n",
    "\n",
    "**A**: Recursive relationship for value functions:\n",
    "\n",
    "$$V^\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V^\\pi(s')]$$\n",
    "\n",
    "Interpretation: Value of state = Expected immediate reward + Discounted value of next state\n",
    "\n",
    "Optimality version:\n",
    "\n",
    "$$V^*(s) = \\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V^*(s')]$$\n",
    "\n",
    "---\n",
    "\n",
    "### Algorithms\n",
    "\n",
    "**Q5: Compare Q-Learning and SARSA.**\n",
    "\n",
    "**A**:\n",
    "\n",
    "**Q-Learning (off-policy)**:\n",
    "- Update: $Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]$\n",
    "- Learns optimal policy while following exploratory policy\n",
    "- More aggressive, can diverge\n",
    "\n",
    "**SARSA (on-policy)**:\n",
    "- Update: $Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma Q(s',a') - Q(s,a)]$\n",
    "- Learns policy it's actually following\n",
    "- More conservative, safer\n",
    "\n",
    "---\n",
    "\n",
    "**Q6: Why does DQN use experience replay and target networks?**\n",
    "\n",
    "**A**:\n",
    "\n",
    "**Experience Replay**:\n",
    "- **Problem**: Consecutive samples are highly correlated\n",
    "- **Solution**: Store transitions, sample random minibatches\n",
    "- **Benefits**: Breaks correlation, reuses data (sample efficient)\n",
    "\n",
    "**Target Network**:\n",
    "- **Problem**: Chasing moving target (Q-values change during training)\n",
    "- **Solution**: Separate network for targets, update slowly\n",
    "- **Benefits**: Stabilizes training, prevents oscillations\n",
    "\n",
    "---\n",
    "\n",
    "**Q7: What is the advantage function and why is it useful?**\n",
    "\n",
    "**A**: \n",
    "\n",
    "$$A(s,a) = Q(s,a) - V(s)$$\n",
    "\n",
    "Measures: How much better is action $a$ compared to average?\n",
    "\n",
    "**Benefits**:\n",
    "- Reduces variance in policy gradients\n",
    "- Centers rewards around zero\n",
    "- Only updates based on relative advantage\n",
    "\n",
    "Used in: A2C, PPO, GAE (Generalized Advantage Estimation)\n",
    "\n",
    "---\n",
    "\n",
    "**Q8: Compare value-based and policy-based methods.**\n",
    "\n",
    "**A**:\n",
    "\n",
    "**Value-Based (Q-Learning, DQN)**:\n",
    "- Learn: Value function $Q(s,a)$\n",
    "- Policy: $\\pi(s) = \\arg\\max_a Q(s,a)$\n",
    "- Pros: Sample efficient, stable\n",
    "- Cons: Can't handle continuous actions, deterministic policy\n",
    "\n",
    "**Policy-Based (REINFORCE, PPO)**:\n",
    "- Learn: Policy $\\pi_\\theta(a|s)$ directly\n",
    "- Pros: Continuous actions, stochastic policies\n",
    "- Cons: High variance, sample inefficient\n",
    "\n",
    "**Actor-Critic**: Best of both worlds!\n",
    "\n",
    "---\n",
    "\n",
    "### Advanced\n",
    "\n",
    "**Q9: How would you handle continuous action spaces?**\n",
    "\n",
    "**A**: Several approaches:\n",
    "\n",
    "1. **Policy Gradient Methods** (PPO, DDPG):\n",
    "   - Output Gaussian distribution: $\\pi_\\theta(a|s) = N(\\mu_\\theta(s), \\sigma_\\theta(s))$\n",
    "   - Sample from distribution\n",
    "\n",
    "2. **Discretization**:\n",
    "   - Bin continuous space into discrete actions\n",
    "   - Simple but loses precision\n",
    "\n",
    "3. **Action Parameterization** (DDPG, TD3, SAC):\n",
    "   - Actor outputs continuous action directly\n",
    "   - Critic evaluates state-action pairs\n",
    "\n",
    "---\n",
    "\n",
    "**Q10: What are the main challenges in deep RL?**\n",
    "\n",
    "**A**:\n",
    "\n",
    "1. **Sample Efficiency**: Need millions of samples (expensive in real world)\n",
    "2. **Stability**: Non-stationary targets, moving distributions\n",
    "3. **Credit Assignment**: Which action caused the reward?\n",
    "4. **Exploration**: How to explore efficiently in large spaces?\n",
    "5. **Sparse Rewards**: Long sequences before reward\n",
    "6. **Partial Observability**: Don't see full state\n",
    "7. **Reproducibility**: Sensitive to hyperparameters, random seeds\n",
    "\n",
    "**Solutions**:\n",
    "- Experience replay, target networks (stability)\n",
    "- Curiosity-driven exploration\n",
    "- Hindsight Experience Replay (sparse rewards)\n",
    "- Recurrent networks (partial observability)\n",
    "- Model-based RL (sample efficiency)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**We covered**:\n",
    "\n",
    "1. **RL Fundamentals**: MDPs, value functions, policies\n",
    "2. **Classical RL**: Q-Learning, value iteration\n",
    "3. **Deep RL**: DQN with experience replay and target networks\n",
    "4. **Policy Gradients**: REINFORCE algorithm\n",
    "5. **Actor-Critic**: A2C combining value and policy\n",
    "6. **Advanced**: PPO, multi-agent RL, model-based RL\n",
    "\n",
    "**Key Takeaways**:\n",
    "- RL learns from trial and error to maximize cumulative reward\n",
    "- Exploration vs exploitation is fundamental\n",
    "- Value-based methods (DQN) vs Policy-based (REINFORCE) vs Actor-Critic (best of both)\n",
    "- Deep RL enables solving complex tasks (games, robotics, etc.)\n",
    "- Still active research area with many open challenges\n",
    "\n",
    "**Further Learning**:\n",
    "- Sutton & Barto: \"Reinforcement Learning: An Introduction\"\n",
    "- OpenAI Spinning Up: https://spinningup.openai.com/\n",
    "- DeepMind x UCL RL Course\n",
    "\n",
    "---\n",
    "\n",
    "**References**:\n",
    "1. Sutton & Barto, \"Reinforcement Learning: An Introduction\" (2018)\n",
    "2. Mnih et al., \"Playing Atari with Deep Reinforcement Learning\" (2013)\n",
    "3. Mnih et al., \"Human-level control through deep reinforcement learning\" (2015)\n",
    "4. Schulman et al., \"Proximal Policy Optimization Algorithms\" (2017)\n",
    "5. Lillicrap et al., \"Continuous control with deep reinforcement learning\" (2015)\n",
    "\n",
    "---\n",
    "\n",
    "*Created: October 2025*  \n",
    "*Last Updated: October 2025*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
