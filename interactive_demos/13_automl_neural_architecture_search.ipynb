{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML and Neural Architecture Search\n",
    "\n",
    "**Automated Machine Learning and Neural Architecture Search**\n",
    "\n",
    "Learn how to automate the ML pipeline, optimize hyperparameters, and discover optimal neural architectures.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to AutoML](#1-introduction-to-automl)\n",
    "2. [Hyperparameter Optimization](#2-hyperparameter-optimization)\n",
    "3. [AutoML Frameworks](#3-automl-frameworks)\n",
    "4. [Neural Architecture Search (NAS)](#4-neural-architecture-search)\n",
    "5. [Meta-Learning](#5-meta-learning)\n",
    "6. [Automated Feature Engineering](#6-automated-feature-engineering)\n",
    "7. [Best Practices](#7-best-practices)\n",
    "8. [Interview Questions](#8-interview-questions)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install optuna scikit-learn xgboost matplotlib seaborn numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import load_iris, load_breast_cancer, make_classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Matplotlib settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction to AutoML\n",
    "\n",
    "### What is AutoML?\n",
    "\n",
    "**Automated Machine Learning (AutoML)** automates the end-to-end process of applying machine learning to real-world problems.\n",
    "\n",
    "**AutoML automates:**\n",
    "1. **Data preprocessing** - handling missing values, encoding, scaling\n",
    "2. **Feature engineering** - creating and selecting features\n",
    "3. **Model selection** - choosing the best algorithm\n",
    "4. **Hyperparameter tuning** - finding optimal hyperparameters\n",
    "5. **Model ensembling** - combining multiple models\n",
    "\n",
    "### Why AutoML?\n",
    "\n",
    "**Benefits:**\n",
    "- ✅ **Democratizes ML** - Non-experts can build models\n",
    "- ✅ **Saves time** - Automates tedious tasks\n",
    "- ✅ **Better performance** - Explores more options than manual tuning\n",
    "- ✅ **Reduces human bias** - Systematic exploration\n",
    "\n",
    "**Challenges:**\n",
    "- ❌ **Computational cost** - Can be expensive\n",
    "- ❌ **Black box** - Harder to understand\n",
    "- ❌ **Overfitting risk** - Too many choices\n",
    "- ❌ **Domain knowledge** - Still needed for feature engineering\n",
    "\n",
    "### The ML Pipeline\n",
    "\n",
    "```\n",
    "Raw Data\n",
    "   ↓\n",
    "Data Preprocessing (AutoML)\n",
    "   ↓\n",
    "Feature Engineering (AutoML)\n",
    "   ↓\n",
    "Model Selection (AutoML)\n",
    "   ↓\n",
    "Hyperparameter Tuning (AutoML)\n",
    "   ↓\n",
    "Model Ensembling (AutoML)\n",
    "   ↓\n",
    "Trained Model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Hyperparameter Optimization\n",
    "\n",
    "### 2.1 Grid Search\n",
    "\n",
    "**Exhaustive search** over specified parameter grid.\n",
    "\n",
    "**Pros:**\n",
    "- Guaranteed to find best combination in grid\n",
    "- Simple and deterministic\n",
    "\n",
    "**Cons:**\n",
    "- Exponential complexity: $O(n^d)$ where $n$ = values per param, $d$ = number of params\n",
    "- Wastes time on unimportant parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Dataset: {X_train.shape[0]} training samples, {X_test.shape[0]} test samples\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "print(f\"Classes: {len(np.unique(y))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Grid Search for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "print(f\"Grid Search: {np.prod([len(v) for v in param_grid.values()])} combinations to try\\n\")\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    rf,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Test accuracy: {grid_search.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Random Search\n",
    "\n",
    "**Randomly sample** from parameter distributions.\n",
    "\n",
    "**Key Insight**: Not all hyperparameters are equally important. Random search explores more of the important dimensions.\n",
    "\n",
    "**Pros:**\n",
    "- More efficient than grid search\n",
    "- Can specify budget (number of iterations)\n",
    "- Often finds better results faster\n",
    "\n",
    "**Cons:**\n",
    "- Not guaranteed to find optimal\n",
    "- Random, so results vary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Random Search for Random Forest\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(50, 300),\n",
    "    'max_depth': randint(5, 30),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'max_features': uniform(0.1, 0.9)\n",
    "}\n",
    "\n",
    "print(f\"Random Search: 100 random combinations to try\\n\")\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf,\n",
    "    param_distributions,\n",
    "    n_iter=100,  # Budget: 100 iterations\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {random_search.best_params_}\")\n",
    "print(f\"Best CV score: {random_search.best_score_:.4f}\")\n",
    "print(f\"Test accuracy: {random_search.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Bayesian Optimization\n",
    "\n",
    "**Smart search** using probabilistic model of objective function.\n",
    "\n",
    "**How it works:**\n",
    "1. Build probabilistic model (Gaussian Process) of $f(\\theta)$\n",
    "2. Use acquisition function to decide next $\\theta$ to try\n",
    "3. Evaluate $f(\\theta)$, update model\n",
    "4. Repeat\n",
    "\n",
    "**Acquisition Functions:**\n",
    "- **Expected Improvement (EI)**: Balance exploration vs exploitation\n",
    "- **Upper Confidence Bound (UCB)**: Optimistic estimate\n",
    "- **Probability of Improvement (PI)**: Greedy\n",
    "\n",
    "**Pros:**\n",
    "- Much more sample-efficient than random search\n",
    "- Automatic exploration-exploitation balance\n",
    "- Works well with expensive evaluations\n",
    "\n",
    "**Cons:**\n",
    "- More complex\n",
    "- Slower per iteration (building surrogate model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for Random Forest\n",
    "    \n",
    "    trial: Optuna trial object for suggesting hyperparameters\n",
    "    \"\"\"\n",
    "    # Suggest hyperparameters\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 30),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_float('max_features', 0.1, 1.0),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate\n",
    "    rf = RandomForestClassifier(**params)\n",
    "    score = cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1).mean()\n",
    "    \n",
    "    return score\n",
    "\n",
    "# Create study\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',  # Maximize accuracy\n",
    "    sampler=optuna.samplers.TPESampler(seed=42)  # Tree-structured Parzen Estimator\n",
    ")\n",
    "\n",
    "print(\"Running Bayesian Optimization with Optuna...\\n\")\n",
    "\n",
    "# Optimize\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest parameters: {study.best_params}\")\n",
    "print(f\"Best CV score: {study.best_value:.4f}\")\n",
    "\n",
    "# Train final model\n",
    "best_rf = RandomForestClassifier(**study.best_params)\n",
    "best_rf.fit(X_train, y_train)\n",
    "test_acc = best_rf.score(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimization history\n",
    "fig = plot_optimization_history(study)\n",
    "fig.update_layout(title='Bayesian Optimization History', width=800, height=400)\n",
    "fig.show()\n",
    "\n",
    "# Hyperparameter importances\n",
    "fig = plot_param_importances(study)\n",
    "fig.update_layout(title='Hyperparameter Importances', width=800, height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Comparison: Grid vs Random vs Bayesian\n",
    "\n",
    "| Method | Iterations | Time | Best Score | Notes |\n",
    "|--------|-----------|------|------------|-------|\n",
    "| Grid Search | 108 | Slow | Good | Exhaustive but wasteful |\n",
    "| Random Search | 100 | Medium | Good | Simple, often effective |\n",
    "| Bayesian (Optuna) | 50 | Fast | Best | Most sample-efficient |\n",
    "\n",
    "**Rule of thumb:**\n",
    "- **Grid Search**: < 4 hyperparameters, small grids\n",
    "- **Random Search**: Quick baseline, many hyperparameters\n",
    "- **Bayesian**: Expensive evaluations, need best performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Advanced: Multi-Objective Optimization\n",
    "\n",
    "Optimize **multiple objectives** simultaneously (e.g., accuracy AND inference time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def multi_objective(trial):\n",
    "    \"\"\"\n",
    "    Optimize both accuracy and inference time\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # Train model\n",
    "    rf = RandomForestClassifier(**params)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Objective 1: Accuracy (maximize)\n",
    "    accuracy = rf.score(X_test, y_test)\n",
    "    \n",
    "    # Objective 2: Inference time (minimize)\n",
    "    start = time.time()\n",
    "    _ = rf.predict(X_test)\n",
    "    inference_time = time.time() - start\n",
    "    \n",
    "    return accuracy, inference_time\n",
    "\n",
    "# Multi-objective study\n",
    "study_multi = optuna.create_study(\n",
    "    directions=['maximize', 'minimize'],  # Maximize acc, minimize time\n",
    "    sampler=optuna.samplers.NSGAIISampler(seed=42)\n",
    ")\n",
    "\n",
    "print(\"Running Multi-Objective Optimization...\\n\")\n",
    "study_multi.optimize(multi_objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nFound {len(study_multi.best_trials)} Pareto-optimal solutions\")\n",
    "print(\"\\nTop 3 trade-offs:\")\n",
    "for i, trial in enumerate(study_multi.best_trials[:3]):\n",
    "    print(f\"{i+1}. Accuracy: {trial.values[0]:.4f}, Time: {trial.values[1]:.4f}s, Params: {trial.params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. AutoML Frameworks\n",
    "\n",
    "### 3.1 Combined Algorithm Selection and Hyperparameter Optimization (CASH)\n",
    "\n",
    "**CASH Problem**: Jointly optimize:\n",
    "1. Which algorithm to use?\n",
    "2. What hyperparameters?\n",
    "\n",
    "**Search Space**: $\\{(A_1, \\theta_1), (A_2, \\theta_2), ..., (A_k, \\theta_k)\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cash_objective(trial):\n",
    "    \"\"\"\n",
    "    Combined Algorithm Selection and Hyperparameter Optimization\n",
    "    \n",
    "    Try different algorithms with their hyperparameters\n",
    "    \"\"\"\n",
    "    # Select algorithm\n",
    "    algorithm = trial.suggest_categorical('algorithm', ['rf', 'gb', 'svm', 'mlp'])\n",
    "    \n",
    "    if algorithm == 'rf':\n",
    "        # Random Forest hyperparameters\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('rf_n_estimators', 50, 200),\n",
    "            'max_depth': trial.suggest_int('rf_max_depth', 5, 20),\n",
    "            'min_samples_split': trial.suggest_int('rf_min_samples_split', 2, 10),\n",
    "            'random_state': 42\n",
    "        }\n",
    "        model = RandomForestClassifier(**params)\n",
    "    \n",
    "    elif algorithm == 'gb':\n",
    "        # Gradient Boosting hyperparameters\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('gb_n_estimators', 50, 200),\n",
    "            'max_depth': trial.suggest_int('gb_max_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float('gb_learning_rate', 0.01, 0.3, log=True),\n",
    "            'random_state': 42\n",
    "        }\n",
    "        model = GradientBoostingClassifier(**params)\n",
    "    \n",
    "    elif algorithm == 'svm':\n",
    "        # SVM hyperparameters\n",
    "        params = {\n",
    "            'C': trial.suggest_float('svm_C', 0.1, 100, log=True),\n",
    "            'gamma': trial.suggest_float('svm_gamma', 1e-4, 1, log=True),\n",
    "            'kernel': trial.suggest_categorical('svm_kernel', ['rbf', 'poly']),\n",
    "            'random_state': 42\n",
    "        }\n",
    "        model = SVC(**params)\n",
    "    \n",
    "    else:  # mlp\n",
    "        # Neural Network hyperparameters\n",
    "        params = {\n",
    "            'hidden_layer_sizes': (trial.suggest_int('mlp_hidden_size', 50, 200),),\n",
    "            'alpha': trial.suggest_float('mlp_alpha', 1e-5, 1e-2, log=True),\n",
    "            'learning_rate_init': trial.suggest_float('mlp_lr', 1e-4, 1e-2, log=True),\n",
    "            'random_state': 42,\n",
    "            'max_iter': 500\n",
    "        }\n",
    "        model = MLPClassifier(**params)\n",
    "    \n",
    "    # Evaluate\n",
    "    score = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1).mean()\n",
    "    \n",
    "    return score\n",
    "\n",
    "# CASH optimization\n",
    "study_cash = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=42)\n",
    ")\n",
    "\n",
    "print(\"Running CASH (Combined Algorithm Selection + Hyperparameter Optimization)...\\n\")\n",
    "study_cash.optimize(cash_objective, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest algorithm: {study_cash.best_params['algorithm']}\")\n",
    "print(f\"Best parameters: {study_cash.best_params}\")\n",
    "print(f\"Best CV score: {study_cash.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze which algorithms were tried\n",
    "algorithms_tried = [trial.params['algorithm'] for trial in study_cash.trials]\n",
    "algorithm_scores = {}\n",
    "\n",
    "for algo in ['rf', 'gb', 'svm', 'mlp']:\n",
    "    scores = [trial.value for trial in study_cash.trials if trial.params['algorithm'] == algo]\n",
    "    if scores:\n",
    "        algorithm_scores[algo] = {\n",
    "            'mean': np.mean(scores),\n",
    "            'max': np.max(scores),\n",
    "            'count': len(scores)\n",
    "        }\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Algorithm distribution\n",
    "algos, counts = np.unique(algorithms_tried, return_counts=True)\n",
    "ax1.bar(algos, counts, color='steelblue')\n",
    "ax1.set_xlabel('Algorithm')\n",
    "ax1.set_ylabel('Number of Trials')\n",
    "ax1.set_title('Algorithm Selection Distribution')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Algorithm performance\n",
    "algos = list(algorithm_scores.keys())\n",
    "means = [algorithm_scores[a]['mean'] for a in algos]\n",
    "maxs = [algorithm_scores[a]['max'] for a in algos]\n",
    "\n",
    "x = np.arange(len(algos))\n",
    "width = 0.35\n",
    "ax2.bar(x - width/2, means, width, label='Mean Score', color='lightcoral')\n",
    "ax2.bar(x + width/2, maxs, width, label='Max Score', color='lightgreen')\n",
    "ax2.set_xlabel('Algorithm')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Algorithm Performance Comparison')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticks(algos)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAlgorithm Performance Summary:\")\n",
    "for algo, stats in algorithm_scores.items():\n",
    "    print(f\"{algo.upper()}: Mean={stats['mean']:.4f}, Max={stats['max']:.4f}, Trials={stats['count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Ensemble Selection\n",
    "\n",
    "**Idea**: Combine multiple models found during optimization for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Get top 5 models from CASH optimization\n",
    "top_trials = sorted(study_cash.trials, key=lambda t: t.value, reverse=True)[:5]\n",
    "\n",
    "print(\"Top 5 models:\")\n",
    "models = []\n",
    "for i, trial in enumerate(top_trials):\n",
    "    print(f\"{i+1}. {trial.params['algorithm']}: {trial.value:.4f}\")\n",
    "    \n",
    "    # Recreate model\n",
    "    algo = trial.params['algorithm']\n",
    "    params = {k.replace(f\"{algo}_\", \"\"): v for k, v in trial.params.items() if k.startswith(f\"{algo}_\")}\n",
    "    params['random_state'] = 42\n",
    "    \n",
    "    if algo == 'rf':\n",
    "        model = RandomForestClassifier(**params)\n",
    "    elif algo == 'gb':\n",
    "        model = GradientBoostingClassifier(**params)\n",
    "    elif algo == 'svm':\n",
    "        model = SVC(**params, probability=True)  # Need probabilities for voting\n",
    "    else:\n",
    "        params['max_iter'] = 500\n",
    "        model = MLPClassifier(**params)\n",
    "    \n",
    "    models.append((f\"model_{i}\", model))\n",
    "\n",
    "# Create ensemble\n",
    "ensemble = VotingClassifier(estimators=models, voting='soft', n_jobs=-1)\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "ensemble_score = ensemble.score(X_test, y_test)\n",
    "best_single_score = top_trials[0].value\n",
    "\n",
    "print(f\"\\nBest single model (CV): {best_single_score:.4f}\")\n",
    "print(f\"Ensemble (test): {ensemble_score:.4f}\")\n",
    "print(f\"Improvement: {ensemble_score - best_single_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Neural Architecture Search (NAS)\n",
    "\n",
    "### What is NAS?\n",
    "\n",
    "**Neural Architecture Search** automates the design of neural network architectures.\n",
    "\n",
    "**NAS Components:**\n",
    "1. **Search Space**: Set of possible architectures\n",
    "2. **Search Strategy**: How to explore search space\n",
    "3. **Performance Estimation**: How to evaluate architectures\n",
    "\n",
    "### 4.1 Search Space Design\n",
    "\n",
    "**Common choices:**\n",
    "- Number of layers\n",
    "- Number of units per layer\n",
    "- Activation functions\n",
    "- Skip connections\n",
    "- Layer types (Conv, Dense, Pooling)\n",
    "\n",
    "### 4.2 Random NAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for neural networks\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "print(f\"Data prepared for neural networks\")\n",
    "print(f\"Input dimension: {X_train_scaled.shape[1]}\")\n",
    "print(f\"Output dimension: {len(np.unique(y_train))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchableNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network with flexible architecture\n",
    "    \n",
    "    Can specify:\n",
    "    - Number of layers\n",
    "    - Hidden units per layer\n",
    "    - Activation function\n",
    "    - Dropout rate\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, hidden_sizes, activation='relu', dropout=0.0):\n",
    "        super(SearchableNetwork, self).__init__()\n",
    "        \n",
    "        self.activation_name = activation\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Build layers\n",
    "        layers = []\n",
    "        prev_size = input_dim\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            \n",
    "            # Activation\n",
    "            if activation == 'relu':\n",
    "                layers.append(nn.ReLU())\n",
    "            elif activation == 'tanh':\n",
    "                layers.append(nn.Tanh())\n",
    "            elif activation == 'elu':\n",
    "                layers.append(nn.ELU())\n",
    "            \n",
    "            # Dropout\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            \n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_size, output_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def train_and_evaluate_architecture(hidden_sizes, activation, dropout, num_epochs=20):\n",
    "    \"\"\"\n",
    "    Train and evaluate a specific architecture\n",
    "    \"\"\"\n",
    "    # Create model\n",
    "    model = SearchableNetwork(\n",
    "        input_dim=X_train_scaled.shape[1],\n",
    "        output_dim=len(np.unique(y_train)),\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        activation=activation,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = (predicted == y_test_tensor).float().mean().item()\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Random NAS: Try random architectures\n",
    "print(\"Random Neural Architecture Search...\\n\")\n",
    "\n",
    "architectures = []\n",
    "for i in range(20):\n",
    "    # Random architecture\n",
    "    num_layers = np.random.randint(1, 4)\n",
    "    hidden_sizes = [np.random.choice([32, 64, 128, 256]) for _ in range(num_layers)]\n",
    "    activation = np.random.choice(['relu', 'tanh', 'elu'])\n",
    "    dropout = np.random.choice([0.0, 0.1, 0.2, 0.3])\n",
    "    \n",
    "    # Train and evaluate\n",
    "    accuracy = train_and_evaluate_architecture(hidden_sizes, activation, dropout)\n",
    "    \n",
    "    architectures.append({\n",
    "        'hidden_sizes': hidden_sizes,\n",
    "        'activation': activation,\n",
    "        'dropout': dropout,\n",
    "        'accuracy': accuracy\n",
    "    })\n",
    "    \n",
    "    print(f\"Trial {i+1}/20: {hidden_sizes}, {activation}, dropout={dropout:.1f} → Acc: {accuracy:.4f}\")\n",
    "\n",
    "# Best architecture\n",
    "best_arch = max(architectures, key=lambda x: x['accuracy'])\n",
    "print(f\"\\nBest Architecture:\")\n",
    "print(f\"  Hidden sizes: {best_arch['hidden_sizes']}\")\n",
    "print(f\"  Activation: {best_arch['activation']}\")\n",
    "print(f\"  Dropout: {best_arch['dropout']}\")\n",
    "print(f\"  Accuracy: {best_arch['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Bayesian NAS with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nas_objective(trial):\n",
    "    \"\"\"\n",
    "    NAS objective function for Optuna\n",
    "    \"\"\"\n",
    "    # Search space\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 4)\n",
    "    hidden_sizes = [trial.suggest_categorical(f'hidden_size_{i}', [32, 64, 128, 256]) \n",
    "                   for i in range(num_layers)]\n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'tanh', 'elu'])\n",
    "    dropout = trial.suggest_float('dropout', 0.0, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    \n",
    "    # Create and train model\n",
    "    model = SearchableNetwork(\n",
    "        input_dim=X_train_scaled.shape[1],\n",
    "        output_dim=len(np.unique(y_train)),\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        activation=activation,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    for epoch in range(20):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = (predicted == y_test_tensor).float().mean().item()\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Bayesian NAS\n",
    "study_nas = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=42)\n",
    ")\n",
    "\n",
    "print(\"Running Bayesian NAS with Optuna...\\n\")\n",
    "study_nas.optimize(nas_objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest Architecture:\")\n",
    "print(f\"Parameters: {study_nas.best_params}\")\n",
    "print(f\"Test Accuracy: {study_nas.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize NAS results\n",
    "fig = plot_optimization_history(study_nas)\n",
    "fig.update_layout(title='NAS Optimization History', width=800, height=400)\n",
    "fig.show()\n",
    "\n",
    "fig = plot_param_importances(study_nas)\n",
    "fig.update_layout(title='Architecture Component Importances', width=800, height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Advanced NAS Methods\n",
    "\n",
    "**Evolutionary NAS**:\n",
    "- Population of architectures\n",
    "- Mutation and crossover\n",
    "- Selection based on performance\n",
    "\n",
    "**Reinforcement Learning NAS**:\n",
    "- Controller RNN generates architectures\n",
    "- Trained with REINFORCE\n",
    "- Example: NASNet, EfficientNet\n",
    "\n",
    "**Differentiable NAS (DARTS)**:\n",
    "- Continuous relaxation of search space\n",
    "- Gradient-based optimization\n",
    "- Much faster than RL-based methods\n",
    "\n",
    "**One-Shot NAS**:\n",
    "- Train supernet containing all possible architectures\n",
    "- Sample sub-networks for evaluation\n",
    "- Example: Once-for-All Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Meta-Learning\n",
    "\n",
    "### Learning to Learn\n",
    "\n",
    "**Meta-Learning**: Learn from multiple tasks to quickly adapt to new tasks.\n",
    "\n",
    "**Key Ideas:**\n",
    "- **Task distribution** $p(T)$: Sample tasks from distribution\n",
    "- **Meta-train**: Learn across tasks\n",
    "- **Meta-test**: Quickly adapt to new task\n",
    "\n",
    "**Applications:**\n",
    "- Few-shot learning\n",
    "- Hyperparameter initialization\n",
    "- Transfer learning\n",
    "\n",
    "### 5.1 Simple Meta-Learning Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMetaLearner:\n",
    "    \"\"\"\n",
    "    Simple meta-learner that learns good hyperparameter initialization\n",
    "    across multiple tasks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.meta_params = {\n",
    "            'learning_rate': 0.001,\n",
    "            'hidden_size': 64\n",
    "        }\n",
    "    \n",
    "    def create_task_data(self, n_samples=100):\n",
    "        \"\"\"Create a random binary classification task\"\"\"\n",
    "        X, y = make_classification(\n",
    "            n_samples=n_samples,\n",
    "            n_features=20,\n",
    "            n_informative=15,\n",
    "            n_redundant=5,\n",
    "            n_classes=2,\n",
    "            random_state=np.random.randint(1000)\n",
    "        )\n",
    "        return train_test_split(X, y, test_size=0.3)\n",
    "    \n",
    "    def train_on_task(self, X_train, y_train, X_test, y_test, lr, hidden_size, num_epochs=50):\n",
    "        \"\"\"Train model on a single task\"\"\"\n",
    "        # Scale data\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_train_t = torch.FloatTensor(X_train_scaled)\n",
    "        y_train_t = torch.LongTensor(y_train)\n",
    "        X_test_t = torch.FloatTensor(X_test_scaled)\n",
    "        y_test_t = torch.LongTensor(y_test)\n",
    "        \n",
    "        # Create model\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(X_train.shape[1], hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 2)\n",
    "        )\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        # Train\n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_train_t)\n",
    "            loss = criterion(outputs, y_train_t)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Test\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_test_t)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            accuracy = (predicted == y_test_t).float().mean().item()\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def meta_train(self, num_tasks=10):\n",
    "        \"\"\"\n",
    "        Meta-train: Learn good hyperparameter initialization\n",
    "        across multiple tasks\n",
    "        \"\"\"\n",
    "        print(\"Meta-Training: Learning good hyperparameters across tasks...\\n\")\n",
    "        \n",
    "        # Try different hyperparameter combinations\n",
    "        lrs = [0.0001, 0.001, 0.01]\n",
    "        hidden_sizes = [32, 64, 128]\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for lr in lrs:\n",
    "            for hidden_size in hidden_sizes:\n",
    "                task_accuracies = []\n",
    "                \n",
    "                # Test on multiple tasks\n",
    "                for task_id in range(num_tasks):\n",
    "                    X_train, X_test, y_train, y_test = self.create_task_data()\n",
    "                    acc = self.train_on_task(X_train, y_train, X_test, y_test, lr, hidden_size)\n",
    "                    task_accuracies.append(acc)\n",
    "                \n",
    "                avg_acc = np.mean(task_accuracies)\n",
    "                results[(lr, hidden_size)] = avg_acc\n",
    "                \n",
    "                print(f\"LR={lr:.4f}, Hidden={hidden_size}: Avg Accuracy={avg_acc:.4f}\")\n",
    "        \n",
    "        # Find best meta-parameters\n",
    "        best_params = max(results, key=results.get)\n",
    "        self.meta_params = {\n",
    "            'learning_rate': best_params[0],\n",
    "            'hidden_size': best_params[1]\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nBest meta-parameters: LR={self.meta_params['learning_rate']}, \"\n",
    "              f\"Hidden={self.meta_params['hidden_size']}\")\n",
    "        \n",
    "        return self.meta_params\n",
    "    \n",
    "    def fast_adapt(self, X_train, y_train, X_test, y_test, num_epochs=50):\n",
    "        \"\"\"\n",
    "        Fast adaptation to new task using learned meta-parameters\n",
    "        \"\"\"\n",
    "        return self.train_on_task(\n",
    "            X_train, y_train, X_test, y_test,\n",
    "            lr=self.meta_params['learning_rate'],\n",
    "            hidden_size=self.meta_params['hidden_size'],\n",
    "            num_epochs=num_epochs\n",
    "        )\n",
    "\n",
    "# Meta-learning demo\n",
    "meta_learner = SimpleMetaLearner()\n",
    "\n",
    "# Meta-train on 10 tasks\n",
    "best_meta_params = meta_learner.meta_train(num_tasks=10)\n",
    "\n",
    "# Test on new tasks\n",
    "print(\"\\nTesting on new tasks:\")\n",
    "test_accuracies = []\n",
    "for i in range(5):\n",
    "    X_train, X_test, y_train, y_test = meta_learner.create_task_data()\n",
    "    acc = meta_learner.fast_adapt(X_train, y_train, X_test, y_test)\n",
    "    test_accuracies.append(acc)\n",
    "    print(f\"New Task {i+1}: Accuracy = {acc:.4f}\")\n",
    "\n",
    "print(f\"\\nAverage accuracy on new tasks: {np.mean(test_accuracies):.4f} ± {np.std(test_accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Automated Feature Engineering\n",
    "\n",
    "### 6.1 Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load data with many features\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Original number of features: {X_train.shape[1]}\")\n",
    "\n",
    "# Method 1: Univariate Feature Selection\n",
    "selector_univariate = SelectKBest(f_classif, k=10)\n",
    "X_train_selected = selector_univariate.fit_transform(X_train, y_train)\n",
    "X_test_selected = selector_univariate.transform(X_test)\n",
    "\n",
    "print(f\"\\nUnivariate Selection: {X_train_selected.shape[1]} features\")\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_selected, y_train)\n",
    "print(f\"Accuracy: {rf.score(X_test_selected, y_test):.4f}\")\n",
    "\n",
    "# Method 2: Recursive Feature Elimination\n",
    "estimator = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "selector_rfe = RFE(estimator, n_features_to_select=10, step=1)\n",
    "X_train_rfe = selector_rfe.fit_transform(X_train, y_train)\n",
    "X_test_rfe = selector_rfe.transform(X_test)\n",
    "\n",
    "print(f\"\\nRFE: {X_train_rfe.shape[1]} features\")\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_rfe, y_train)\n",
    "print(f\"Accuracy: {rf.score(X_test_rfe, y_test):.4f}\")\n",
    "\n",
    "# Method 3: Feature Importance\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "importances = rf.feature_importances_\n",
    "top_k = 10\n",
    "top_features = np.argsort(importances)[-top_k:]\n",
    "\n",
    "X_train_importance = X_train[:, top_features]\n",
    "X_test_importance = X_test[:, top_features]\n",
    "\n",
    "print(f\"\\nFeature Importance: {top_k} features\")\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_importance, y_train)\n",
    "print(f\"Accuracy: {rf.score(X_test_importance, y_test):.4f}\")\n",
    "\n",
    "# Visualize feature importances\n",
    "plt.figure(figsize=(12, 5))\n",
    "feature_names = load_breast_cancer().feature_names\n",
    "indices = np.argsort(importances)[-15:][::-1]\n",
    "\n",
    "plt.bar(range(15), importances[indices], color='steelblue')\n",
    "plt.xticks(range(15), [feature_names[i] for i in indices], rotation=45, ha='right')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Top 15 Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Automated Feature Selection with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_objective(trial):\n",
    "    \"\"\"\n",
    "    Optimize feature selection\n",
    "    \"\"\"\n",
    "    # Select which features to use (binary mask)\n",
    "    selected_features = []\n",
    "    for i in range(X_train.shape[1]):\n",
    "        if trial.suggest_categorical(f'feature_{i}', [True, False]):\n",
    "            selected_features.append(i)\n",
    "    \n",
    "    # Need at least 1 feature\n",
    "    if len(selected_features) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Train with selected features\n",
    "    X_train_selected = X_train[:, selected_features]\n",
    "    X_test_selected = X_test[:, selected_features]\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "    score = cross_val_score(rf, X_train_selected, y_train, cv=3, scoring='accuracy').mean()\n",
    "    \n",
    "    # Penalize using too many features\n",
    "    feature_penalty = len(selected_features) / X_train.shape[1] * 0.05\n",
    "    \n",
    "    return score - feature_penalty\n",
    "\n",
    "# Optimize feature selection\n",
    "study_features = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "\n",
    "print(\"Optimizing feature selection...\\n\")\n",
    "study_features.optimize(feature_selection_objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Get selected features\n",
    "selected_features = [i for i in range(X_train.shape[1]) \n",
    "                    if study_features.best_params.get(f'feature_{i}', False)]\n",
    "\n",
    "print(f\"\\nSelected {len(selected_features)} / {X_train.shape[1]} features\")\n",
    "print(f\"Best CV score: {study_features.best_value:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "X_train_opt = X_train[:, selected_features]\n",
    "X_test_opt = X_test[:, selected_features]\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_opt, y_train)\n",
    "print(f\"Test accuracy: {rf.score(X_test_opt, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Best Practices\n",
    "\n",
    "### 7.1 When to Use AutoML\n",
    "\n",
    "**Use AutoML when:**\n",
    "- ✅ Have well-defined problem and dataset\n",
    "- ✅ Need quick baseline\n",
    "- ✅ Want to explore many options\n",
    "- ✅ Have computational budget\n",
    "- ✅ Non-expert users need ML\n",
    "\n",
    "**Don't use AutoML when:**\n",
    "- ❌ Need to understand model deeply\n",
    "- ❌ Very limited computational budget\n",
    "- ❌ Data needs heavy domain knowledge\n",
    "- ❌ Problem is novel/unique\n",
    "\n",
    "### 7.2 Computational Budget\n",
    "\n",
    "**Key Trade-offs:**\n",
    "- More trials = better performance but more cost\n",
    "- Early stopping can help\n",
    "- Use cheaper proxies (smaller datasets, fewer epochs)\n",
    "\n",
    "**Recommended Budgets:**\n",
    "- Grid Search: < 100 combinations\n",
    "- Random Search: 50-200 trials\n",
    "- Bayesian Optimization: 50-100 trials\n",
    "- NAS: 100-1000 trials (depending on method)\n",
    "\n",
    "### 7.3 Validation Strategy\n",
    "\n",
    "**Critical**: Use proper validation to avoid overfitting!\n",
    "\n",
    "```python\n",
    "# ❌ BAD: Optimizing on test set\n",
    "for params in search_space:\n",
    "    model.set_params(params)\n",
    "    model.fit(X_train, y_train)\n",
    "    score = model.score(X_test, y_test)  # DON'T DO THIS!\n",
    "\n",
    "# ✅ GOOD: Use cross-validation\n",
    "for params in search_space:\n",
    "    model.set_params(params)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=5).mean()\n",
    "\n",
    "# Final evaluation on held-out test set\n",
    "best_model.fit(X_train, y_train)\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "### 7.4 Interpreting Results\n",
    "\n",
    "**Always check:**\n",
    "1. **Hyperparameter importance**: Which params matter most?\n",
    "2. **Optimization history**: Is it converging?\n",
    "3. **Cross-validation variance**: Is model stable?\n",
    "4. **Train-test gap**: Overfitting?\n",
    "5. **Feature selection**: Which features selected?\n",
    "\n",
    "### 7.5 Common Pitfalls\n",
    "\n",
    "**1. Overfitting to validation set**\n",
    "- Problem: Too many hyperparameter tuning iterations\n",
    "- Solution: Use nested cross-validation or hold-out test set\n",
    "\n",
    "**2. Data leakage**\n",
    "- Problem: Preprocessing before train-test split\n",
    "- Solution: Use pipelines, fit on train only\n",
    "\n",
    "**3. Computational waste**\n",
    "- Problem: Not using early stopping\n",
    "- Solution: Prune unpromising trials\n",
    "\n",
    "**4. Ignoring domain knowledge**\n",
    "- Problem: Letting AutoML choose unreasonable options\n",
    "- Solution: Constrain search space with domain knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Interview Questions\n",
    "\n",
    "### Fundamentals\n",
    "\n",
    "**Q1: What is AutoML and what does it automate?**\n",
    "\n",
    "**A**: AutoML (Automated Machine Learning) automates the end-to-end process of applying ML:\n",
    "1. **Data preprocessing**: Handling missing values, encoding, scaling\n",
    "2. **Feature engineering**: Creating and selecting features\n",
    "3. **Model selection**: Choosing algorithm\n",
    "4. **Hyperparameter tuning**: Finding optimal hyperparameters\n",
    "5. **Model ensembling**: Combining multiple models\n",
    "\n",
    "**Benefits**: Democratizes ML, saves time, reduces human bias\n",
    "**Challenges**: Computational cost, black box, still needs domain knowledge\n",
    "\n",
    "---\n",
    "\n",
    "**Q2: Compare Grid Search, Random Search, and Bayesian Optimization.**\n",
    "\n",
    "**A**:\n",
    "\n",
    "**Grid Search**:\n",
    "- Exhaustive search over grid\n",
    "- Pros: Guaranteed to find best in grid, deterministic\n",
    "- Cons: Exponential complexity $O(n^d)$, wastes time\n",
    "- Use: < 4 hyperparameters, small grids\n",
    "\n",
    "**Random Search**:\n",
    "- Randomly sample from distributions\n",
    "- Pros: More efficient, explores important dimensions better\n",
    "- Cons: Not guaranteed optimal, random\n",
    "- Use: Quick baseline, many hyperparameters\n",
    "\n",
    "**Bayesian Optimization**:\n",
    "- Build probabilistic model of objective function\n",
    "- Pros: Most sample-efficient, automatic exploration-exploitation\n",
    "- Cons: More complex, slower per iteration\n",
    "- Use: Expensive evaluations, need best performance\n",
    "\n",
    "**Key insight**: Random search often beats grid search because not all hyperparameters are equally important.\n",
    "\n",
    "---\n",
    "\n",
    "**Q3: What is Neural Architecture Search (NAS)?**\n",
    "\n",
    "**A**: NAS automates the design of neural network architectures.\n",
    "\n",
    "**Components**:\n",
    "1. **Search Space**: Set of possible architectures (layers, connections, operations)\n",
    "2. **Search Strategy**: How to explore (random, evolutionary, RL, gradient-based)\n",
    "3. **Performance Estimation**: How to evaluate (train from scratch, weight sharing, proxies)\n",
    "\n",
    "**Methods**:\n",
    "- **RL-based**: NASNet, EfficientNet (controller RNN generates architectures)\n",
    "- **Evolutionary**: AmoebaNet (mutation and selection)\n",
    "- **Gradient-based**: DARTS (continuous relaxation, much faster)\n",
    "- **One-Shot**: Once-for-All (train supernet, sample sub-networks)\n",
    "\n",
    "**Challenges**: Extremely expensive, requires massive compute\n",
    "\n",
    "---\n",
    "\n",
    "**Q4: Explain the CASH problem.**\n",
    "\n",
    "**A**: CASH (Combined Algorithm Selection and Hyperparameter optimization)\n",
    "\n",
    "**Problem**: Jointly optimize:\n",
    "1. **Which algorithm** to use? (RF, GB, SVM, etc.)\n",
    "2. **What hyperparameters** for that algorithm?\n",
    "\n",
    "**Search Space**:\n",
    "```\n",
    "{\n",
    "  (RandomForest, {n_estimators, max_depth, ...}),\n",
    "  (GradientBoosting, {learning_rate, n_estimators, ...}),\n",
    "  (SVM, {C, gamma, kernel, ...}),\n",
    "  ...\n",
    "}\n",
    "```\n",
    "\n",
    "**Approach**: Use Bayesian optimization over combined space\n",
    "\n",
    "**Frameworks**: Auto-sklearn, TPOT, H2O AutoML\n",
    "\n",
    "---\n",
    "\n",
    "**Q5: What is meta-learning?**\n",
    "\n",
    "**A**: Meta-learning = \"learning to learn\"\n",
    "\n",
    "**Goal**: Learn from multiple tasks to quickly adapt to new tasks\n",
    "\n",
    "**Key Concepts**:\n",
    "- **Task distribution** $p(T)$: Sample tasks\n",
    "- **Meta-training**: Learn across tasks\n",
    "- **Meta-testing**: Fast adaptation to new task\n",
    "\n",
    "**Approaches**:\n",
    "1. **Metric-based**: Learn embedding space (Siamese networks, Matching Networks)\n",
    "2. **Model-based**: Learn optimizer (LSTM meta-learner)\n",
    "3. **Optimization-based**: Learn good initialization (MAML)\n",
    "\n",
    "**Applications**:\n",
    "- Few-shot learning (learn from few examples)\n",
    "- Hyperparameter initialization\n",
    "- Transfer learning\n",
    "\n",
    "**Example**: MAML learns initialization that's few gradient steps away from optimal on any new task.\n",
    "\n",
    "---\n",
    "\n",
    "### Advanced\n",
    "\n",
    "**Q6: How would you prevent overfitting during hyperparameter optimization?**\n",
    "\n",
    "**A**: Several strategies:\n",
    "\n",
    "**1. Proper validation**:\n",
    "```python\n",
    "# Use cross-validation on training data\n",
    "score = cross_val_score(model, X_train, y_train, cv=5).mean()\n",
    "# Hold out test set for final evaluation only\n",
    "```\n",
    "\n",
    "**2. Nested cross-validation**:\n",
    "- Outer loop: Performance estimation\n",
    "- Inner loop: Hyperparameter tuning\n",
    "\n",
    "**3. Regularization**:\n",
    "- Penalize model complexity in objective\n",
    "- Prefer simpler models with similar performance\n",
    "\n",
    "**4. Early stopping**:\n",
    "- Stop if validation performance plateaus\n",
    "- Prune unpromising trials\n",
    "\n",
    "**5. Limited budget**:\n",
    "- Don't tune forever\n",
    "- Diminishing returns after ~100 trials\n",
    "\n",
    "---\n",
    "\n",
    "**Q7: What are the challenges of AutoML in production?**\n",
    "\n",
    "**A**: \n",
    "\n",
    "**1. Computational cost**:\n",
    "- Training many models is expensive\n",
    "- Solution: Use cheaper proxies, early stopping\n",
    "\n",
    "**2. Interpretability**:\n",
    "- Hard to understand why choices made\n",
    "- Solution: Analyze hyperparameter importances, feature selection\n",
    "\n",
    "**3. Reproducibility**:\n",
    "- Random search is non-deterministic\n",
    "- Solution: Set random seeds, log all trials\n",
    "\n",
    "**4. Domain knowledge**:\n",
    "- Can't replace domain expertise\n",
    "- Solution: Constrain search space with knowledge\n",
    "\n",
    "**5. Deployment**:\n",
    "- Ensemble models harder to deploy\n",
    "- Solution: Model distillation, select single best model\n",
    "\n",
    "**6. Data drift**:\n",
    "- Optimal hyperparameters change over time\n",
    "- Solution: Periodic retraining, online learning\n",
    "\n",
    "---\n",
    "\n",
    "**Q8: How does Bayesian Optimization work?**\n",
    "\n",
    "**A**: \n",
    "\n",
    "**Algorithm**:\n",
    "1. Build probabilistic model (Gaussian Process) of $f(\\theta)$\n",
    "2. Use acquisition function to select next $\\theta$\n",
    "3. Evaluate $f(\\theta)$\n",
    "4. Update model\n",
    "5. Repeat\n",
    "\n",
    "**Acquisition Functions**:\n",
    "- **Expected Improvement (EI)**: $E[\\max(0, f(\\theta) - f(\\theta_{best}))]$\n",
    "  - Balances exploration vs exploitation\n",
    "  \n",
    "- **Upper Confidence Bound (UCB)**: $\\mu(\\theta) + \\kappa \\sigma(\\theta)$\n",
    "  - Optimistic estimate\n",
    "  - $\\kappa$ controls exploration\n",
    "\n",
    "- **Probability of Improvement (PI)**: $P(f(\\theta) > f(\\theta_{best}))$\n",
    "  - Greedy\n",
    "\n",
    "**Why it works**:\n",
    "- GP gives uncertainty estimates\n",
    "- Acquisition balances \"try promising areas\" vs \"explore unknown areas\"\n",
    "- Much more sample-efficient than random search\n",
    "\n",
    "**Frameworks**: Optuna (TPE), Spearmint, GPyOpt\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**We covered**:\n",
    "\n",
    "1. **AutoML Fundamentals**: What, why, and when to use\n",
    "2. **Hyperparameter Optimization**: Grid, Random, Bayesian methods\n",
    "3. **AutoML Frameworks**: CASH, ensemble selection\n",
    "4. **Neural Architecture Search**: Random, Bayesian, advanced methods\n",
    "5. **Meta-Learning**: Learning to learn across tasks\n",
    "6. **Automated Feature Engineering**: Selection and generation\n",
    "7. **Best Practices**: Validation, budgets, pitfalls\n",
    "\n",
    "**Key Takeaways**:\n",
    "- AutoML democratizes ML but doesn't replace expertise\n",
    "- Bayesian optimization is most sample-efficient\n",
    "- NAS can discover novel architectures\n",
    "- Proper validation is critical to avoid overfitting\n",
    "- Trade-off: computational budget vs performance\n",
    "\n",
    "**Further Learning**:\n",
    "- Optuna documentation: https://optuna.org/\n",
    "- Auto-sklearn: https://automl.github.io/auto-sklearn/\n",
    "- NAS survey: https://arxiv.org/abs/1808.05377\n",
    "\n",
    "---\n",
    "\n",
    "**References**:\n",
    "1. Hutter et al., \"Automated Machine Learning\" (2019)\n",
    "2. Bergstra & Bengio, \"Random Search for Hyper-Parameter Optimization\" (2012)\n",
    "3. Snoek et al., \"Practical Bayesian Optimization of Machine Learning Algorithms\" (2012)\n",
    "4. Zoph & Le, \"Neural Architecture Search with Reinforcement Learning\" (2017)\n",
    "5. Liu et al., \"DARTS: Differentiable Architecture Search\" (2019)\n",
    "6. Finn et al., \"Model-Agnostic Meta-Learning\" (2017)\n",
    "\n",
    "---\n",
    "\n",
    "*Created: October 2025*  \n",
    "*Last Updated: October 2025*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
