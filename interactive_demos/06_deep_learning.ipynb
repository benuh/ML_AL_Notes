{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧠 Deep Learning Fundamentals - Neural Networks Mastery\n",
    "\n",
    "Master neural networks from first principles to modern architectures!\n",
    "\n",
    "**Learning Goals:**\n",
    "- Build neural networks from scratch\n",
    "- Understand backpropagation mathematically\n",
    "- Master activation functions and their properties\n",
    "- Learn optimization algorithms (SGD, Adam, RMSprop)\n",
    "- Implement CNNs, RNNs, and Transformers concepts\n",
    "- Handle overfitting (dropout, batch norm, regularization)\n",
    "- Prepare for deep learning interviews\n",
    "\n",
    "**Topics Covered:**\n",
    "1. **Neural Network Basics:** Perceptron, MLP, Forward/Backward propagation\n",
    "2. **Activation Functions:** Sigmoid, ReLU, Leaky ReLU, tanh, Swish\n",
    "3. **Optimization:** SGD, Momentum, Adam, Learning rate scheduling\n",
    "4. **Regularization:** Dropout, Batch Normalization, L1/L2\n",
    "5. **Architectures:** CNN (Computer Vision), RNN/LSTM (Sequences), Transformers\n",
    "6. **Advanced Topics:** Transfer Learning, Fine-tuning, Hyperparameter tuning\n",
    "\n",
    "**Interview Topics:**\n",
    "- Vanishing/Exploding gradients\n",
    "- Batch normalization intuition\n",
    "- Why Adam works better than SGD\n",
    "- CNN vs RNN vs Transformer\n",
    "- Overfitting prevention\n",
    "\n",
    "**Sources:**\n",
    "- \"Deep Learning\" - Goodfellow, Bengio, Courville (2016)\n",
    "- \"Neural Networks and Deep Learning\" - Nielsen (online)\n",
    "- \"Dive into Deep Learning\" - Zhang et al. (2021)\n",
    "- Stanford CS231n, CS224n lecture notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.datasets import make_classification, make_moons, make_circles, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Deep Learning frameworks (if available)\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers, models\n",
    "    print(f\"✅ TensorFlow: {tf.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"⚠️ TensorFlow not available - will use NumPy implementations\")\n",
    "\n",
    "# Plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "sns.set_palette('husl')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✅ Libraries loaded successfully!\")\n",
    "print(f\"NumPy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔬 Part 1: Neural Network from Scratch\n",
    "\n",
    "**Interview Question:** *\"Explain how backpropagation works.\"*\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Backpropagation = Chain Rule Application**\n",
    "\n",
    "**Forward Pass:**\n",
    "1. Input → Layer 1: $z^{[1]} = W^{[1]}x + b^{[1]}$\n",
    "2. Activation: $a^{[1]} = g(z^{[1]})$\n",
    "3. Layer 2: $z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}$\n",
    "4. Output: $\\hat{y} = g(z^{[2]})$\n",
    "\n",
    "**Backward Pass (Chain Rule):**\n",
    "1. **Output gradient:** $\\frac{\\partial L}{\\partial z^{[2]}} = \\hat{y} - y$\n",
    "2. **Weight gradient:** $\\frac{\\partial L}{\\partial W^{[2]}} = \\frac{\\partial L}{\\partial z^{[2]}} \\cdot (a^{[1]})^T$\n",
    "3. **Propagate back:** $\\frac{\\partial L}{\\partial a^{[1]}} = (W^{[2]})^T \\cdot \\frac{\\partial L}{\\partial z^{[2]}}$\n",
    "4. **Through activation:** $\\frac{\\partial L}{\\partial z^{[1]}} = \\frac{\\partial L}{\\partial a^{[1]}} \\cdot g'(z^{[1]})$\n",
    "5. **First layer weights:** $\\frac{\\partial L}{\\partial W^{[1]}} = \\frac{\\partial L}{\\partial z^{[1]}} \\cdot x^T$\n",
    "\n",
    "**Update Rule:**\n",
    "$$W := W - \\alpha \\frac{\\partial L}{\\partial W}$$\n",
    "\n",
    "**Key Insight:** Gradients flow backward through network using chain rule!\n",
    "\n",
    "**Source:** \"Deep Learning\" Chapter 6, \"Neural Networks and Deep Learning\" Chapter 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network from Scratch\n",
    "print(\"🧠 NEURAL NETWORK FROM SCRATCH\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    2-Layer Neural Network (1 hidden layer) implemented from scratch\n",
    "    \n",
    "    Architecture: Input → Hidden (ReLU) → Output (Sigmoid)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize network with random weights\n",
    "        \n",
    "        Parameters:\n",
    "        - input_size: Number of input features\n",
    "        - hidden_size: Number of neurons in hidden layer\n",
    "        - output_size: Number of output classes\n",
    "        - learning_rate: Learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # Xavier/He initialization\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "        # For tracking\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "    \n",
    "    def relu(self, z):\n",
    "        \"\"\"ReLU activation: max(0, z)\"\"\"\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def relu_derivative(self, z):\n",
    "        \"\"\"ReLU derivative: 1 if z > 0, else 0\"\"\"\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Sigmoid activation: 1 / (1 + exp(-z))\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # Clip for numerical stability\n",
    "    \n",
    "    def binary_cross_entropy(self, y_true, y_pred):\n",
    "        \"\"\"Binary cross-entropy loss\"\"\"\n",
    "        epsilon = 1e-15  # Avoid log(0)\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation\n",
    "        \n",
    "        Returns: output, (hidden activation, hidden pre-activation)\n",
    "        \"\"\"\n",
    "        # Hidden layer\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        \n",
    "        # Output layer\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        \"\"\"\n",
    "        Backward propagation (compute gradients)\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dz2 = self.a2 - y  # Derivative of BCE + sigmoid\n",
    "        dW2 = (self.a1.T @ dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Hidden layer gradients (chain rule!)\n",
    "        da1 = dz2 @ self.W2.T\n",
    "        dz1 = da1 * self.relu_derivative(self.z1)\n",
    "        dW1 = (X.T @ dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def update_weights(self, dW1, db1, dW2, db2):\n",
    "        \"\"\"\n",
    "        Update weights using gradient descent\n",
    "        \"\"\"\n",
    "        self.W1 -= self.lr * dW1\n",
    "        self.b1 -= self.lr * db1\n",
    "        self.W2 -= self.lr * dW2\n",
    "        self.b2 -= self.lr * db2\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None, epochs=1000, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the network\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            output = self.forward(X_train)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.binary_cross_entropy(y_train, output)\n",
    "            self.train_losses.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            dW1, db1, dW2, db2 = self.backward(X_train, y_train)\n",
    "            \n",
    "            # Update weights\n",
    "            self.update_weights(dW1, db1, dW2, db2)\n",
    "            \n",
    "            # Validation\n",
    "            if X_val is not None:\n",
    "                val_output = self.forward(X_val)\n",
    "                val_loss = self.binary_cross_entropy(y_val, val_output)\n",
    "                self.val_losses.append(val_loss)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and epoch % 100 == 0:\n",
    "                if X_val is not None:\n",
    "                    print(f\"Epoch {epoch:4d}: Train Loss = {loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "                else:\n",
    "                    print(f\"Epoch {epoch:4d}: Train Loss = {loss:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions (0 or 1)\n",
    "        \"\"\"\n",
    "        output = self.forward(X)\n",
    "        return (output > 0.5).astype(int)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Get probability predictions\n",
    "        \"\"\"\n",
    "        return self.forward(X)\n",
    "\n",
    "print(\"✅ Neural Network class implemented!\")\n",
    "print(\"\\n📚 Key Methods:\")\n",
    "print(\"  • forward(): Forward propagation (X → hidden → output)\")\n",
    "print(\"  • backward(): Backpropagation (compute gradients using chain rule)\")\n",
    "print(\"  • update_weights(): Gradient descent update\")\n",
    "print(\"  • train(): Full training loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our neural network\n",
    "print(\"🧪 TESTING NEURAL NETWORK FROM SCRATCH\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Generate non-linear dataset (circles)\n",
    "X, y = make_circles(n_samples=1000, noise=0.1, random_state=42, factor=0.5)\n",
    "y = y.reshape(-1, 1)  # Reshape for network\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\n📊 Dataset: Concentric Circles (Non-linear)\")\n",
    "print(f\"  Training samples: {X_train.shape[0]}\")\n",
    "print(f\"  Test samples: {X_test.shape[0]}\")\n",
    "print(f\"  Features: {X_train.shape[1]}\")\n",
    "print(f\"  Classes: {np.unique(y)}\")\n",
    "\n",
    "# Create and train network\n",
    "print(\"\\n🏗️ Network Architecture:\")\n",
    "print(\"  Input Layer: 2 neurons (2 features)\")\n",
    "print(\"  Hidden Layer: 10 neurons (ReLU activation)\")\n",
    "print(\"  Output Layer: 1 neuron (Sigmoid activation)\")\n",
    "print(\"  Total parameters:\", 2*10 + 10 + 10*1 + 1, \"=\", \"(2→10) + (10 biases) + (10→1) + (1 bias)\")\n",
    "\n",
    "nn = NeuralNetwork(input_size=2, hidden_size=10, output_size=1, learning_rate=0.1)\n",
    "\n",
    "print(\"\\n🎓 Training Network...\\n\")\n",
    "nn.train(X_train, y_train, X_test, y_test, epochs=1000, verbose=True)\n",
    "\n",
    "# Evaluate\n",
    "train_pred = nn.predict(X_train)\n",
    "test_pred = nn.predict(X_test)\n",
    "\n",
    "train_acc = accuracy_score(y_train, train_pred)\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "\n",
    "print(f\"\\n📈 Final Performance:\")\n",
    "print(f\"  Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"  Overfitting gap: {train_acc - test_acc:.4f}\")\n",
    "print(\"\\n✅ Network successfully learned non-linear decision boundary!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize neural network learning\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Plot 1: Dataset\n",
    "ax = axes[0, 0]\n",
    "scatter = ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train.ravel(), \n",
    "                     cmap='RdYlBu', alpha=0.6, edgecolors='black', s=30)\n",
    "ax.set_title('Training Data\\n(Non-linear circles)', fontweight='bold', fontsize=14)\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "plt.colorbar(scatter, ax=ax)\n",
    "\n",
    "# Plot 2: Decision boundary\n",
    "ax = axes[0, 1]\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "Z = nn.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "ax.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu', levels=20)\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test.ravel(), \n",
    "          cmap='RdYlBu', edgecolors='black', s=30)\n",
    "ax.set_title(f'Decision Boundary\\nTest Acc: {test_acc:.3f}', fontweight='bold', fontsize=14)\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "\n",
    "# Plot 3: Loss curves\n",
    "ax = axes[0, 2]\n",
    "ax.plot(nn.train_losses, label='Training Loss', linewidth=2, alpha=0.8)\n",
    "if nn.val_losses:\n",
    "    ax.plot(nn.val_losses, label='Validation Loss', linewidth=2, alpha=0.8)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Binary Cross-Entropy Loss', fontsize=12)\n",
    "ax.set_title('Learning Curves\\n(Loss decreasing = learning)', fontweight='bold', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Weight distributions (Layer 1)\n",
    "ax = axes[1, 0]\n",
    "ax.hist(nn.W1.ravel(), bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax.axvline(0, color='red', linestyle='--', linewidth=2, label='Zero')\n",
    "ax.set_xlabel('Weight Value', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.set_title('Hidden Layer Weights\\n(Should be well-distributed)', fontweight='bold', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Confusion Matrix\n",
    "ax = axes[1, 1]\n",
    "cm = confusion_matrix(y_test, test_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "           xticklabels=['Class 0', 'Class 1'],\n",
    "           yticklabels=['Class 0', 'Class 1'])\n",
    "ax.set_title('Confusion Matrix', fontweight='bold', fontsize=14)\n",
    "ax.set_ylabel('True Label')\n",
    "ax.set_xlabel('Predicted Label')\n",
    "\n",
    "# Plot 6: Activation visualization (hidden layer)\n",
    "ax = axes[1, 2]\n",
    "hidden_activations = nn.a1[:100]  # First 100 samples\n",
    "im = ax.imshow(hidden_activations.T, aspect='auto', cmap='viridis')\n",
    "ax.set_xlabel('Sample', fontsize=12)\n",
    "ax.set_ylabel('Hidden Neuron', fontsize=12)\n",
    "ax.set_title('Hidden Layer Activations\\n(Each row = one neuron)', fontweight='bold', fontsize=14)\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 Key Observations:\")\n",
    "print(\"  • Decision boundary is curved (non-linear) - linear models can't do this!\")\n",
    "print(\"  • Loss decreases over time - network is learning\")\n",
    "print(\"  • Hidden layer activations show learned features\")\n",
    "print(\"  • Weight distribution shows network isn't dead (no all-zeros)\")\n",
    "\n",
    "print(\"\\n🎯 Interview Insight:\")\n",
    "print(\"  'The hidden layer learns non-linear features that make the data\")\n",
    "print(\"   linearly separable in the hidden space. This is why neural networks\")\n",
    "print(\"   can solve problems that linear models cannot. The key is the\")\n",
    "print(\"   combination of linear transformations (weights) and non-linear\")\n",
    "print(\"   activations (ReLU, sigmoid).'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚡ Part 2: Activation Functions Deep Dive\n",
    "\n",
    "**Interview Question:** *\"Why do we need activation functions? Why not just stack linear layers?\"*\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Why Activation Functions:**\n",
    "- **Without them:** Multiple linear layers = single linear layer!\n",
    "  - Proof: $f(W_2(W_1x)) = W_2W_1x = Wx$ (just another linear transform)\n",
    "- **With non-linearity:** Can approximate any function (Universal Approximation Theorem)\n",
    "\n",
    "**Common Activation Functions:**\n",
    "\n",
    "| Function | Formula | Range | Pros | Cons | Use Case |\n",
    "|----------|---------|-------|------|------|----------|\n",
    "| **Sigmoid** | $\\frac{1}{1+e^{-x}}$ | (0, 1) | Smooth, interpretable | Vanishing gradient, slow | Output layer (binary) |\n",
    "| **Tanh** | $\\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | (-1, 1) | Zero-centered | Vanishing gradient | Better than sigmoid |\n",
    "| **ReLU** | $\\max(0, x)$ | [0, ∞) | Fast, no vanishing | Dying ReLU problem | **Default choice** |\n",
    "| **Leaky ReLU** | $\\max(0.01x, x)$ | (-∞, ∞) | Fixes dying ReLU | Hyperparameter (slope) | When ReLU dies |\n",
    "| **ELU** | $x$ if $x>0$ else $\\alpha(e^x-1)$ | (-α, ∞) | Smooth, negative values | Slower computation | Research |\n",
    "| **Swish** | $x \\cdot \\sigma(x)$ | (-∞, ∞) | Self-gated, smooth | Computational cost | Modern architectures |\n",
    "| **Softmax** | $\\frac{e^{x_i}}{\\sum e^{x_j}}$ | (0, 1) sum=1 | Probability distribution | - | Multi-class output |\n",
    "\n",
    "**Interview Tip:** ReLU is default because:\n",
    "1. Computationally cheap (just max)\n",
    "2. No vanishing gradient for positive values\n",
    "3. Sparse activation (many zeros)\n",
    "4. Empirically works very well\n",
    "\n",
    "**Source:** \"Deep Learning\" Chapter 6.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all activation functions\n",
    "print(\"⚡ ACTIVATION FUNCTIONS COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def elu(x, alpha=1.0):\n",
    "    return np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "def swish(x):\n",
    "    return x * sigmoid(x)\n",
    "\n",
    "# Define derivatives\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def leaky_relu_derivative(x, alpha=0.01):\n",
    "    return np.where(x > 0, 1, alpha)\n",
    "\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "\n",
    "# Plot activation functions and their derivatives\n",
    "fig, axes = plt.subplots(3, 4, figsize=(20, 14))\n",
    "\n",
    "activations = [\n",
    "    ('Sigmoid', sigmoid, sigmoid_derivative),\n",
    "    ('Tanh', tanh, tanh_derivative),\n",
    "    ('ReLU', relu, relu_derivative),\n",
    "    ('Leaky ReLU', leaky_relu, leaky_relu_derivative),\n",
    "    ('ELU', elu, lambda x: np.where(x > 0, 1, elu(x) + 1)),\n",
    "    ('Swish', swish, lambda x: swish(x) + sigmoid(x) * (1 - swish(x)))\n",
    "]\n",
    "\n",
    "for idx, (name, func, deriv) in enumerate(activations):\n",
    "    # Plot activation function\n",
    "    row = idx // 2\n",
    "    col = (idx % 2) * 2\n",
    "    \n",
    "    ax = axes[row, col]\n",
    "    y = func(x)\n",
    "    ax.plot(x, y, linewidth=3, label=name, color=f'C{idx}')\n",
    "    ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_title(f'{name} Function', fontweight='bold', fontsize=12)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel(f'{name}(x)')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Plot derivative\n",
    "    ax = axes[row, col + 1]\n",
    "    y_deriv = deriv(x)\n",
    "    ax.plot(x, y_deriv, linewidth=3, label=f\"{name}'\", color=f'C{idx}', linestyle='--')\n",
    "    ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_title(f'{name} Derivative', fontweight='bold', fontsize=12)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel(f\"d/dx {name}(x)\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Key Properties:\")\n",
    "print(\"\\n1️⃣ Sigmoid:\")\n",
    "print(\"   • Output: (0, 1) - good for probabilities\")\n",
    "print(\"   • Problem: Gradient vanishes for |x| > 4\")\n",
    "print(\"   • Use: Output layer for binary classification\")\n",
    "\n",
    "print(\"\\n2️⃣ Tanh:\")\n",
    "print(\"   • Output: (-1, 1) - zero-centered (better than sigmoid)\")\n",
    "print(\"   • Problem: Still vanishes for large |x|\")\n",
    "print(\"   • Use: RNNs, sometimes hidden layers\")\n",
    "\n",
    "print(\"\\n3️⃣ ReLU:\")\n",
    "print(\"   • Output: [0, ∞) - half rectified\")\n",
    "print(\"   • Advantage: No vanishing gradient for x > 0\")\n",
    "print(\"   • Problem: 'Dying ReLU' (neurons stuck at 0)\")\n",
    "print(\"   • Use: DEFAULT choice for hidden layers\")\n",
    "\n",
    "print(\"\\n4️⃣ Leaky ReLU:\")\n",
    "print(\"   • Output: (-∞, ∞) - small slope for negative\")\n",
    "print(\"   • Advantage: Fixes dying ReLU problem\")\n",
    "print(\"   • Use: When ReLU causes too many dead neurons\")\n",
    "\n",
    "print(\"\\n5️⃣ ELU:\")\n",
    "print(\"   • Output: (-α, ∞) - smooth everywhere\")\n",
    "print(\"   • Advantage: Negative saturation regularizes\")\n",
    "print(\"   • Problem: Exponential is slower\")\n",
    "\n",
    "print(\"\\n6️⃣ Swish:\")\n",
    "print(\"   • Output: (-∞, ∞) - self-gated\")\n",
    "print(\"   • Advantage: Smooth, works well empirically\")\n",
    "print(\"   • Use: Modern architectures (EfficientNet)\")\n",
    "\n",
    "print(\"\\n🎯 Interview Answer Template:\")\n",
    "print(\"  'I typically use ReLU for hidden layers because it's fast and\")\n",
    "print(\"   avoids vanishing gradients. For output layers, I use sigmoid for\")\n",
    "print(\"   binary classification and softmax for multi-class. If I notice\")\n",
    "print(\"   dying ReLU (many dead neurons), I switch to Leaky ReLU or ELU.\")\n",
    "print(\"   The key is that non-linearity allows the network to learn complex\")\n",
    "print(\"   patterns - without it, stacking layers is pointless.'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 Summary: What You've Learned\n",
    "\n",
    "### ✅ Core Concepts Mastered:\n",
    "\n",
    "**1. Neural Network Fundamentals:**\n",
    "- Forward propagation: Input → Hidden → Output\n",
    "- Backpropagation: Chain rule application for gradients\n",
    "- Weight updates: Gradient descent optimization\n",
    "- Loss functions: Cross-entropy, MSE\n",
    "\n",
    "**2. Activation Functions:**\n",
    "- Why non-linearity is essential\n",
    "- ReLU as default choice\n",
    "- Vanishing gradient problem\n",
    "- Choosing the right activation\n",
    "\n",
    "**3. Key Interview Topics:**\n",
    "- How backpropagation works (chain rule!)\n",
    "- Why deep learning works (universal approximation)\n",
    "- Overfitting prevention techniques\n",
    "- Architecture design choices\n",
    "\n",
    "### 🚀 Next Steps:\n",
    "\n",
    "1. **Practice Implementation:** Build networks from scratch for different problems\n",
    "2. **Study Advanced Topics:** CNNs (computer vision), RNNs (sequences), Transformers (NLP)\n",
    "3. **Read Papers:** Understanding attention mechanisms, residual connections\n",
    "4. **Hands-on Projects:** Image classification, time series, NLP tasks\n",
    "\n",
    "### 📖 Recommended Resources:\n",
    "\n",
    "**Books:**\n",
    "- \"Deep Learning\" - Goodfellow, Bengio, Courville\n",
    "- \"Neural Networks and Deep Learning\" - Michael Nielsen (free online)\n",
    "- \"Dive into Deep Learning\" - Zhang et al. (free online, interactive)\n",
    "\n",
    "**Courses:**\n",
    "- Stanford CS231n (Computer Vision)\n",
    "- Stanford CS224n (NLP)\n",
    "- Fast.ai Practical Deep Learning\n",
    "- DeepLearning.AI Specialization (Coursera)\n",
    "\n",
    "**Practice:**\n",
    "- Kaggle competitions\n",
    "- LeetCode ML problems\n",
    "- Build projects and deploy them\n",
    "\n",
    "### 🎯 Interview Preparation Checklist:\n",
    "\n",
    "- ✅ Explain backpropagation from first principles\n",
    "- ✅ Implement neural network from scratch\n",
    "- ✅ Know when to use each activation function\n",
    "- ✅ Understand vanishing/exploding gradients\n",
    "- ✅ Explain overfitting and how to prevent it\n",
    "- ✅ Compare different optimization algorithms\n",
    "- ✅ Understand batch normalization\n",
    "- ✅ Know architecture choices (CNN vs RNN vs Transformer)\n",
    "\n",
    "**Remember:** Understanding WHY things work is more important than just knowing HOW to use libraries!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
