{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîß Data Processing & Feature Engineering for ML\n",
    "\n",
    "**\"Garbage In, Garbage Out\"** - Data quality determines model quality!\n",
    "\n",
    "This comprehensive notebook covers everything you need to prepare data for production ML systems.\n",
    "\n",
    "**Learning Goals:**\n",
    "- Master data cleaning techniques\n",
    "- Handle missing data professionally\n",
    "- Engineer powerful features\n",
    "- Scale and normalize data properly\n",
    "- Handle imbalanced datasets\n",
    "- Build production-ready pipelines\n",
    "\n",
    "**Interview Topics Covered:**\n",
    "- Data preprocessing strategies\n",
    "- Feature engineering techniques\n",
    "- Handling categorical variables\n",
    "- Dealing with outliers\n",
    "- Data leakage prevention\n",
    "- Pipeline design\n",
    "\n",
    "**Sources:**\n",
    "- \"Feature Engineering for Machine Learning\" - Zheng & Casari (2018)\n",
    "- \"Hands-On Machine Learning\" - G√©ron (2019), Chapter 2\n",
    "- \"Python for Data Analysis\" - McKinney (2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Scikit-learn preprocessing\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, MinMaxScaler, RobustScaler,\n",
    "    LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    ")\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Feature engineering\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "sns.set_palette('husl')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully!\")\n",
    "print(f\"Pandas: {pd.__version__}, NumPy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Part 1: Data Cleaning - The Foundation\n",
    "\n",
    "**80% of ML work is data preparation!**\n",
    "\n",
    "**Common Data Quality Issues:**\n",
    "1. Missing values\n",
    "2. Duplicate records\n",
    "3. Inconsistent formatting\n",
    "4. Outliers\n",
    "5. Invalid values\n",
    "6. Data type mismatches\n",
    "\n",
    "**Interview Question:** *\"How do you handle missing data in a dataset?\"*\n",
    "\n",
    "**Source:** \"Python for Data Analysis\" Chapter 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create realistic messy dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate base data\n",
    "data = {\n",
    "    'customer_id': range(1, n_samples + 1),\n",
    "    'age': np.random.randint(18, 80, n_samples),\n",
    "    'income': np.random.normal(60000, 25000, n_samples),\n",
    "    'credit_score': np.random.randint(300, 850, n_samples),\n",
    "    'years_employed': np.random.exponential(5, n_samples),\n",
    "    'num_credit_cards': np.random.poisson(3, n_samples),\n",
    "    'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], n_samples),\n",
    "    'city': np.random.choice(['NYC', 'LA', 'Chicago', 'Houston', 'Phoenix'], n_samples),\n",
    "    'default': np.random.choice([0, 1], n_samples, p=[0.85, 0.15])\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Introduce realistic data quality issues\n",
    "# 1. Missing values (MCAR, MAR, MNAR)\n",
    "missing_indices_age = np.random.choice(df.index, size=50, replace=False)\n",
    "df.loc[missing_indices_age, 'age'] = np.nan\n",
    "\n",
    "# Missing income (MAR - depends on default)\n",
    "missing_income = df[df['default'] == 1].sample(frac=0.2).index\n",
    "df.loc[missing_income, 'income'] = np.nan\n",
    "\n",
    "# Missing credit score\n",
    "missing_credit = np.random.choice(df.index, size=80, replace=False)\n",
    "df.loc[missing_credit, 'credit_score'] = np.nan\n",
    "\n",
    "# 2. Duplicates\n",
    "duplicate_rows = df.sample(n=20)\n",
    "df = pd.concat([df, duplicate_rows], ignore_index=True)\n",
    "\n",
    "# 3. Outliers\n",
    "outlier_indices = np.random.choice(df.index, size=10, replace=False)\n",
    "df.loc[outlier_indices, 'income'] = df.loc[outlier_indices, 'income'] * 10\n",
    "\n",
    "# 4. Invalid values\n",
    "invalid_indices = np.random.choice(df.index, size=5, replace=False)\n",
    "df.loc[invalid_indices, 'age'] = -999\n",
    "\n",
    "# 5. Inconsistent formatting\n",
    "df.loc[df['city'] == 'NYC', 'city'] = np.random.choice(['NYC', 'New York', 'ny'], \n",
    "                                                         size=(df['city'] == 'NYC').sum())\n",
    "\n",
    "print(\"üîç DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìè Dataset Shape: {df.shape}\")\n",
    "print(f\"\\nüìä Data Types:\\n{df.dtypes}\")\n",
    "print(f\"\\n‚ùå Missing Values:\")\n",
    "missing_summary = df.isnull().sum()\n",
    "missing_pct = (missing_summary / len(df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing': missing_summary,\n",
    "    'Percentage': missing_pct\n",
    "})\n",
    "print(missing_df[missing_df['Missing'] > 0])\n",
    "\n",
    "print(f\"\\nüîÑ Duplicates: {df.duplicated().sum()} rows\")\n",
    "print(f\"\\n‚ö†Ô∏è Invalid Ages (negative): {(df['age'] < 0).sum()}\")\n",
    "\n",
    "print(\"\\nüìä First few rows:\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data quality issues\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# 1. Missing data heatmap\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "sns.heatmap(df.isnull().head(100), cmap='viridis', cbar=False, yticklabels=False, ax=ax1)\n",
    "ax1.set_title('Missing Data Pattern (First 100 rows)\\nYellow = Missing', fontweight='bold')\n",
    "ax1.set_xlabel('Features')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# 2. Missing data bar chart\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_counts = missing_counts[missing_counts > 0].sort_values(ascending=False)\n",
    "ax2.barh(range(len(missing_counts)), missing_counts.values, color='coral')\n",
    "ax2.set_yticks(range(len(missing_counts)))\n",
    "ax2.set_yticklabels(missing_counts.index)\n",
    "ax2.set_xlabel('Count')\n",
    "ax2.set_title('Missing Values by Feature', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 3. Income distribution with outliers\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "ax3.boxplot(df['income'].dropna(), vert=True, patch_artist=True,\n",
    "            boxprops=dict(facecolor='lightblue'))\n",
    "ax3.set_ylabel('Income ($)')\n",
    "ax3.set_title('Income Distribution\\n(Note: Extreme outliers)', fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. City value inconsistencies\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "city_counts = df['city'].value_counts()\n",
    "ax4.bar(range(len(city_counts)), city_counts.values, color='skyblue', edgecolor='black')\n",
    "ax4.set_xticks(range(len(city_counts)))\n",
    "ax4.set_xticklabels(city_counts.index, rotation=45, ha='right')\n",
    "ax4.set_ylabel('Count')\n",
    "ax4.set_title('City Distribution\\n(Note: NYC inconsistency)', fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 5. Age distribution showing invalid values\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "ax5.hist(df['age'].dropna(), bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "ax5.axvline(0, color='red', linestyle='--', linewidth=2, label='Invalid threshold')\n",
    "ax5.set_xlabel('Age')\n",
    "ax5.set_ylabel('Frequency')\n",
    "ax5.set_title(f'Age Distribution\\n({(df[\"age\"] < 0).sum()} invalid values)', fontweight='bold')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Summary statistics table\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "ax6.axis('off')\n",
    "\n",
    "summary_data = [\n",
    "    ['Issue', 'Count', 'Action Needed'],\n",
    "    ['Missing Values', f\"{df.isnull().sum().sum()}\", 'Impute/Remove'],\n",
    "    ['Duplicates', f\"{df.duplicated().sum()}\", 'Remove'],\n",
    "    ['Invalid Ages', f\"{(df['age'] < 0).sum()}\", 'Replace/Remove'],\n",
    "    ['NYC Variants', f\"{(df['city'].isin(['NYC', 'New York', 'ny'])).sum()}\", 'Standardize'],\n",
    "    ['Outliers (Income)', '~10', 'Cap/Transform'],\n",
    "]\n",
    "\n",
    "table = ax6.table(cellText=summary_data, cellLoc='left', loc='center',\n",
    "                  colWidths=[0.3, 0.2, 0.3])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2.5)\n",
    "\n",
    "for i in range(3):\n",
    "    table[(0, i)].set_facecolor('lightblue')\n",
    "    table[(0, i)].set_text_props(weight='bold')\n",
    "\n",
    "ax6.set_title('Data Quality Issues Summary', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Data Cleaning Strategy:\")\n",
    "print(\"  1. Remove duplicates first\")\n",
    "print(\"  2. Fix invalid/impossible values\")\n",
    "print(\"  3. Standardize inconsistent formats\")\n",
    "print(\"  4. Handle outliers\")\n",
    "print(\"  5. Impute missing values last\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Handling Missing Data - Complete Guide\n",
    "\n",
    "**Interview Question:** *\"What are the different types of missing data and how do you handle them?\"*\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Three Types of Missingness:**\n",
    "1. **MCAR** (Missing Completely At Random) - Missingness is random, unrelated to any variable\n",
    "   - Safe to delete or impute with simple methods\n",
    "   \n",
    "2. **MAR** (Missing At Random) - Missingness depends on observed variables\n",
    "   - Example: Income missing more often for defaulters\n",
    "   - Use advanced imputation (KNN, MICE)\n",
    "   \n",
    "3. **MNAR** (Missing Not At Random) - Missingness depends on the missing value itself\n",
    "   - Example: High earners don't report income\n",
    "   - Most difficult, may need domain knowledge\n",
    "\n",
    "**Strategies:**\n",
    "- Delete: If < 5% missing and MCAR\n",
    "- Mean/Median/Mode: Simple, works for MCAR\n",
    "- Forward/Backward Fill: For time series\n",
    "- KNN Imputation: Uses similar records\n",
    "- Model-based: MICE, iterative imputation\n",
    "- Flag missing: Add indicator variable\n",
    "\n",
    "**Source:** \"Feature Engineering for Machine Learning\" Chapter 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive missing data handling\n",
    "print(\"üîß MISSING DATA HANDLING STRATEGIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Start with clean copy\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Step 1: Analyze missing patterns\n",
    "print(\"\\nüìä Missing Data Analysis:\")\n",
    "for col in df_clean.columns:\n",
    "    missing_count = df_clean[col].isnull().sum()\n",
    "    if missing_count > 0:\n",
    "        missing_pct = missing_count / len(df_clean) * 100\n",
    "        print(f\"  {col}: {missing_count} ({missing_pct:.2f}%)\")\n",
    "\n",
    "# Strategy 1: Simple imputation (mean/median/mode)\n",
    "print(\"\\nüìç Strategy 1: Simple Imputation\")\n",
    "\n",
    "# Age: Use median (robust to outliers)\n",
    "age_median = df_clean['age'][df_clean['age'] >= 0].median()\n",
    "df_clean['age'].fillna(age_median, inplace=True)\n",
    "print(f\"  Age: Filled with median = {age_median:.1f}\")\n",
    "\n",
    "# Income: Use median\n",
    "income_median = df_clean['income'].median()\n",
    "df_clean['income'].fillna(income_median, inplace=True)\n",
    "print(f\"  Income: Filled with median = ${income_median:,.0f}\")\n",
    "\n",
    "# Strategy 2: KNN Imputation for credit score\n",
    "print(\"\\nüìç Strategy 2: KNN Imputation (credit_score)\")\n",
    "print(\"  Using 5 nearest neighbors based on age, income, years_employed\")\n",
    "\n",
    "# Prepare data for KNN imputation\n",
    "features_for_imputation = ['age', 'income', 'years_employed', 'credit_score']\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df_clean[features_for_imputation] = imputer.fit_transform(df_clean[features_for_imputation])\n",
    "print(\"  ‚úÖ Credit score imputed using KNN\")\n",
    "\n",
    "# Strategy 3: Create missing indicator\n",
    "print(\"\\nüìç Strategy 3: Missing Indicator Features\")\n",
    "df_clean['age_was_missing'] = df['age'].isnull().astype(int)\n",
    "df_clean['income_was_missing'] = df['income'].isnull().astype(int)\n",
    "df_clean['credit_was_missing'] = df['credit_score'].isnull().astype(int)\n",
    "print(\"  ‚úÖ Added binary indicators for originally missing values\")\n",
    "\n",
    "print(\"\\n‚úÖ Missing data handled!\")\n",
    "print(f\"Remaining missing: {df_clean.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare imputation methods visually\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Original data (before missing values)\n",
    "original_credit = data['credit_score']\n",
    "df_missing_credit = df['credit_score'].copy()\n",
    "\n",
    "# Method 1: Mean imputation\n",
    "df_mean = df_missing_credit.copy()\n",
    "df_mean.fillna(df_mean.mean(), inplace=True)\n",
    "\n",
    "# Method 2: Median imputation\n",
    "df_median = df_missing_credit.copy()\n",
    "df_median.fillna(df_median.median(), inplace=True)\n",
    "\n",
    "# Method 3: KNN imputation (already done above)\n",
    "df_knn = df_clean['credit_score'].copy()\n",
    "\n",
    "# Plot comparisons\n",
    "axes[0, 0].hist(original_credit, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0, 0].set_title('Original Distribution (Before Missing)', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Credit Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].hist(df_mean, bins=30, alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[0, 1].axvline(df_missing_credit.mean(), color='red', linestyle='--', \n",
    "                   linewidth=2, label=f'Mean: {df_missing_credit.mean():.0f}')\n",
    "axes[0, 1].set_title('Mean Imputation\\n(Creates spike at mean)', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Credit Score')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].hist(df_median, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[1, 0].axvline(df_missing_credit.median(), color='red', linestyle='--', \n",
    "                   linewidth=2, label=f'Median: {df_missing_credit.median():.0f}')\n",
    "axes[1, 0].set_title('Median Imputation\\n(Creates spike at median)', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Credit Score')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].hist(df_knn, bins=30, alpha=0.7, color='purple', edgecolor='black')\n",
    "axes[1, 1].set_title('KNN Imputation\\n(Preserves distribution better)', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Credit Score')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"  ‚Ä¢ Mean/Median imputation creates artificial spikes\")\n",
    "print(\"  ‚Ä¢ KNN imputation preserves distribution shape better\")\n",
    "print(\"  ‚Ä¢ Always compare original vs imputed distributions\")\n",
    "print(\"  ‚Ä¢ Consider adding 'was_missing' indicator features\")\n",
    "\n",
    "print(\"\\nüéØ Interview Tip:\")\n",
    "print(\"  'I would first analyze the missing pattern (MCAR/MAR/MNAR),\")\n",
    "print(\"   then choose appropriate imputation based on percentage missing\")\n",
    "print(\"   and relationship with other variables. I always add missing\")\n",
    "print(\"   indicators and validate imputation preserves distributions.'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Outlier Detection and Treatment\n",
    "\n",
    "**Interview Question:** *\"How do you detect and handle outliers?\"*\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Detection Methods:**\n",
    "1. **Statistical:**\n",
    "   - Z-score (|z| > 3)\n",
    "   - IQR method (Q1 - 1.5√óIQR, Q3 + 1.5√óIQR)\n",
    "   - Modified Z-score (MAD-based)\n",
    "\n",
    "2. **Distance-based:**\n",
    "   - DBSCAN\n",
    "   - Isolation Forest\n",
    "   - Local Outlier Factor (LOF)\n",
    "\n",
    "**Treatment Strategies:**\n",
    "- Remove: If data errors or < 1% of data\n",
    "- Cap/Winsorize: Replace with percentile values\n",
    "- Transform: Log, square root, Box-Cox\n",
    "- Separate Model: Train separate model for outliers\n",
    "- Robust Methods: Use algorithms less sensitive to outliers\n",
    "\n",
    "**Important:** Always understand WHY outliers exist before removing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive outlier detection\n",
    "print(\"üîç OUTLIER DETECTION & TREATMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Fix invalid ages first\n",
    "df_clean.loc[df_clean['age'] < 0, 'age'] = age_median\n",
    "\n",
    "# Method 1: Z-score\n",
    "print(\"\\nüìä Method 1: Z-Score (|z| > 3)\")\n",
    "income_z = np.abs(stats.zscore(df_clean['income'].dropna()))\n",
    "outliers_zscore = df_clean[income_z > 3].index if len(income_z) == len(df_clean) else []\n",
    "print(f\"  Income outliers detected: {len(outliers_zscore)}\")\n",
    "\n",
    "# Method 2: IQR\n",
    "print(\"\\nüìä Method 2: IQR (Interquartile Range)\")\n",
    "Q1 = df_clean['income'].quantile(0.25)\n",
    "Q3 = df_clean['income'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers_iqr = df_clean[(df_clean['income'] < lower_bound) | \n",
    "                        (df_clean['income'] > upper_bound)].index\n",
    "print(f\"  Lower bound: ${lower_bound:,.0f}\")\n",
    "print(f\"  Upper bound: ${upper_bound:,.0f}\")\n",
    "print(f\"  Outliers detected: {len(outliers_iqr)}\")\n",
    "\n",
    "# Method 3: Isolation Forest\n",
    "print(\"\\nüìä Method 3: Isolation Forest (ML-based)\")\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "outlier_labels = iso_forest.fit_predict(df_clean[['income', 'age', 'credit_score']])\n",
    "outliers_iso = df_clean[outlier_labels == -1].index\n",
    "print(f\"  Outliers detected: {len(outliers_iso)}\")\n",
    "\n",
    "# Treatment Strategy\n",
    "print(\"\\nüîß Treatment: Winsorization (Capping)\")\n",
    "print(\"  Capping at 1st and 99th percentiles\")\n",
    "\n",
    "df_clean['income_original'] = df_clean['income'].copy()\n",
    "lower_cap = df_clean['income'].quantile(0.01)\n",
    "upper_cap = df_clean['income'].quantile(0.99)\n",
    "\n",
    "df_clean['income_capped'] = df_clean['income'].clip(lower=lower_cap, upper=upper_cap)\n",
    "\n",
    "print(f\"  Lower cap: ${lower_cap:,.0f}\")\n",
    "print(f\"  Upper cap: ${upper_cap:,.0f}\")\n",
    "print(f\"  Values capped: {(df_clean['income'] != df_clean['income_capped']).sum()}\")\n",
    "\n",
    "# Treatment Strategy 2: Log transformation\n",
    "print(\"\\nüîß Treatment: Log Transformation\")\n",
    "df_clean['income_log'] = np.log1p(df_clean['income'])  # log(1 + x) to handle zeros\n",
    "print(\"  ‚úÖ Applied log(1 + income) transformation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outlier detection methods\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Plot 1: Original distribution with outliers marked\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "ax1.hist(df_clean['income_original'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax1.axvline(lower_bound, color='red', linestyle='--', linewidth=2, label='IQR bounds')\n",
    "ax1.axvline(upper_bound, color='red', linestyle='--', linewidth=2)\n",
    "ax1.set_xlabel('Income ($)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Original Distribution\\n(Note extreme outliers)', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Box plot comparison\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "box_data = [df_clean['income_original'], df_clean['income_capped']]\n",
    "bp = ax2.boxplot(box_data, labels=['Original', 'Capped'], patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], ['lightblue', 'lightgreen']):\n",
    "    patch.set_facecolor(color)\n",
    "ax2.set_ylabel('Income ($)')\n",
    "ax2.set_title('Box Plot Comparison\\n(Before vs After Capping)', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 3: Capped distribution\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "ax3.hist(df_clean['income_capped'], bins=50, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "ax3.set_xlabel('Income ($)')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.set_title('After Winsorization\\n(Outliers capped)', fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Log transformed\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "ax4.hist(df_clean['income_log'], bins=50, alpha=0.7, color='coral', edgecolor='black')\n",
    "ax4.set_xlabel('Log(Income)')\n",
    "ax4.set_ylabel('Frequency')\n",
    "ax4.set_title('Log Transformation\\n(More normal distribution)', fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Q-Q plot before\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "stats.probplot(df_clean['income_original'], dist=\"norm\", plot=ax5)\n",
    "ax5.set_title('Q-Q Plot: Original\\n(Deviates from normal)', fontweight='bold')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Q-Q plot after log transform\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "stats.probplot(df_clean['income_log'], dist=\"norm\", plot=ax6)\n",
    "ax6.set_title('Q-Q Plot: Log Transformed\\n(Closer to normal)', fontweight='bold')\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"  ‚Ä¢ Outliers can severely skew distributions\")\n",
    "print(\"  ‚Ä¢ Capping preserves more data than removal\")\n",
    "print(\"  ‚Ä¢ Log transformation makes data more normal\")\n",
    "print(\"  ‚Ä¢ Q-Q plots help verify normality assumptions\")\n",
    "\n",
    "print(\"\\nüéØ Interview Tip:\")\n",
    "print(\"  'I use multiple methods (IQR, Z-score, Isolation Forest) to\")\n",
    "print(\"   detect outliers, then investigate whether they are errors or\")\n",
    "print(\"   valid extreme values. For treatment, I prefer Winsorization\")\n",
    "print(\"   over removal to preserve sample size, or use robust models\")\n",
    "print(\"   like tree-based methods that handle outliers naturally.'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Part 2: Feature Engineering - Creating Predictive Power\n",
    "\n",
    "**\"Applied machine learning is basically feature engineering\"** - Andrew Ng\n",
    "\n",
    "**Interview Question:** *\"What feature engineering techniques do you know?\"*\n",
    "\n",
    "**Key Techniques:**\n",
    "1. **Numerical Transformations:**\n",
    "   - Scaling/Normalization\n",
    "   - Log, sqrt, power transforms\n",
    "   - Binning/Discretization\n",
    "   - Polynomial features\n",
    "\n",
    "2. **Categorical Encoding:**\n",
    "   - One-Hot Encoding\n",
    "   - Label Encoding\n",
    "   - Target Encoding\n",
    "   - Frequency Encoding\n",
    "\n",
    "3. **Feature Creation:**\n",
    "   - Domain-specific features\n",
    "   - Interactions\n",
    "   - Aggregations\n",
    "   - Time-based features\n",
    "\n",
    "4. **Feature Selection:**\n",
    "   - Filter methods (correlation, chi-square)\n",
    "   - Wrapper methods (RFE)\n",
    "   - Embedded methods (L1 regularization)\n",
    "\n",
    "**Source:** \"Feature Engineering for Machine Learning\" Chapters 2-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete feature engineering pipeline\n",
    "print(\"üé® FEATURE ENGINEERING PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Use capped income for stability\n",
    "df_clean['income'] = df_clean['income_capped']\n",
    "\n",
    "# Clean up city names (handle inconsistencies)\n",
    "df_clean['city'] = df_clean['city'].replace({'New York': 'NYC', 'ny': 'NYC'})\n",
    "\n",
    "# 1. Create domain-specific features\n",
    "print(\"\\nüîß Step 1: Domain-Specific Feature Creation\")\n",
    "\n",
    "# Debt-to-income ratio (financial risk indicator)\n",
    "df_clean['debt_to_income'] = df_clean['num_credit_cards'] * 1000 / (df_clean['income'] + 1)\n",
    "print(\"  ‚úÖ Created: debt_to_income\")\n",
    "\n",
    "# Age groups (life stages)\n",
    "df_clean['age_group'] = pd.cut(df_clean['age'], \n",
    "                                bins=[0, 25, 35, 50, 65, 100],\n",
    "                                labels=['18-25', '26-35', '36-50', '51-65', '65+'])\n",
    "print(\"  ‚úÖ Created: age_group (binned age)\")\n",
    "\n",
    "# Income brackets\n",
    "df_clean['income_bracket'] = pd.cut(df_clean['income'],\n",
    "                                     bins=[0, 30000, 60000, 100000, np.inf],\n",
    "                                     labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "print(\"  ‚úÖ Created: income_bracket\")\n",
    "\n",
    "# Credit score categories\n",
    "df_clean['credit_category'] = pd.cut(df_clean['credit_score'],\n",
    "                                      bins=[0, 580, 670, 740, 800, 850],\n",
    "                                      labels=['Poor', 'Fair', 'Good', 'Very Good', 'Excellent'])\n",
    "print(\"  ‚úÖ Created: credit_category\")\n",
    "\n",
    "# Financial stability score (composite)\n",
    "df_clean['stability_score'] = (\n",
    "    (df_clean['credit_score'] / 850) * 0.4 +\n",
    "    (df_clean['years_employed'] / df_clean['years_employed'].max()) * 0.3 +\n",
    "    (df_clean['income'] / df_clean['income'].max()) * 0.3\n",
    ")\n",
    "print(\"  ‚úÖ Created: stability_score (weighted composite)\")\n",
    "\n",
    "# 2. Interaction features\n",
    "print(\"\\nüîß Step 2: Interaction Features\")\n",
    "\n",
    "df_clean['income_per_age'] = df_clean['income'] / (df_clean['age'] + 1)\n",
    "print(\"  ‚úÖ Created: income_per_age\")\n",
    "\n",
    "df_clean['credit_income_interaction'] = df_clean['credit_score'] * df_clean['income'] / 1000000\n",
    "print(\"  ‚úÖ Created: credit_income_interaction\")\n",
    "\n",
    "# 3. Polynomial features (for specific numerical columns)\n",
    "print(\"\\nüîß Step 3: Polynomial Features\")\n",
    "\n",
    "df_clean['income_squared'] = df_clean['income'] ** 2\n",
    "df_clean['age_squared'] = df_clean['age'] ** 2\n",
    "print(\"  ‚úÖ Created: squared features for income and age\")\n",
    "\n",
    "# 4. Aggregation features (by categorical groups)\n",
    "print(\"\\nüîß Step 4: Aggregation Features\")\n",
    "\n",
    "# Mean income by city\n",
    "city_income_mean = df_clean.groupby('city')['income'].transform('mean')\n",
    "df_clean['city_income_mean'] = city_income_mean\n",
    "df_clean['income_vs_city_avg'] = df_clean['income'] / (city_income_mean + 1)\n",
    "print(\"  ‚úÖ Created: city_income_mean, income_vs_city_avg\")\n",
    "\n",
    "# Education level aggregations\n",
    "edu_default_rate = df_clean.groupby('education')['default'].transform('mean')\n",
    "df_clean['education_risk'] = edu_default_rate\n",
    "print(\"  ‚úÖ Created: education_risk (default rate by education)\")\n",
    "\n",
    "print(f\"\\nüìä Total features created: {len(df_clean.columns)}\")\n",
    "print(f\"   Original features: {len(data.keys())}\")\n",
    "print(f\"   New features: {len(df_clean.columns) - len(data.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Encoding Categorical Variables\n",
    "\n",
    "**Interview Question:** *\"When would you use One-Hot Encoding vs Label Encoding?\"*\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**One-Hot Encoding:**\n",
    "- **Use when:** Categorical variable has no ordinal relationship\n",
    "- **Examples:** city, color, product type\n",
    "- **Pros:** No false ordinality\n",
    "- **Cons:** High dimensionality with many categories\n",
    "- **Models:** Linear models, neural networks\n",
    "\n",
    "**Label Encoding:**\n",
    "- **Use when:** Variable has ordinal relationship OR using tree-based models\n",
    "- **Examples:** education level (HS < BS < MS < PhD)\n",
    "- **Pros:** Compact representation\n",
    "- **Cons:** Implies order (false for nominal variables)\n",
    "- **Models:** Tree-based (handle it well), ordinal features\n",
    "\n",
    "**Other Methods:**\n",
    "- **Target Encoding:** Replace with target mean (watch for leakage!)\n",
    "- **Frequency Encoding:** Replace with category frequency\n",
    "- **Binary Encoding:** For high-cardinality features\n",
    "- **Embeddings:** For neural networks, high-cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive categorical encoding\n",
    "print(\"üè∑Ô∏è CATEGORICAL ENCODING STRATEGIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare data\n",
    "df_encoded = df_clean.copy()\n",
    "\n",
    "# Method 1: Label Encoding (for ordinal)\n",
    "print(\"\\nüìä Method 1: Label Encoding (Ordinal)\")\n",
    "\n",
    "# Education has natural order\n",
    "education_mapping = {\n",
    "    'High School': 0,\n",
    "    'Bachelor': 1,\n",
    "    'Master': 2,\n",
    "    'PhD': 3\n",
    "}\n",
    "df_encoded['education_encoded'] = df_encoded['education'].map(education_mapping)\n",
    "print(f\"  Education: {education_mapping}\")\n",
    "print(\"  ‚úÖ Preserves ordinal relationship\")\n",
    "\n",
    "# Credit category has order\n",
    "credit_cat_mapping = {\n",
    "    'Poor': 0,\n",
    "    'Fair': 1,\n",
    "    'Good': 2,\n",
    "    'Very Good': 3,\n",
    "    'Excellent': 4\n",
    "}\n",
    "df_encoded['credit_category_encoded'] = df_encoded['credit_category'].map(credit_cat_mapping)\n",
    "print(f\"  Credit Category: Poor=0 ‚Üí Excellent=4\")\n",
    "\n",
    "# Method 2: One-Hot Encoding (for nominal)\n",
    "print(\"\\nüìä Method 2: One-Hot Encoding (Nominal)\")\n",
    "\n",
    "# City has no natural order\n",
    "city_dummies = pd.get_dummies(df_encoded['city'], prefix='city', drop_first=True)\n",
    "df_encoded = pd.concat([df_encoded, city_dummies], axis=1)\n",
    "print(f\"  City: Created {len(city_dummies.columns)} binary columns\")\n",
    "print(f\"  Columns: {list(city_dummies.columns)}\")\n",
    "print(\"  ‚úÖ drop_first=True to avoid dummy variable trap\")\n",
    "\n",
    "# Method 3: Frequency Encoding\n",
    "print(\"\\nüìä Method 3: Frequency Encoding\")\n",
    "\n",
    "city_freq = df_encoded['city'].value_counts(normalize=True)\n",
    "df_encoded['city_frequency'] = df_encoded['city'].map(city_freq)\n",
    "print(\"  City: Replaced with occurrence frequency\")\n",
    "print(f\"  Example frequencies: {city_freq.head().to_dict()}\")\n",
    "\n",
    "# Method 4: Target Encoding (with proper CV)\n",
    "print(\"\\nüìä Method 4: Target Encoding (Mean of target)\")\n",
    "print(\"  ‚ö†Ô∏è WARNING: Must use cross-validation to prevent leakage!\")\n",
    "\n",
    "# Simple version (in production, use proper CV)\n",
    "city_target_mean = df_encoded.groupby('city')['default'].mean()\n",
    "df_encoded['city_target_encoded'] = df_encoded['city'].map(city_target_mean)\n",
    "print(f\"  City default rates: {city_target_mean.to_dict()}\")\n",
    "print(\"  ‚úÖ High correlation with target (powerful but risky!)\")\n",
    "\n",
    "# Method 5: Binary Encoding (for high cardinality)\n",
    "print(\"\\nüìä Method 5: Binary Encoding\")\n",
    "print(\"  Useful for features with 100s of categories (e.g., zip codes)\")\n",
    "print(\"  Represents category as binary digits\")\n",
    "print(\"  Example: 7 ‚Üí 111, 3 ‚Üí 011 (uses log2(n) columns)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Encoding complete!\")\n",
    "print(f\"   Total columns: {len(df_encoded.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize encoding effects\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Plot 1: Label encoding visualization\n",
    "ax1 = axes[0, 0]\n",
    "edu_counts = df_encoded.groupby('education_encoded')['default'].mean()\n",
    "ax1.bar(edu_counts.index, edu_counts.values, color='skyblue', edgecolor='black')\n",
    "ax1.set_xticks(edu_counts.index)\n",
    "ax1.set_xticklabels(['HS', 'Bachelor', 'Master', 'PhD'])\n",
    "ax1.set_xlabel('Education Level')\n",
    "ax1.set_ylabel('Default Rate')\n",
    "ax1.set_title('Label Encoding: Education\\n(Preserves order)', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: One-hot encoding visualization\n",
    "ax2 = axes[0, 1]\n",
    "city_cols = [col for col in df_encoded.columns if col.startswith('city_')]\n",
    "city_encoded_df = df_encoded[city_cols[:5]].head(10)\n",
    "sns.heatmap(city_encoded_df.T, cmap='RdYlGn', cbar=True, \n",
    "            linewidths=0.5, annot=True, fmt='g', ax=ax2)\n",
    "ax2.set_title('One-Hot Encoding: City\\n(Binary columns)', fontweight='bold')\n",
    "ax2.set_xlabel('Sample Index')\n",
    "\n",
    "# Plot 3: Frequency encoding\n",
    "ax3 = axes[0, 2]\n",
    "city_freq_df = df_encoded.groupby('city')['city_frequency'].first().sort_values(ascending=False)\n",
    "ax3.barh(range(len(city_freq_df)), city_freq_df.values, color='coral', edgecolor='black')\n",
    "ax3.set_yticks(range(len(city_freq_df)))\n",
    "ax3.set_yticklabels(city_freq_df.index)\n",
    "ax3.set_xlabel('Frequency')\n",
    "ax3.set_title('Frequency Encoding\\n(Occurrence rate)', fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Plot 4: Target encoding\n",
    "ax4 = axes[1, 0]\n",
    "city_target_df = df_encoded.groupby('city')['city_target_encoded'].first().sort_values()\n",
    "bars = ax4.barh(range(len(city_target_df)), city_target_df.values, \n",
    "                color='lightgreen', edgecolor='black')\n",
    "ax4.set_yticks(range(len(city_target_df)))\n",
    "ax4.set_yticklabels(city_target_df.index)\n",
    "ax4.set_xlabel('Default Rate')\n",
    "ax4.set_title('Target Encoding\\n(Mean of target variable)', fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Plot 5: Comparison of methods\n",
    "ax5 = axes[1, 1]\n",
    "encoding_summary = pd.DataFrame({\n",
    "    'Method': ['Label', 'One-Hot', 'Frequency', 'Target'],\n",
    "    'Columns Created': [1, len([col for col in df_encoded.columns if col.startswith('city_')]), 1, 1],\n",
    "    'Best For': ['Ordinal', 'Nominal', 'All', 'High Corr']\n",
    "})\n",
    "\n",
    "ax5.axis('off')\n",
    "table = ax5.table(cellText=encoding_summary.values, \n",
    "                  colLabels=encoding_summary.columns,\n",
    "                  cellLoc='center', loc='center',\n",
    "                  colWidths=[0.3, 0.35, 0.35])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2.5)\n",
    "\n",
    "for i in range(len(encoding_summary.columns)):\n",
    "    table[(0, i)].set_facecolor('lightblue')\n",
    "    table[(0, i)].set_text_props(weight='bold')\n",
    "\n",
    "ax5.set_title('Encoding Methods Summary', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# Plot 6: Dimensionality comparison\n",
    "ax6 = axes[1, 2]\n",
    "methods = ['Original', 'Label\\nEncoding', 'One-Hot\\nEncoding']\n",
    "dimensions = [\n",
    "    1,  # Original city column\n",
    "    1,  # Label encoding: 1 column\n",
    "    len([col for col in df_encoded.columns if col.startswith('city_')])  # One-hot: n columns\n",
    "]\n",
    "colors = ['gray', 'lightblue', 'lightcoral']\n",
    "bars = ax6.bar(methods, dimensions, color=colors, edgecolor='black')\n",
    "ax6.set_ylabel('Number of Columns')\n",
    "ax6.set_title('Dimensionality Impact\\n(City encoding)', fontweight='bold')\n",
    "ax6.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, dim in zip(bars, dimensions):\n",
    "    height = bar.get_height()\n",
    "    ax6.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(dim)}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Takeaways:\")\n",
    "print(\"  ‚Ä¢ One-Hot: Safe but increases dimensionality\")\n",
    "print(\"  ‚Ä¢ Label: Compact but implies order (use for trees)\")\n",
    "print(\"  ‚Ä¢ Frequency: Useful for high-cardinality features\")\n",
    "print(\"  ‚Ä¢ Target: Powerful but needs CV to prevent leakage\")\n",
    "\n",
    "print(\"\\nüéØ Interview Answer Template:\")\n",
    "print(\"  'For nominal variables like city, I use One-Hot encoding for\")\n",
    "print(\"   linear models, but Label encoding for tree-based models since\")\n",
    "print(\"   they can handle it. For ordinal variables like education, I use\")\n",
    "print(\"   Label encoding with proper ordering. For high-cardinality features,\")\n",
    "print(\"   I consider Target encoding with cross-validation or embeddings.'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Part 3: Feature Scaling - Critical for Many Algorithms\n",
    "\n",
    "**Interview Question:** *\"When and why do you need to scale features?\"*\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**When Scaling Matters:**\n",
    "- ‚úÖ **REQUIRED:** Linear Regression, Logistic Regression, SVM, KNN, Neural Networks, PCA\n",
    "- ‚ùå **NOT NEEDED:** Tree-based models (Decision Trees, Random Forest, XGBoost)\n",
    "\n",
    "**Why?**\n",
    "- Distance-based algorithms are sensitive to feature magnitude\n",
    "- Gradient descent converges faster with scaled features\n",
    "- Regularization (L1/L2) penalizes large coefficients equally\n",
    "\n",
    "**Scaling Methods:**\n",
    "\n",
    "1. **StandardScaler** (Z-score normalization)\n",
    "   - Formula: (x - Œº) / œÉ\n",
    "   - Result: mean=0, std=1\n",
    "   - Use: Most common, assumes normal distribution\n",
    "\n",
    "2. **MinMaxScaler** (Min-Max normalization)\n",
    "   - Formula: (x - min) / (max - min)\n",
    "   - Result: range [0, 1]\n",
    "   - Use: Bounded features, neural networks\n",
    "\n",
    "3. **RobustScaler**\n",
    "   - Formula: (x - median) / IQR\n",
    "   - Result: robust to outliers\n",
    "   - Use: Data with outliers\n",
    "\n",
    "4. **MaxAbsScaler**\n",
    "   - Formula: x / |max|\n",
    "   - Result: range [-1, 1]\n",
    "   - Use: Sparse data\n",
    "\n",
    "**CRITICAL:** Always fit scaler on training set only, then transform test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive feature scaling\n",
    "print(\"‚öñÔ∏è FEATURE SCALING STRATEGIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Select numerical features for scaling\n",
    "numerical_features = ['age', 'income', 'credit_score', 'years_employed', 'num_credit_cards']\n",
    "X = df_clean[numerical_features].copy()\n",
    "\n",
    "# Remove any remaining NaN\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "print(f\"\\nüìä Original Data Statistics:\")\n",
    "print(X.describe())\n",
    "\n",
    "# Method 1: StandardScaler\n",
    "print(\"\\n‚öñÔ∏è Method 1: StandardScaler (Z-score)\")\n",
    "scaler_standard = StandardScaler()\n",
    "X_standard = pd.DataFrame(\n",
    "    scaler_standard.fit_transform(X),\n",
    "    columns=[f\"{col}_standard\" for col in X.columns]\n",
    ")\n",
    "print(\"  Formula: (x - mean) / std\")\n",
    "print(\"  Result: mean ‚âà 0, std ‚âà 1\")\n",
    "print(f\"\\n  Transformed means: {X_standard.mean().round(6).to_dict()}\")\n",
    "print(f\"  Transformed stds: {X_standard.std().round(6).to_dict()}\")\n",
    "\n",
    "# Method 2: MinMaxScaler\n",
    "print(\"\\n‚öñÔ∏è Method 2: MinMaxScaler (Min-Max)\")\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_minmax = pd.DataFrame(\n",
    "    scaler_minmax.fit_transform(X),\n",
    "    columns=[f\"{col}_minmax\" for col in X.columns]\n",
    ")\n",
    "print(\"  Formula: (x - min) / (max - min)\")\n",
    "print(\"  Result: range [0, 1]\")\n",
    "print(f\"\\n  Transformed mins: {X_minmax.min().round(6).to_dict()}\")\n",
    "print(f\"  Transformed maxs: {X_minmax.max().round(6).to_dict()}\")\n",
    "\n",
    "# Method 3: RobustScaler\n",
    "print(\"\\n‚öñÔ∏è Method 3: RobustScaler (Robust to outliers)\")\n",
    "scaler_robust = RobustScaler()\n",
    "X_robust = pd.DataFrame(\n",
    "    scaler_robust.fit_transform(X),\n",
    "    columns=[f\"{col}_robust\" for col in X.columns]\n",
    ")\n",
    "print(\"  Formula: (x - median) / IQR\")\n",
    "print(\"  Result: median ‚âà 0, less affected by outliers\")\n",
    "print(f\"\\n  Transformed medians: {X_robust.median().round(6).to_dict()}\")\n",
    "\n",
    "# Compare distributions\n",
    "print(\"\\nüìä Comparison Summary:\")\n",
    "comparison = pd.DataFrame({\n",
    "    'Method': ['Original', 'StandardScaler', 'MinMaxScaler', 'RobustScaler'],\n",
    "    'Income Mean': [\n",
    "        X['income'].mean(),\n",
    "        X_standard['income_standard'].mean(),\n",
    "        X_minmax['income_minmax'].mean(),\n",
    "        X_robust['income_robust'].mean()\n",
    "    ],\n",
    "    'Income Std': [\n",
    "        X['income'].std(),\n",
    "        X_standard['income_standard'].std(),\n",
    "        X_minmax['income_minmax'].std(),\n",
    "        X_robust['income_robust'].std()\n",
    "    ],\n",
    "    'Income Range': [\n",
    "        X['income'].max() - X['income'].min(),\n",
    "        X_standard['income_standard'].max() - X_standard['income_standard'].min(),\n",
    "        X_minmax['income_minmax'].max() - X_minmax['income_minmax'].min(),\n",
    "        X_robust['income_robust'].max() - X_robust['income_robust'].min()\n",
    "    ]\n",
    "})\n",
    "print(comparison.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scaling effects\n",
    "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "\n",
    "# Feature to visualize in detail\n",
    "feature = 'income'\n",
    "feature_idx = numerical_features.index(feature)\n",
    "\n",
    "# Row 1: Distributions\n",
    "axes[0, 0].hist(X[feature], bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0, 0].set_title(f'Original {feature}', fontweight='bold')\n",
    "axes[0, 0].set_xlabel(feature)\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].hist(X_standard.iloc[:, feature_idx], bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[0, 1].set_title(f'StandardScaler', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Scaled Value')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 2].hist(X_minmax.iloc[:, feature_idx], bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[0, 2].set_title(f'MinMaxScaler', fontweight='bold')\n",
    "axes[0, 2].set_xlabel('Scaled Value')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 3].hist(X_robust.iloc[:, feature_idx], bins=50, alpha=0.7, color='purple', edgecolor='black')\n",
    "axes[0, 3].set_title(f'RobustScaler', fontweight='bold')\n",
    "axes[0, 3].set_xlabel('Scaled Value')\n",
    "axes[0, 3].grid(True, alpha=0.3)\n",
    "\n",
    "# Row 2: Box plots\n",
    "box_data = [\n",
    "    X[feature],\n",
    "    X_standard.iloc[:, feature_idx],\n",
    "    X_minmax.iloc[:, feature_idx],\n",
    "    X_robust.iloc[:, feature_idx]\n",
    "]\n",
    "\n",
    "for idx, (ax, data, title, color) in enumerate(zip(\n",
    "    axes[1, :],\n",
    "    box_data,\n",
    "    ['Original', 'StandardScaler', 'MinMaxScaler', 'RobustScaler'],\n",
    "    ['lightblue', 'lightgreen', 'lightyellow', 'lightpink']\n",
    ")):\n",
    "    bp = ax.boxplot(data, vert=True, patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor(color)\n",
    "    ax.set_title(title, fontweight='bold')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Row 3: Compare all features side by side\n",
    "# Original data\n",
    "ax = axes[2, 0]\n",
    "X_sample = X.head(100)\n",
    "for col in X_sample.columns:\n",
    "    ax.plot(X_sample[col].values, alpha=0.6, label=col)\n",
    "ax.set_title('Original Features\\n(Different scales)', fontweight='bold')\n",
    "ax.set_xlabel('Sample')\n",
    "ax.set_ylabel('Value')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# StandardScaler\n",
    "ax = axes[2, 1]\n",
    "X_standard_sample = X_standard.head(100)\n",
    "for col in X_standard_sample.columns:\n",
    "    ax.plot(X_standard_sample[col].values, alpha=0.6)\n",
    "ax.set_title('StandardScaler\\n(All features comparable)', fontweight='bold')\n",
    "ax.set_xlabel('Sample')\n",
    "ax.set_ylabel('Scaled Value')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# MinMaxScaler\n",
    "ax = axes[2, 2]\n",
    "X_minmax_sample = X_minmax.head(100)\n",
    "for col in X_minmax_sample.columns:\n",
    "    ax.plot(X_minmax_sample[col].values, alpha=0.6)\n",
    "ax.set_title('MinMaxScaler\\n(All in [0,1])', fontweight='bold')\n",
    "ax.set_xlabel('Sample')\n",
    "ax.set_ylabel('Scaled Value')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# RobustScaler\n",
    "ax = axes[2, 3]\n",
    "X_robust_sample = X_robust.head(100)\n",
    "for col in X_robust_sample.columns:\n",
    "    ax.plot(X_robust_sample[col].values, alpha=0.6)\n",
    "ax.set_title('RobustScaler\\n(Less affected by outliers)', fontweight='bold')\n",
    "ax.set_xlabel('Sample')\n",
    "ax.set_ylabel('Scaled Value')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° When to Use Each Scaler:\")\n",
    "print(\"\\nüìä StandardScaler:\")\n",
    "print(\"  ‚úÖ Most common choice\")\n",
    "print(\"  ‚úÖ Data is approximately normal\")\n",
    "print(\"  ‚úÖ Linear models, SVM, neural networks\")\n",
    "print(\"  ‚ùå Many outliers present\")\n",
    "\n",
    "print(\"\\nüìä MinMaxScaler:\")\n",
    "print(\"  ‚úÖ Need bounded range [0, 1]\")\n",
    "print(\"  ‚úÖ Neural networks (sigmoid/tanh activation)\")\n",
    "print(\"  ‚úÖ Image data\")\n",
    "print(\"  ‚ùå Sensitive to outliers\")\n",
    "\n",
    "print(\"\\nüìä RobustScaler:\")\n",
    "print(\"  ‚úÖ Many outliers in data\")\n",
    "print(\"  ‚úÖ Want outliers to have less influence\")\n",
    "print(\"  ‚úÖ Heavy-tailed distributions\")\n",
    "print(\"  ‚ùå Assumes median-based normality\")\n",
    "\n",
    "print(\"\\nüéØ Interview Answer Template:\")\n",
    "print(\"  'I always scale features for distance-based algorithms like KNN,\")\n",
    "print(\"   SVM, and linear models with regularization. StandardScaler is my\")\n",
    "print(\"   default choice, but I use RobustScaler if I detect outliers, and\")\n",
    "print(\"   MinMaxScaler for neural networks when I need bounded inputs.\")\n",
    "print(\"   Critically, I fit the scaler only on training data to prevent\")\n",
    "print(\"   data leakage, then transform both train and test sets.'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Part 4: Building Production Pipeline\n",
    "\n",
    "**Interview Question:** *\"How do you prevent data leakage in your ML pipeline?\"*\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Data Leakage:** When information from test set leaks into training process\n",
    "\n",
    "**Common Causes:**\n",
    "1. Scaling before train-test split\n",
    "2. Feature engineering using entire dataset\n",
    "3. Target encoding without cross-validation\n",
    "4. Time series: using future to predict past\n",
    "5. Including target variable in features\n",
    "\n",
    "**Prevention:**\n",
    "- ‚úÖ Always split data FIRST\n",
    "- ‚úÖ Fit preprocessors only on training data\n",
    "- ‚úÖ Use Pipelines (sklearn.pipeline.Pipeline)\n",
    "- ‚úÖ Cross-validation for meta-features\n",
    "- ‚úÖ Be careful with time-based features\n",
    "\n",
    "**Pipeline Benefits:**\n",
    "- Prevents leakage automatically\n",
    "- Reproducible\n",
    "- Easier to deploy\n",
    "- Cleaner code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build complete production pipeline\n",
    "print(\"üèóÔ∏è PRODUCTION-READY ML PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare clean dataset\n",
    "# Select features and target\n",
    "feature_columns = ['age', 'income', 'credit_score', 'years_employed', \n",
    "                   'num_credit_cards', 'education', 'city']\n",
    "X = df_clean[feature_columns].copy()\n",
    "y = df_clean['default'].copy()\n",
    "\n",
    "# Handle any remaining missing values\n",
    "X['age'] = X['age'].fillna(X['age'].median())\n",
    "X['income'] = X['income'].fillna(X['income'].median())\n",
    "X['credit_score'] = X['credit_score'].fillna(X['credit_score'].median())\n",
    "\n",
    "print(\"\\nüìä Dataset Preparation:\")\n",
    "print(f\"  Samples: {len(X)}\")\n",
    "print(f\"  Features: {X.shape[1]}\")\n",
    "print(f\"  Target distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# CRITICAL: Split BEFORE any preprocessing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÇÔ∏è Train-Test Split:\")\n",
    "print(f\"  Train: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  Test: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  ‚úÖ Stratified split maintains class balance\")\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numerical_features = ['age', 'income', 'credit_score', 'years_employed', 'num_credit_cards']\n",
    "categorical_features = ['education', 'city']\n",
    "\n",
    "print(f\"\\nüîß Feature Types:\")\n",
    "print(f\"  Numerical: {numerical_features}\")\n",
    "print(f\"  Categorical: {categorical_features}\")\n",
    "\n",
    "# Create preprocessing pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Numerical pipeline\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine pipelines\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ])\n",
    "\n",
    "print(\"\\nüèóÔ∏è Pipeline Structure:\")\n",
    "print(\"\")\n",
    "print(\"  Numerical Pipeline:\")\n",
    "print(\"    1. SimpleImputer (median)\")\n",
    "print(\"    2. StandardScaler\")\n",
    "print(\"\")\n",
    "print(\"  Categorical Pipeline:\")\n",
    "print(\"    1. SimpleImputer (constant='missing')\")\n",
    "print(\"    2. OneHotEncoder (handle_unknown='ignore')\")\n",
    "print(\"\")\n",
    "\n",
    "# Fit and transform\n",
    "print(\"\\n‚öôÔ∏è Fitting Pipeline on Training Data...\")\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "print(\"  ‚úÖ Pipeline fitted on training data only\")\n",
    "\n",
    "print(\"\\n‚öôÔ∏è Transforming Test Data...\")\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "print(\"  ‚úÖ Test data transformed using training statistics\")\n",
    "\n",
    "print(f\"\\nüìä Processed Data Shape:\")\n",
    "print(f\"  Train: {X_train_processed.shape}\")\n",
    "print(f\"  Test: {X_test_processed.shape}\")\n",
    "print(f\"  Features expanded: {X.shape[1]} ‚Üí {X_train_processed.shape[1]}\")\n",
    "print(f\"  (Due to one-hot encoding)\")\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "feature_names = (numerical_features + \n",
    "                list(preprocessor.named_transformers_['cat']\n",
    "                     .named_steps['onehot'].get_feature_names_out(categorical_features)))\n",
    "\n",
    "print(f\"\\nüìã Final Feature Names ({len(feature_names)} total):\")\n",
    "print(f\"  Numerical (5): {numerical_features}\")\n",
    "print(f\"  Categorical ({len(feature_names) - 5}): {list(feature_names[5:])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete ML pipeline with model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "print(\"ü§ñ COMPLETE ML PIPELINE WITH MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create full pipeline (preprocessing + model)\n",
    "full_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "print(\"\\nüèóÔ∏è Full Pipeline:\")\n",
    "print(\"  1. Preprocessing (numerical + categorical)\")\n",
    "print(\"  2. Logistic Regression Classifier\")\n",
    "\n",
    "# Train\n",
    "print(\"\\n‚öôÔ∏è Training Pipeline...\")\n",
    "full_pipeline.fit(X_train, y_train)\n",
    "print(\"  ‚úÖ Training complete!\")\n",
    "\n",
    "# Predict\n",
    "print(\"\\nüîÆ Making Predictions...\")\n",
    "y_train_pred = full_pipeline.predict(X_train)\n",
    "y_test_pred = full_pipeline.predict(X_test)\n",
    "y_test_proba = full_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"\\nüìä Model Performance:\")\n",
    "print(\"\\nTraining Set:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_train, y_train_pred):.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_train, y_train_pred):.4f}\")\n",
    "print(f\"  Recall: {recall_score(y_train, y_train_pred):.4f}\")\n",
    "print(f\"  F1-Score: {f1_score(y_train, y_train_pred):.4f}\")\n",
    "\n",
    "print(\"\\nTest Set:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_test, y_test_pred):.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_test_pred):.4f}\")\n",
    "print(f\"  Recall: {recall_score(y_test, y_test_pred):.4f}\")\n",
    "print(f\"  F1-Score: {f1_score(y_test, y_test_pred):.4f}\")\n",
    "print(f\"  ROC-AUC: {roc_auc_score(y_test, y_test_proba):.4f}\")\n",
    "\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['No Default', 'Default']))\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['No Default', 'Default'],\n",
    "            yticklabels=['No Default', 'Default'])\n",
    "axes[0].set_title('Confusion Matrix', fontweight='bold', fontsize=14)\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba)\n",
    "axes[1].plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {roc_auc_score(y_test, y_test_proba):.3f})')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')\n",
    "axes[1].set_xlabel('False Positive Rate', fontsize=12)\n",
    "axes[1].set_ylabel('True Positive Rate', fontsize=12)\n",
    "axes[1].set_title('ROC Curve', fontweight='bold', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline Benefits:\")\n",
    "print(\"  ‚Ä¢ No data leakage (transformations fit on train only)\")\n",
    "print(\"  ‚Ä¢ Easy to deploy (single object)\")\n",
    "print(\"  ‚Ä¢ Reproducible (same transformations always)\")\n",
    "print(\"  ‚Ä¢ Clean code (no manual transformations)\")\n",
    "\n",
    "print(\"\\nüéØ Interview Answer Template:\")\n",
    "print(\"  'I always build scikit-learn Pipelines that combine preprocessing\")\n",
    "print(\"   and modeling. I split data first, fit the pipeline only on training\")\n",
    "print(\"   data to prevent leakage, use ColumnTransformer for different feature\")\n",
    "print(\"   types, and the pipeline makes deployment straightforward since all\")\n",
    "print(\"   transformations are encapsulated in a single object.'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
