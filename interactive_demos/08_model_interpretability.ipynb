{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç Model Interpretability & Explainability - SHAP, LIME, and Beyond\n",
    "\n",
    "**\"A model you can't explain is a model you can't trust\"**\n",
    "\n",
    "Master the techniques to explain black-box models - critical for production ML, compliance, and interviews!\n",
    "\n",
    "## üìö What You'll Learn:\n",
    "\n",
    "### **1. Why Interpretability Matters**\n",
    "- Regulatory compliance (GDPR, finance, healthcare)\n",
    "- Building trust with stakeholders\n",
    "- Debugging and improving models\n",
    "- Detecting bias and fairness issues\n",
    "\n",
    "### **2. Feature Importance Methods**\n",
    "- Permutation importance\n",
    "- Drop-column importance\n",
    "- Built-in feature importances\n",
    "- Comparison and when to use each\n",
    "\n",
    "### **3. SHAP (SHapley Additive exPlanations)**\n",
    "- Game theory foundations\n",
    "- Shapley values explained\n",
    "- Global and local interpretability\n",
    "- Summary plots, dependence plots, force plots\n",
    "- TreeSHAP for fast computation\n",
    "\n",
    "### **4. LIME (Local Interpretable Model-agnostic Explanations)**\n",
    "- Local linear approximations\n",
    "- Perturbing instances\n",
    "- When LIME is better than SHAP\n",
    "- Applications to images and text\n",
    "\n",
    "### **5. Partial Dependence Plots (PDP)**\n",
    "- Marginal effects of features\n",
    "- Individual Conditional Expectation (ICE)\n",
    "- Accumulated Local Effects (ALE)\n",
    "\n",
    "### **6. Production Best Practices**\n",
    "- Explaining predictions in real-time\n",
    "- Model documentation\n",
    "- Stakeholder communication\n",
    "\n",
    "## üéØ Interview Topics:\n",
    "\n",
    "- **\"How do you explain a complex model to non-technical stakeholders?\"**\n",
    "- **\"What's the difference between SHAP and LIME?\"**\n",
    "- **\"How would you detect if your model is biased?\"**\n",
    "- **\"Explain feature importance vs. feature effect\"**\n",
    "- **\"Why might built-in feature importances be misleading?\"**\n",
    "\n",
    "**Sources:**\n",
    "- \"A Unified Approach to Interpreting Model Predictions\" - Lundberg & Lee (2017)\n",
    "- \"Why Should I Trust You?\" - Ribeiro et al. (2016)\n",
    "- \"Interpretable Machine Learning\" - Molnar (2020)\n",
    "- Shapley (1953) - Game Theory Foundations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_boston, load_breast_cancer, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Interpretability libraries\n",
    "try:\n",
    "    import shap\n",
    "    print(f\"‚úÖ SHAP: {shap.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è SHAP not installed: pip install shap\")\n",
    "\n",
    "try:\n",
    "    import lime\n",
    "    import lime.lime_tabular\n",
    "    print(f\"‚úÖ LIME installed\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è LIME not installed: pip install lime\")\n",
    "\n",
    "# Plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "sns.set_palette('husl')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"\\n‚úÖ Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Part 1: Why Interpretability Matters\n",
    "\n",
    "**Interview Question:** *\"Why is model interpretability important? Give real-world examples.\"*\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Model interpretability is crucial for multiple reasons beyond just understanding how a model works.\n",
    "\n",
    "### **1. Regulatory Compliance & Legal Requirements**\n",
    "\n",
    "**GDPR (Europe) - Right to Explanation:**\n",
    "- Users have right to understand automated decisions\n",
    "- Must explain why loan was denied, insurance rejected, etc.\n",
    "- Cannot use pure \"black box\" without explanation\n",
    "- **Penalty:** Up to 4% of global revenue\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Bank loan application rejected by ML model:\n",
    "\n",
    "‚ùå Unacceptable: \"Our algorithm rejected your application\"\n",
    "‚úÖ Required: \"Rejected due to: \n",
    "   - Debt-to-income ratio too high (35% vs max 30%)\n",
    "   - Credit score below threshold (650 vs min 680)\n",
    "   - Recent late payments (2 in last 6 months)\"\n",
    "```\n",
    "\n",
    "**Healthcare (FDA Requirements):**\n",
    "- Medical diagnosis systems must be explainable\n",
    "- Doctors need to understand WHY model recommended treatment\n",
    "- Can't deploy black box that recommends surgery\n",
    "\n",
    "**Finance (Fair Lending Laws):**\n",
    "- Equal Credit Opportunity Act\n",
    "- Must prove decisions not based on protected characteristics\n",
    "- Need to explain any adverse action\n",
    "\n",
    "### **2. Building Trust with Stakeholders**\n",
    "\n",
    "**Business Stakeholders:**\n",
    "- CFO won't approve $1M budget for \"magic black box\"\n",
    "- Product managers need to understand limitations\n",
    "- Sales team needs to explain to customers\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Churn Prediction Model Presentation:\n",
    "\n",
    "‚ùå Bad: \"Our neural network predicts 23% churn rate\"\n",
    "‚úÖ Good: \"Model predicts 23% churn driven by:\n",
    "   ‚Ä¢ 15% due to price sensitivity (top feature)\n",
    "   ‚Ä¢ 5% due to poor customer service scores\n",
    "   ‚Ä¢ 3% due to competitor offerings\n",
    "   \n",
    "   Actionable insights:\n",
    "   - Target price-sensitive users with discounts\n",
    "   - Improve customer service for high-risk segments\n",
    "   - Monitor competitor pricing\"\n",
    "```\n",
    "\n",
    "**End Users:**\n",
    "- Customers want to know why they got recommendation\n",
    "- Doctors need explanation before trusting diagnosis\n",
    "- Drivers want to understand autonomous vehicle decisions\n",
    "\n",
    "### **3. Model Debugging & Improvement**\n",
    "\n",
    "**Detecting Data Leakage:**\n",
    "\n",
    "```python\n",
    "# Example: Model too accurate (99%!)\n",
    "feature_importance = model.feature_importances_\n",
    "\n",
    "print(\"Top features:\")\n",
    "print(\"1. transaction_id: 0.85\")  # üö® RED FLAG!\n",
    "print(\"2. customer_age: 0.10\")\n",
    "print(\"3. purchase_amount: 0.05\")\n",
    "\n",
    "# Issue: transaction_id shouldn't predict anything!\n",
    "# This reveals data leakage - model memorizing training IDs\n",
    "```\n",
    "\n",
    "**Finding Model Limitations:**\n",
    "```\n",
    "Credit scoring model shows:\n",
    "- Income is most important (expected)\n",
    "- Zip code is 2nd most important (üö® potential bias!)\n",
    "- Employment length barely matters (ü§î unexpected)\n",
    "\n",
    "Actions:\n",
    "- Investigate zip code bias (redlining?)\n",
    "- Review why employment length isn't important\n",
    "- Ensure model aligns with domain knowledge\n",
    "```\n",
    "\n",
    "### **4. Detecting Bias & Ensuring Fairness**\n",
    "\n",
    "**Real Example - Amazon Recruiting Tool (2018):**\n",
    "```\n",
    "Problem: Resume screening model biased against women\n",
    "\n",
    "Model learned:\n",
    "- Penalized resumes with \"women's\" (e.g., women's chess club)\n",
    "- Penalized graduates from all-women's colleges\n",
    "\n",
    "Why: Training data (10 years of resumes) had bias\n",
    "- Tech industry historically male-dominated  \n",
    "- Past hiring patterns encoded gender bias\n",
    "\n",
    "Result: Amazon scrapped the system\n",
    "```\n",
    "\n",
    "**How Interpretability Helps:**\n",
    "```python\n",
    "# SHAP analysis reveals:\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Feature: \"attended_womens_college\"\n",
    "# Average SHAP value: -0.3  # Negative = hurts chances!\n",
    "\n",
    "# This immediately flags potential discrimination\n",
    "# Would be invisible without interpretability\n",
    "```\n",
    "\n",
    "### **5. Model Validation & Sanity Checks**\n",
    "\n",
    "**Domain Expert Validation:**\n",
    "```\n",
    "Medical diagnosis model says:\n",
    "\"Patient has 90% chance of diabetes\"\n",
    "\n",
    "SHAP explanation:\n",
    "+ Age (60): +0.3\n",
    "+ BMI (32): +0.4  \n",
    "+ Blood pressure (140/90): +0.2\n",
    "- Exercise (regular): -0.1\n",
    "\n",
    "Doctor review: ‚úÖ Makes medical sense!\n",
    "Model is learning correct risk factors.\n",
    "```\n",
    "\n",
    "**Catching Errors:**\n",
    "```\n",
    "House price model shows:\n",
    "\"Predicted price: $500,000\"\n",
    "\n",
    "SHAP:\n",
    "+ Location (suburb): +$200k ‚úÖ\n",
    "+ Square footage (2000): +$150k ‚úÖ  \n",
    "+ Pool: +$100k ‚úÖ\n",
    "+ Number of bathrooms (7): +$80k üö® Wait...\n",
    "\n",
    "Issue: Data entry error (probably meant 2-3 bathrooms)\n",
    "Without interpretability, would deploy flawed model!\n",
    "```\n",
    "\n",
    "### **6. Scientific Understanding**\n",
    "\n",
    "**Research Applications:**\n",
    "- Climate models: Understanding which factors drive warming\n",
    "- Drug discovery: Which molecular features matter\n",
    "- Social science: What predicts policy outcomes\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "ML model predicts disease outbreak:\n",
    "\n",
    "Interpretation reveals:\n",
    "- Temperature: Most important (new scientific insight!)\n",
    "- Humidity: Second (validates existing theory)\n",
    "- Population density: Third (expected)\n",
    "\n",
    "‚Üí Leads to new hypothesis about temperature-disease link\n",
    "‚Üí Further research validates mechanism  \n",
    "‚Üí Model interpretability drives scientific discovery!\n",
    "```\n",
    "\n",
    "### **Interview Answer Template:**\n",
    "\n",
    "\"Model interpretability is crucial for multiple reasons:\n",
    "\n",
    "1. **Legal/Regulatory:** GDPR requires explainability for automated decisions. In finance and healthcare, you must explain rejections.\n",
    "\n",
    "2. **Trust:** Stakeholders won't adopt a model they don't understand. In my experience, explaining feature importance and decision logic is key to getting buy-in.\n",
    "\n",
    "3. **Debugging:** Interpretability helps catch data leakage, bias, and model errors. For example, if you see transaction_id as the top feature, you know something's wrong.\n",
    "\n",
    "4. **Bias Detection:** Critical for fairness. Amazon's biased recruiting tool could have been caught early with proper interpretability analysis.\n",
    "\n",
    "5. **Domain Validation:** Subject matter experts can verify model logic makes sense. A medical diagnosis model should rely on known risk factors.\n",
    "\n",
    "In production, I always include explainability alongside accuracy metrics. It's not just nice-to-have‚Äîit's essential for responsible AI deployment.\"\n",
    "\n",
    "### **Common Follow-up: \"What's the tradeoff between accuracy and interpretability?\"**\n",
    "\n",
    "\"There IS a tradeoff, but it's smaller than people think:\n",
    "\n",
    "- **High interpretability:** Linear regression, decision trees (can visualize)\n",
    "- **Medium:** Random Forest (feature importance), XGBoost (SHAP)\n",
    "- **Low:** Deep neural networks (need LIME/SHAP)\n",
    "\n",
    "Modern techniques like SHAP can make even neural networks interpretable. The key is matching the tool to the use case:\n",
    "\n",
    "- Medical diagnosis: May sacrifice 2% accuracy for full interpretability\n",
    "- Image recognition: Can use black box with local explanations (LIME)\n",
    "- Credit scoring: Must be interpretable, so use interpretable models or SHAP\n",
    "\n",
    "In most cases, interpretable models (RF, XGBoost with SHAP) are accurate enough AND explainable.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Part 2: SHAP - The Gold Standard\n",
    "\n",
    "**Interview Question:** *\"Explain SHAP values and why they're better than traditional feature importance.\"*\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) is based on game theory and provides the most theoretically sound feature attribution method.\n",
    "\n",
    "### **The Shapley Value Concept:**\n",
    "\n",
    "**Origin: Game Theory (1953)**\n",
    "\n",
    "Imagine 3 players cooperate to win $100:\n",
    "- How much should each player get?\n",
    "- Depends on their marginal contribution\n",
    "\n",
    "**Shapley Value = Fair distribution based on contribution**\n",
    "\n",
    "**Applied to ML:**\n",
    "- \"Players\" = Features\n",
    "- \"Payoff\" = Model prediction\n",
    "- \"SHAP value\" = How much each feature contributed to prediction\n",
    "\n",
    "### **Mathematical Definition:**\n",
    "\n",
    "$$\\phi_i = \\sum_{S \\subseteq F \\setminus \\{i\\}} \\frac{|S|!(|F|-|S|-1)!}{|F|!} [f(S \\cup \\{i\\}) - f(S)]$$\n",
    "\n",
    "Where:\n",
    "- $\\phi_i$ = SHAP value for feature $i$\n",
    "- $F$ = all features\n",
    "- $S$ = subset of features\n",
    "- $f(S)$ = model prediction using only features in $S$\n",
    "\n",
    "**Intuition:** Average marginal contribution of feature $i$ across all possible feature coalitions.\n",
    "\n",
    "### **Key Properties (Why SHAP is Superior):**\n",
    "\n",
    "**1. Local Accuracy (Additivity)**\n",
    "$$f(x) = \\phi_0 + \\sum_{i=1}^{n} \\phi_i$$\n",
    "\n",
    "- SHAP values sum to the prediction!\n",
    "- Can decompose any prediction exactly\n",
    "- Unlike feature importance, accounts for interactions\n",
    "\n",
    "**2. Consistency**\n",
    "- If feature becomes more important, SHAP value increases\n",
    "- Sounds obvious, but many methods violate this!\n",
    "\n",
    "**3. Missingness**\n",
    "- If feature isn't used, SHAP value = 0\n",
    "- Guaranteed by theory\n",
    "\n",
    "**4. Symmetry** \n",
    "- Features with same contribution get same SHAP value\n",
    "- Fair attribution\n",
    "\n",
    "### **SHAP vs Traditional Feature Importance:**\n",
    "\n",
    "| Aspect | Traditional Importance | SHAP Values |\n",
    "|--------|----------------------|-------------|\n",
    "| **What it measures** | Global contribution | Local + Global |\n",
    "| **Handles interactions** | No | Yes |\n",
    "| **Sums to prediction** | No | Yes |\n",
    "| **Direction** | Only magnitude | Positive/Negative |\n",
    "| **Per-prediction** | No | Yes |\n",
    "| **Theoretical guarantee** | No | Yes (Shapley axioms) |\n",
    "| **Comparison across models** | Hard | Easy |\n",
    "| **Computation** | Fast | Slower (but TreeSHAP is fast) |\n",
    "\n",
    "### **Example Comparison:**\n",
    "\n",
    "```python\n",
    "# House price prediction: $500,000\n",
    "baseline = $300,000 (average house price)\n",
    "\n",
    "Traditional Feature Importance:\n",
    "- Location: 0.4 (40% important)\n",
    "- Size: 0.3 (30% important)\n",
    "- Age: 0.2 (20% important)\n",
    "- Pool: 0.1 (10% important)\n",
    "\n",
    "‚ùå Problems:\n",
    "- Doesn't explain THIS prediction\n",
    "- Doesn't say if features increase or decrease price\n",
    "- Doesn't sum to anything meaningful\n",
    "\n",
    "SHAP Values for THIS House:\n",
    "Baseline: $300,000\n",
    "+ Location (premium area): +$120,000\n",
    "+ Size (2000 sqft): +$60,000\n",
    "+ Age (new): +$30,000\n",
    "+ Pool: +$20,000\n",
    "- Needs repairs: -$30,000\n",
    "= Prediction: $500,000 ‚úÖ\n",
    "\n",
    "‚úÖ Advantages:\n",
    "- Explains THIS specific prediction\n",
    "- Shows direction (positive/negative)\n",
    "- Values sum to prediction!\n",
    "- Can show to customer/stakeholder\n",
    "```\n",
    "\n",
    "### **Interview Pro Tip:**\n",
    "\n",
    "\"SHAP values are better than traditional feature importance because:\n",
    "\n",
    "1. **Local explanations:** Can explain individual predictions, not just global model behavior\n",
    "2. **Additivity:** SHAP values sum to the actual prediction, making them intuitive\n",
    "3. **Direction:** Shows whether feature increases or decreases prediction\n",
    "4. **Interactions:** Accounts for feature interactions, not just individual effects\n",
    "5. **Theoretical foundation:** Based on game theory, not heuristics\n",
    "\n",
    "Traditional importance just says 'age is 30% important' - but SHAP says 'this 60-year-old person gets +0.3 risk score from their age, while a 30-year-old gets -0.2'. That's actionable!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
