{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Natural Language Processing (NLP) Fundamentals\n",
    "\n",
    "## Complete Guide from Text Processing to Transformers\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Text preprocessing and tokenization\n",
    "- Word embeddings (Word2Vec, GloVe)\n",
    "- Sequence models (RNN, LSTM, GRU)\n",
    "- Attention mechanisms\n",
    "- Transformers architecture\n",
    "- Real NLP applications\n",
    "\n",
    "**Prerequisites:** Deep Learning Fundamentals (Notebook 06)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Deep learning\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    TORCH_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"PyTorch not installed. Some examples will use NumPy instead.\")\n",
    "    TORCH_AVAILABLE = False\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úì Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Text Preprocessing\n",
    "\n",
    "### 1.1 Tokenization\n",
    "\n",
    "**Tokenization:** Breaking text into individual tokens (words, subwords, or characters)\n",
    "\n",
    "**Why it's important:**\n",
    "- First step in any NLP pipeline\n",
    "- Affects model performance significantly\n",
    "- Different tokenization strategies for different tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"\"\"\n",
    "Natural Language Processing (NLP) is fascinating! It allows computers to understand, \n",
    "interpret, and generate human language. Modern NLP uses deep learning techniques.\n",
    "\"\"\"\n",
    "\n",
    "# Word tokenization\n",
    "word_tokens = word_tokenize(text.lower())\n",
    "print(\"Word tokens:\")\n",
    "print(word_tokens[:20])\n",
    "print(f\"\\nTotal word tokens: {len(word_tokens)}\")\n",
    "\n",
    "# Sentence tokenization\n",
    "sent_tokens = sent_tokenize(text)\n",
    "print(f\"\\nSentences: {len(sent_tokens)}\")\n",
    "for i, sent in enumerate(sent_tokens, 1):\n",
    "    print(f\"{i}. {sent.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Complete text cleaning pipeline.\"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Example\n",
    "dirty_text = \"Check out https://example.com! Email: test@test.com. Price: $99.99\"\n",
    "clean = clean_text(dirty_text)\n",
    "\n",
    "print(f\"Original: {dirty_text}\")\n",
    "print(f\"Cleaned:  {clean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Stopword Removal and Stemming/Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stopwords\n",
    "filtered_tokens = [w for w in word_tokens if w not in stop_words and w.isalpha()]\n",
    "print(f\"Tokens after stopword removal: {len(filtered_tokens)}\")\n",
    "print(filtered_tokens[:15])\n",
    "\n",
    "# Stemming (faster, less accurate)\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(w) for w in filtered_tokens[:5]]\n",
    "print(f\"\\nStemmed: {stemmed}\")\n",
    "\n",
    "# Lemmatization (slower, more accurate)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(w) for w in filtered_tokens[:5]]\n",
    "print(f\"Lemmatized: {lemmatized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Complete Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"Complete text preprocessing pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, lowercase=True, remove_stopwords=True, \n",
    "                 lemmatize=True, min_token_length=2):\n",
    "        self.lowercase = lowercase\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatize = lemmatize\n",
    "        self.min_token_length = min_token_length\n",
    "        \n",
    "        if remove_stopwords:\n",
    "            self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        if lemmatize:\n",
    "            self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Preprocess a single text.\"\"\"\n",
    "        # Clean\n",
    "        text = clean_text(text)\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text.lower() if self.lowercase else text)\n",
    "        \n",
    "        # Filter\n",
    "        tokens = [t for t in tokens if len(t) >= self.min_token_length]\n",
    "        \n",
    "        # Remove stopwords\n",
    "        if self.remove_stopwords:\n",
    "            tokens = [t for t in tokens if t not in self.stop_words]\n",
    "        \n",
    "        # Lemmatize\n",
    "        if self.lemmatize:\n",
    "            tokens = [self.lemmatizer.lemmatize(t) for t in tokens]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def preprocess_corpus(self, texts):\n",
    "        \"\"\"Preprocess multiple texts.\"\"\"\n",
    "        return [self.preprocess(text) for text in texts]\n",
    "\n",
    "# Usage\n",
    "preprocessor = TextPreprocessor()\n",
    "processed = preprocessor.preprocess(text)\n",
    "print(f\"Processed tokens: {processed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Text Representation\n",
    "\n",
    "### 2.1 Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"I love machine learning\",\n",
    "    \"Machine learning is amazing\",\n",
    "    \"I love deep learning\",\n",
    "    \"Deep learning uses neural networks\"\n",
    "]\n",
    "\n",
    "# Create BoW\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Display\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=feature_names)\n",
    "print(\"Bag of Words Matrix:\")\n",
    "print(bow_df)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 4))\n",
    "sns.heatmap(bow_df, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Bag of Words Representation')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Documents')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "**Formula:**\n",
    "$$\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)$$\n",
    "\n",
    "Where:\n",
    "- $\\text{TF}(t, d) = \\frac{\\text{count of } t \\text{ in } d}{\\text{total terms in } d}$\n",
    "- $\\text{IDF}(t) = \\log\\left(\\frac{\\text{total documents}}{\\text{documents containing } t}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Display\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(), \n",
    "    columns=tfidf_vectorizer.get_feature_names_out()\n",
    ")\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_df.round(3))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 4))\n",
    "sns.heatmap(tfidf_df, annot=True, fmt='.2f', cmap='YlOrRd', cbar=True)\n",
    "plt.title('TF-IDF Representation')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Documents')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Word Embeddings - Word2Vec from Scratch\n",
    "\n",
    "**Word2Vec:** Learn dense vector representations where similar words have similar vectors.\n",
    "\n",
    "**Two architectures:**\n",
    "1. **CBOW (Continuous Bag of Words):** Predict center word from context\n",
    "2. **Skip-gram:** Predict context words from center word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecSimple:\n",
    "    \"\"\"Simple Word2Vec implementation (Skip-gram).\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim=50, window_size=2, learning_rate=0.01):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.window_size = window_size\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def build_vocab(self, sentences):\n",
    "        \"\"\"Build vocabulary from sentences.\"\"\"\n",
    "        words = [word for sent in sentences for word in sent]\n",
    "        self.vocab = list(set(words))\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "        # Word to index mapping\n",
    "        self.word2idx = {w: i for i, w in enumerate(self.vocab)}\n",
    "        self.idx2word = {i: w for w, i in self.word2idx.items()}\n",
    "        \n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "    \n",
    "    def initialize_embeddings(self):\n",
    "        \"\"\"Initialize embedding matrices.\"\"\"\n",
    "        # Input embeddings (center words)\n",
    "        self.W1 = np.random.randn(self.vocab_size, self.embedding_dim) * 0.01\n",
    "        \n",
    "        # Output embeddings (context words)\n",
    "        self.W2 = np.random.randn(self.embedding_dim, self.vocab_size) * 0.01\n",
    "    \n",
    "    def generate_training_data(self, sentences):\n",
    "        \"\"\"Generate (center, context) pairs.\"\"\"\n",
    "        training_data = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            indices = [self.word2idx[w] for w in sentence]\n",
    "            \n",
    "            for center_idx in range(len(indices)):\n",
    "                center_word = indices[center_idx]\n",
    "                \n",
    "                # Context window\n",
    "                start = max(0, center_idx - self.window_size)\n",
    "                end = min(len(indices), center_idx + self.window_size + 1)\n",
    "                \n",
    "                for context_idx in range(start, end):\n",
    "                    if context_idx != center_idx:\n",
    "                        context_word = indices[context_idx]\n",
    "                        training_data.append((center_word, context_word))\n",
    "        \n",
    "        return training_data\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        \"\"\"Stable softmax.\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x))\n",
    "        return exp_x / exp_x.sum()\n",
    "    \n",
    "    def forward(self, center_word_idx):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        # Get center word embedding\n",
    "        h = self.W1[center_word_idx]  # (embedding_dim,)\n",
    "        \n",
    "        # Compute output scores\n",
    "        u = np.dot(h, self.W2)  # (vocab_size,)\n",
    "        \n",
    "        # Softmax\n",
    "        y_pred = self.softmax(u)\n",
    "        \n",
    "        return h, u, y_pred\n",
    "    \n",
    "    def backward(self, center_word_idx, context_word_idx, h, y_pred):\n",
    "        \"\"\"Backward pass.\"\"\"\n",
    "        # Create target vector\n",
    "        y_true = np.zeros(self.vocab_size)\n",
    "        y_true[context_word_idx] = 1\n",
    "        \n",
    "        # Error\n",
    "        error = y_pred - y_true  # (vocab_size,)\n",
    "        \n",
    "        # Gradients\n",
    "        dW2 = np.outer(h, error)  # (embedding_dim, vocab_size)\n",
    "        dW1 = np.dot(self.W2, error)  # (embedding_dim,)\n",
    "        \n",
    "        # Update\n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.W1[center_word_idx] -= self.learning_rate * dW1\n",
    "        \n",
    "        return error\n",
    "    \n",
    "    def train(self, sentences, epochs=100):\n",
    "        \"\"\"Train Word2Vec model.\"\"\"\n",
    "        self.build_vocab(sentences)\n",
    "        self.initialize_embeddings()\n",
    "        \n",
    "        training_data = self.generate_training_data(sentences)\n",
    "        print(f\"Training samples: {len(training_data)}\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            loss = 0\n",
    "            \n",
    "            for center_idx, context_idx in training_data:\n",
    "                h, u, y_pred = self.forward(center_idx)\n",
    "                error = self.backward(center_idx, context_idx, h, y_pred)\n",
    "                \n",
    "                # Cross-entropy loss\n",
    "                loss += -np.log(y_pred[context_idx] + 1e-10)\n",
    "            \n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                avg_loss = loss / len(training_data)\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    def get_embedding(self, word):\n",
    "        \"\"\"Get embedding for a word.\"\"\"\n",
    "        idx = self.word2idx.get(word)\n",
    "        if idx is not None:\n",
    "            return self.W1[idx]\n",
    "        return None\n",
    "    \n",
    "    def most_similar(self, word, top_n=5):\n",
    "        \"\"\"Find most similar words.\"\"\"\n",
    "        word_vec = self.get_embedding(word)\n",
    "        if word_vec is None:\n",
    "            return []\n",
    "        \n",
    "        # Cosine similarity\n",
    "        similarities = []\n",
    "        for w in self.vocab:\n",
    "            if w != word:\n",
    "                w_vec = self.get_embedding(w)\n",
    "                sim = np.dot(word_vec, w_vec) / (\n",
    "                    np.linalg.norm(word_vec) * np.linalg.norm(w_vec)\n",
    "                )\n",
    "                similarities.append((w, sim))\n",
    "        \n",
    "        # Sort by similarity\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_n]\n",
    "\n",
    "# Train on sample corpus\n",
    "corpus = [\n",
    "    \"i love machine learning\".split(),\n",
    "    \"i love deep learning\".split(),\n",
    "    \"machine learning is great\".split(),\n",
    "    \"deep learning uses neural networks\".split(),\n",
    "    \"neural networks are powerful\".split()\n",
    "]\n",
    "\n",
    "w2v = Word2VecSimple(embedding_dim=10, window_size=2, learning_rate=0.05)\n",
    "w2v.train(corpus, epochs=100)\n",
    "\n",
    "# Test similarity\n",
    "print(\"\\nMost similar to 'learning':\")\n",
    "for word, sim in w2v.most_similar('learning', top_n=5):\n",
    "    print(f\"  {word}: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Sequence Models (RNN, LSTM, GRU)\n",
    "\n",
    "### 3.1 Recurrent Neural Networks (RNN)\n",
    "\n",
    "**Key Idea:** Process sequences by maintaining hidden state\n",
    "\n",
    "**Equations:**\n",
    "$$h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$$\n",
    "$$y_t = W_{hy} h_t + b_y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    class SimpleRNN(nn.Module):\n",
    "        \"\"\"Simple RNN for sequence classification.\"\"\"\n",
    "        \n",
    "        def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "            super().__init__()\n",
    "            \n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "            self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            # x: (batch_size, seq_len)\n",
    "            embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "            \n",
    "            # RNN\n",
    "            output, hidden = self.rnn(embedded)\n",
    "            # output: (batch_size, seq_len, hidden_dim)\n",
    "            # hidden: (1, batch_size, hidden_dim)\n",
    "            \n",
    "            # Use last hidden state\n",
    "            final_hidden = hidden.squeeze(0)  # (batch_size, hidden_dim)\n",
    "            \n",
    "            # Classification\n",
    "            logits = self.fc(final_hidden)  # (batch_size, output_dim)\n",
    "            \n",
    "            return logits\n",
    "    \n",
    "    # Example\n",
    "    rnn = SimpleRNN(vocab_size=1000, embedding_dim=50, hidden_dim=128, output_dim=2)\n",
    "    \n",
    "    # Dummy input\n",
    "    x = torch.randint(0, 1000, (4, 10))  # Batch of 4, seq_len 10\n",
    "    output = rnn(x)\n",
    "    print(f\"RNN output shape: {output.shape}\")  # (4, 2)\n",
    "else:\n",
    "    print(\"PyTorch not available. Install with: pip install torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 LSTM (Long Short-Term Memory)\n",
    "\n",
    "**Problem with RNN:** Vanishing gradients in long sequences\n",
    "\n",
    "**LSTM Solution:** Gating mechanisms to control information flow\n",
    "\n",
    "**Gates:**\n",
    "1. **Forget gate:** What to forget from cell state\n",
    "2. **Input gate:** What new information to add\n",
    "3. **Output gate:** What to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    class SentimentLSTM(nn.Module):\n",
    "        \"\"\"LSTM for sentiment analysis.\"\"\"\n",
    "        \n",
    "        def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                     n_layers=2, dropout=0.5):\n",
    "            super().__init__()\n",
    "            \n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "            self.lstm = nn.LSTM(\n",
    "                embedding_dim, \n",
    "                hidden_dim, \n",
    "                num_layers=n_layers,\n",
    "                dropout=dropout if n_layers > 1 else 0,\n",
    "                batch_first=True\n",
    "            )\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            # Embedding\n",
    "            embedded = self.dropout(self.embedding(x))\n",
    "            \n",
    "            # LSTM\n",
    "            output, (hidden, cell) = self.lstm(embedded)\n",
    "            \n",
    "            # Use last layer's hidden state\n",
    "            final_hidden = hidden[-1]  # (batch_size, hidden_dim)\n",
    "            \n",
    "            # Classification\n",
    "            logits = self.fc(self.dropout(final_hidden))\n",
    "            \n",
    "            return logits\n",
    "    \n",
    "    # Example\n",
    "    lstm_model = SentimentLSTM(\n",
    "        vocab_size=5000, \n",
    "        embedding_dim=100, \n",
    "        hidden_dim=256, \n",
    "        output_dim=2,\n",
    "        n_layers=2,\n",
    "        dropout=0.5\n",
    "    )\n",
    "    \n",
    "    print(lstm_model)\n",
    "    print(f\"\\nTotal parameters: {sum(p.numel() for p in lstm_model.parameters()):,}\")\n",
    "else:\n",
    "    print(\"PyTorch not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Attention Mechanisms\n",
    "\n",
    "### 4.1 Attention Intuition\n",
    "\n",
    "**Problem:** LSTMs compress entire sequence into fixed-size vector\n",
    "\n",
    "**Solution:** Let model \"attend\" to different parts of input\n",
    "\n",
    "**Attention Score:**\n",
    "$$\\alpha_{ij} = \\frac{\\exp(\\text{score}(h_i, h_j))}{\\sum_k \\exp(\\text{score}(h_i, h_k))}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    class Attention(nn.Module):\n",
    "        \"\"\"Attention mechanism.\"\"\"\n",
    "        \n",
    "        def __init__(self, hidden_dim):\n",
    "            super().__init__()\n",
    "            self.attention = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        def forward(self, hidden_states):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                hidden_states: (batch_size, seq_len, hidden_dim)\n",
    "            Returns:\n",
    "                context: (batch_size, hidden_dim)\n",
    "                attention_weights: (batch_size, seq_len)\n",
    "            \"\"\"\n",
    "            # Compute attention scores\n",
    "            scores = self.attention(hidden_states)  # (batch, seq_len, hidden_dim)\n",
    "            \n",
    "            # Attention weights\n",
    "            attention_weights = F.softmax(\n",
    "                scores.sum(dim=-1), dim=-1\n",
    "            )  # (batch, seq_len)\n",
    "            \n",
    "            # Weighted sum\n",
    "            context = torch.bmm(\n",
    "                attention_weights.unsqueeze(1),  # (batch, 1, seq_len)\n",
    "                hidden_states  # (batch, seq_len, hidden_dim)\n",
    "            ).squeeze(1)  # (batch, hidden_dim)\n",
    "            \n",
    "            return context, attention_weights\n",
    "    \n",
    "    # Visualize attention\n",
    "    def visualize_attention(tokens, attention_weights):\n",
    "        \"\"\"Visualize attention weights.\"\"\"\n",
    "        plt.figure(figsize=(10, 2))\n",
    "        plt.bar(range(len(tokens)), attention_weights)\n",
    "        plt.xticks(range(len(tokens)), tokens, rotation=45)\n",
    "        plt.ylabel('Attention Weight')\n",
    "        plt.title('Attention Distribution')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Example\n",
    "    attention_layer = Attention(hidden_dim=128)\n",
    "    hidden_states = torch.randn(1, 8, 128)  # 1 batch, 8 tokens\n",
    "    context, weights = attention_layer(hidden_states)\n",
    "    \n",
    "    tokens = [\"I\", \"love\", \"machine\", \"learning\", \"and\", \"deep\", \"learning\", \"!\"]\n",
    "    visualize_attention(tokens, weights[0].detach().numpy())\n",
    "else:\n",
    "    print(\"PyTorch not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Self-Attention (Scaled Dot-Product Attention)\n",
    "\n",
    "**Key Innovation:** Each position attends to all positions\n",
    "\n",
    "**Formula:**\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- Q: Query matrix\n",
    "- K: Key matrix  \n",
    "- V: Value matrix\n",
    "- $d_k$: Dimension of keys (for scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    class SelfAttention(nn.Module):\n",
    "        \"\"\"Self-attention layer (scaled dot-product).\"\"\"\n",
    "        \n",
    "        def __init__(self, embed_dim):\n",
    "            super().__init__()\n",
    "            self.embed_dim = embed_dim\n",
    "            \n",
    "            # Query, Key, Value projections\n",
    "            self.query = nn.Linear(embed_dim, embed_dim)\n",
    "            self.key = nn.Linear(embed_dim, embed_dim)\n",
    "            self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                x: (batch_size, seq_len, embed_dim)\n",
    "            Returns:\n",
    "                output: (batch_size, seq_len, embed_dim)\n",
    "                attention_weights: (batch_size, seq_len, seq_len)\n",
    "            \"\"\"\n",
    "            # Project to Q, K, V\n",
    "            Q = self.query(x)  # (batch, seq_len, embed_dim)\n",
    "            K = self.key(x)\n",
    "            V = self.value(x)\n",
    "            \n",
    "            # Scaled dot-product attention\n",
    "            scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch, seq_len, seq_len)\n",
    "            scores = scores / np.sqrt(self.embed_dim)  # Scale\n",
    "            \n",
    "            attention_weights = F.softmax(scores, dim=-1)\n",
    "            \n",
    "            # Apply attention to values\n",
    "            output = torch.matmul(attention_weights, V)  # (batch, seq_len, embed_dim)\n",
    "            \n",
    "            return output, attention_weights\n",
    "    \n",
    "    # Example\n",
    "    self_attn = SelfAttention(embed_dim=64)\n",
    "    x = torch.randn(2, 5, 64)  # 2 batches, 5 tokens, 64 dims\n",
    "    output, attn_weights = self_attn(x)\n",
    "    \n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "    \n",
    "    # Visualize attention matrix\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(\n",
    "        attn_weights[0].detach().numpy(), \n",
    "        cmap='Blues', \n",
    "        annot=True, \n",
    "        fmt='.2f'\n",
    "    )\n",
    "    plt.title('Self-Attention Matrix')\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"PyTorch not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Transformer Architecture\n",
    "\n",
    "### 5.1 Multi-Head Attention\n",
    "\n",
    "**Idea:** Run multiple attention operations in parallel\n",
    "\n",
    "**Benefits:**\n",
    "- Learn different types of relationships\n",
    "- More expressiveness\n",
    "- Better generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    class MultiHeadAttention(nn.Module):\n",
    "        \"\"\"Multi-head attention.\"\"\"\n",
    "        \n",
    "        def __init__(self, embed_dim, num_heads):\n",
    "            super().__init__()\n",
    "            assert embed_dim % num_heads == 0\n",
    "            \n",
    "            self.embed_dim = embed_dim\n",
    "            self.num_heads = num_heads\n",
    "            self.head_dim = embed_dim // num_heads\n",
    "            \n",
    "            # Projections for all heads (batched)\n",
    "            self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "            self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            batch_size, seq_len, embed_dim = x.shape\n",
    "            \n",
    "            # Project and split into Q, K, V\n",
    "            qkv = self.qkv_proj(x)  # (batch, seq_len, 3*embed_dim)\n",
    "            qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
    "            qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, batch, heads, seq_len, head_dim)\n",
    "            q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "            \n",
    "            # Scaled dot-product attention\n",
    "            scores = torch.matmul(q, k.transpose(-2, -1))  # (batch, heads, seq_len, seq_len)\n",
    "            scores = scores / np.sqrt(self.head_dim)\n",
    "            attention = F.softmax(scores, dim=-1)\n",
    "            \n",
    "            # Apply attention\n",
    "            out = torch.matmul(attention, v)  # (batch, heads, seq_len, head_dim)\n",
    "            \n",
    "            # Concatenate heads\n",
    "            out = out.transpose(1, 2).contiguous()  # (batch, seq_len, heads, head_dim)\n",
    "            out = out.reshape(batch_size, seq_len, embed_dim)\n",
    "            \n",
    "            # Final projection\n",
    "            out = self.out_proj(out)\n",
    "            \n",
    "            return out, attention\n",
    "    \n",
    "    # Example\n",
    "    mha = MultiHeadAttention(embed_dim=128, num_heads=8)\n",
    "    x = torch.randn(2, 10, 128)\n",
    "    output, attention = mha(x)\n",
    "    \n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Attention shape: {attention.shape}\")\n",
    "else:\n",
    "    print(\"PyTorch not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Complete Transformer Encoder\n",
    "\n",
    "**Components:**\n",
    "1. Multi-head self-attention\n",
    "2. Feed-forward network\n",
    "3. Layer normalization\n",
    "4. Residual connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    class TransformerEncoderLayer(nn.Module):\n",
    "        \"\"\"Single Transformer encoder layer.\"\"\"\n",
    "        \n",
    "        def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "            super().__init__()\n",
    "            \n",
    "            # Multi-head attention\n",
    "            self.self_attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "            \n",
    "            # Feed-forward network\n",
    "            self.ff = nn.Sequential(\n",
    "                nn.Linear(embed_dim, ff_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(ff_dim, embed_dim)\n",
    "            )\n",
    "            \n",
    "            # Layer normalization\n",
    "            self.norm1 = nn.LayerNorm(embed_dim)\n",
    "            self.norm2 = nn.LayerNorm(embed_dim)\n",
    "            \n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            # Multi-head attention with residual\n",
    "            attn_out, _ = self.self_attn(x)\n",
    "            x = self.norm1(x + self.dropout(attn_out))\n",
    "            \n",
    "            # Feed-forward with residual\n",
    "            ff_out = self.ff(x)\n",
    "            x = self.norm2(x + self.dropout(ff_out))\n",
    "            \n",
    "            return x\n",
    "    \n",
    "    class TransformerEncoder(nn.Module):\n",
    "        \"\"\"Complete Transformer encoder for classification.\"\"\"\n",
    "        \n",
    "        def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, \n",
    "                     num_layers, max_seq_len, num_classes, dropout=0.1):\n",
    "            super().__init__()\n",
    "            \n",
    "            # Token embedding\n",
    "            self.token_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "            \n",
    "            # Positional embedding\n",
    "            self.pos_embed = nn.Embedding(max_seq_len, embed_dim)\n",
    "            \n",
    "            # Encoder layers\n",
    "            self.layers = nn.ModuleList([\n",
    "                TransformerEncoderLayer(embed_dim, num_heads, ff_dim, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ])\n",
    "            \n",
    "            # Classification head\n",
    "            self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "            \n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            batch_size, seq_len = x.shape\n",
    "            \n",
    "            # Token embeddings\n",
    "            token_emb = self.token_embed(x)  # (batch, seq_len, embed_dim)\n",
    "            \n",
    "            # Positional embeddings\n",
    "            positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n",
    "            pos_emb = self.pos_embed(positions)  # (1, seq_len, embed_dim)\n",
    "            \n",
    "            # Combine\n",
    "            x = self.dropout(token_emb + pos_emb)\n",
    "            \n",
    "            # Pass through encoder layers\n",
    "            for layer in self.layers:\n",
    "                x = layer(x)\n",
    "            \n",
    "            # Global average pooling\n",
    "            x = x.mean(dim=1)  # (batch, embed_dim)\n",
    "            \n",
    "            # Classification\n",
    "            logits = self.classifier(x)\n",
    "            \n",
    "            return logits\n",
    "    \n",
    "    # Example\n",
    "    transformer = TransformerEncoder(\n",
    "        vocab_size=5000,\n",
    "        embed_dim=128,\n",
    "        num_heads=8,\n",
    "        ff_dim=512,\n",
    "        num_layers=4,\n",
    "        max_seq_len=100,\n",
    "        num_classes=2,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    \n",
    "    print(transformer)\n",
    "    print(f\"\\nTotal parameters: {sum(p.numel() for p in transformer.parameters()):,}\")\n",
    "    \n",
    "    # Test\n",
    "    x = torch.randint(0, 5000, (4, 50))  # 4 sequences of length 50\n",
    "    output = transformer(x)\n",
    "    print(f\"\\nOutput shape: {output.shape}\")  # (4, 2)\n",
    "else:\n",
    "    print(\"PyTorch not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Practical NLP Application\n",
    "\n",
    "### 6.1 Sentiment Analysis with Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using transformers library\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "    \n",
    "    # Load sentiment analysis pipeline\n",
    "    sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "    \n",
    "    # Test sentences\n",
    "    sentences = [\n",
    "        \"I love this product! It's amazing!\",\n",
    "        \"This is terrible. I hate it.\",\n",
    "        \"It's okay, nothing special.\",\n",
    "        \"Absolutely fantastic! Best purchase ever!\"\n",
    "    ]\n",
    "    \n",
    "    results = sentiment_analyzer(sentences)\n",
    "    \n",
    "    for sent, result in zip(sentences, results):\n",
    "        print(f\"Text: {sent}\")\n",
    "        print(f\"Sentiment: {result['label']} (confidence: {result['score']:.3f})\\n\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Install transformers: pip install transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    \n",
    "    # Generate text\n",
    "    prompt = \"Machine learning is\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=50,\n",
    "        num_return_sequences=3,\n",
    "        temperature=0.8,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    for i, seq in enumerate(output, 1):\n",
    "        text = tokenizer.decode(seq, skip_special_tokens=True)\n",
    "        print(f\"Generation {i}: {text}\\n\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Install transformers: pip install transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Text Preprocessing:**\n",
    "   - Tokenization (word, sentence, subword)\n",
    "   - Cleaning and normalization\n",
    "   - Stopword removal, stemming, lemmatization\n",
    "\n",
    "2. **Text Representation:**\n",
    "   - Bag of Words (BoW)\n",
    "   - TF-IDF\n",
    "   - Word embeddings (Word2Vec, GloVe)\n",
    "\n",
    "3. **Sequence Models:**\n",
    "   - RNN: Process sequences sequentially\n",
    "   - LSTM: Solve vanishing gradient problem\n",
    "   - GRU: Lighter alternative to LSTM\n",
    "\n",
    "4. **Attention:**\n",
    "   - Focus on relevant parts of input\n",
    "   - Self-attention: Each token attends to all tokens\n",
    "   - Multi-head attention: Multiple attention patterns\n",
    "\n",
    "5. **Transformers:**\n",
    "   - Pure attention-based architecture\n",
    "   - Parallel processing (faster than RNNs)\n",
    "   - State-of-the-art for most NLP tasks\n",
    "\n",
    "### Interview Questions\n",
    "\n",
    "1. **What is the difference between stemming and lemmatization?**\n",
    "   - Stemming: Crude rule-based truncation (\"running\" ‚Üí \"run\")\n",
    "   - Lemmatization: Dictionary-based, considers context (\"better\" ‚Üí \"good\")\n",
    "\n",
    "2. **Why is TF-IDF better than Bag of Words?**\n",
    "   - Downweights common words (\"the\", \"and\")\n",
    "   - Upweights rare, informative words\n",
    "   - Better feature representation\n",
    "\n",
    "3. **What problem does LSTM solve compared to RNN?**\n",
    "   - Vanishing gradient problem\n",
    "   - Can learn long-range dependencies\n",
    "   - Gating mechanisms control information flow\n",
    "\n",
    "4. **How does attention help in NLP?**\n",
    "   - Focuses on relevant parts of input\n",
    "   - No fixed-size bottleneck\n",
    "   - Interpretable (can visualize attention weights)\n",
    "\n",
    "5. **Why are Transformers faster than LSTMs?**\n",
    "   - Parallel processing (no sequential dependency)\n",
    "   - Better GPU utilization\n",
    "   - Can process all tokens simultaneously\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Advanced NLP:** See [ADVANCED_NLP_TECHNIQUES.md](../ADVANCED_NLP_TECHNIQUES.md)\n",
    "- **Modern Techniques:** See [MODERN_ML_AI_TECHNIQUES_2024_2025.md](../MODERN_ML_AI_TECHNIQUES_2024_2025.md)\n",
    "- **Projects:** Build sentiment analysis, text classification, or chatbot\n",
    "- **Fine-tuning:** Learn to fine-tune BERT, GPT for specific tasks\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've completed NLP Fundamentals. You now understand text processing, embeddings, sequence models, attention, and Transformers!\n",
    "\n",
    "**Next:** [10 - Computer Vision with CNNs](./10_computer_vision.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
