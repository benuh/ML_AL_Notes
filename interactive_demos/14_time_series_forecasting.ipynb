{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "---\n\n## 7. Interview Questions\n\n### Conceptual Questions\n\n**Q1: What is stationarity and why is it important in time series analysis?**\n\n**Answer**: A time series is stationary if its statistical properties (mean, variance, autocorrelation) remain constant over time. Specifically, weak stationarity requires:\n1. Constant mean: $E[y_t] = \\mu$\n2. Constant variance: $Var(y_t) = \\sigma^2$\n3. Autocorrelation depends only on lag, not time: $Cov(y_t, y_{t-k})$ is function of $k$ only\n\nStationarity is important because:\n- Most statistical forecasting methods (ARIMA, etc.) assume stationarity\n- Makes the data easier to model and understand\n- Non-stationary series can lead to spurious correlations\n- Enables us to use historical statistics for future predictions\n\n---\n\n**Q2: Explain the difference between AR, MA, and ARMA models.**\n\n**Answer**:\n- **AR(p) - AutoRegressive**: Current value depends on past values\n  - $y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\cdots + \\phi_p y_{t-p} + \\epsilon_t$\n  - PACF cuts off after lag $p$\n  - Good when current value is correlated with recent past values\n\n- **MA(q) - Moving Average**: Current value depends on past forecast errors\n  - $y_t = c + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\cdots + \\theta_q \\epsilon_{t-q}$\n  - ACF cuts off after lag $q$\n  - Good for modeling \\\"shock\\\" effects that persist for limited time\n\n- **ARMA(p,q)**: Combines both AR and MA components\n  - Captures both value dependencies and error dependencies\n  - More flexible than pure AR or MA models\n\n---\n\n**Q3: Why can't we use standard k-fold cross-validation for time series?**\n\n**Answer**: Standard k-fold CV randomly splits data into folds, which:\n1. **Violates temporal order**: Future data could end up in training set\n2. **Causes data leakage**: Model could \\\"see\\\" future information\n3. **Unrealistic evaluation**: In practice, we only have past data to predict future\n\nInstead, use **time series cross-validation** (walk-forward validation):\n- Training set always comes before test set\n- Maintains temporal order\n- Mimics real-world forecasting scenario\n\n---\n\n**Q4: Compare LSTM vs Transformer for time series forecasting.**\n\n**Answer**:\n\n**LSTM:**\n- ✓ Designed specifically for sequences\n- ✓ Maintains cell state for long-term memory\n- ✓ Works well with smaller datasets\n- ✗ Sequential processing (slow)\n- ✗ Vanishing gradient for very long sequences\n- ✗ Hidden state size limits information retention\n\n**Transformer:**\n- ✓ Parallel processing (fast)\n- ✓ Attention captures any-range dependencies directly\n- ✓ No vanishing gradient issues\n- ✓ Interpretable via attention weights\n- ✗ Requires more data\n- ✗ Higher memory usage\n- ✗ Needs positional encoding\n\n**When to use which:**\n- LSTM: Small datasets, shorter sequences, limited compute\n- Transformer: Large datasets, long sequences, when speed and interpretability matter\n\n---\n\n**Q5: What is the difference between additive and multiplicative seasonality?**\n\n**Answer**:\n\n**Additive**: $y_t = T_t + S_t + R_t$\n- Seasonal variation is constant (same amplitude)\n- Use when seasonal fluctuations don't change with level\n- Example: Temperature variations stay roughly same magnitude year-round\n\n**Multiplicative**: $y_t = T_t \\\\times S_t \\\\times R_t$\n- Seasonal variation increases/decreases with level\n- Use when seasonal fluctuations grow with the trend\n- Example: Retail sales - holiday spike is 20% of current level, so grows as business grows\n\n**How to choose:**\n- Plot the data and observe if seasonal swings grow with level\n- If yes → multiplicative\n- If no → additive\n- Can apply log transform to convert multiplicative to additive\n\n---\n\n**Q6: Explain how SARIMA extends ARIMA to handle seasonality.**\n\n**Answer**:\n\n**ARIMA(p,d,q)** has three components:\n- $p$: AR order\n- $d$: differencing order\n- $q$: MA order\n\n**SARIMA(p,d,q)(P,D,Q)s** adds seasonal components:\n- $P$: seasonal AR order\n- $D$: seasonal differencing order\n- $Q$: seasonal MA order\n- $s$: seasonal period (12 for monthly with annual pattern)\n\nExample: SARIMA(1,1,1)(1,1,1)₁₂\n- Non-seasonal: AR(1), 1 difference, MA(1)\n- Seasonal: SAR(1), 1 seasonal difference, SMA(1), period=12\n\nThe model applies:\n1. Regular differencing $d$ times\n2. Seasonal differencing $D$ times (at lag $s$)\n3. AR and MA terms at both regular and seasonal lags\n\n---\n\n**Q7: What are the advantages of using deep learning for time series vs traditional methods?**\n\n**Answer**:\n\n**Advantages of Deep Learning:**\n1. **Non-linear patterns**: Can capture complex, non-linear relationships\n2. **Multivariate**: Naturally handles multiple input features\n3. **Automatic feature extraction**: Learns relevant features from raw data\n4. **Scalability**: Parallelizable, handles large datasets efficiently\n5. **Transfer learning**: Pre-trained models can be fine-tuned\n6. **Multiple horizons**: Can forecast multiple steps simultaneously\n\n**Advantages of Traditional Methods:**\n1. **Interpretability**: Clear mathematical formulation\n2. **Small data**: Works well with limited historical data\n3. **Uncertainty quantification**: Confidence intervals are straightforward\n4. **Computational efficiency**: Faster training, no GPU needed\n5. **Theoretical guarantees**: Well-studied statistical properties\n6. **Debugging**: Easier to diagnose what's wrong\n\n**Best practice**: Start with traditional methods (SARIMA) as baseline, then try deep learning if:\n- You have large datasets (1000+ samples)\n- Patterns are complex/non-linear\n- Multiple input features available\n- Computational resources available\n\n---\n\n**Q8: How would you detect and handle outliers in time series data?**\n\n**Answer**:\n\n**Detection methods:**\n\n1. **Statistical methods:**\n   - Z-score: Points beyond 3 standard deviations\n   - IQR method: Points beyond Q1 - 1.5×IQR or Q3 + 1.5×IQR\n   - Moving average residuals: Large deviations from moving average\n\n2. **Model-based:**\n   - Fit ARIMA, identify points with large residuals\n   - Seasonal decomposition: outliers in residual component\n   - LSTM reconstruction: High reconstruction error\n\n3. **Domain-specific:**\n   - Business logic (e.g., negative sales impossible)\n   - Known events (Black Friday spike is expected, not outlier)\n\n**Handling strategies:**\n\n1. **Remove**: If clearly erroneous (measurement error)\n2. **Impute**: Replace with interpolated value\n   - Linear interpolation\n   - Seasonal average\n   - Model-based prediction\n3. **Cap**: Limit to maximum reasonable value\n4. **Model separately**: Add binary indicator variable\n5. **Robust methods**: Use models resistant to outliers (e.g., MAE loss)\n\n**Important**: Document all outlier handling decisions!\n\n---\n\n### Coding Questions\n\n**Q9: Implement a function to create sliding window sequences from a time series.**\n\n```python\ndef create_sequences(data, window_size, horizon=1):\n    \\\"\\\"\\\"\n    Create input-output sequences using sliding window.\n    \n    Args:\n        data: 1D array of time series values\n        window_size: Number of past observations for input\n        horizon: Number of future steps to predict\n    \n    Returns:\n        X: Array of shape (n_samples, window_size)\n        y: Array of shape (n_samples, horizon)\n    \\\"\\\"\\\"\n    X, y = [], []\n    \n    for i in range(len(data) - window_size - horizon + 1):\n        X.append(data[i:i + window_size])\n        y.append(data[i + window_size:i + window_size + horizon])\n    \n    return np.array(X), np.array(y)\n\n# Test\ndata = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\nX, y = create_sequences(data, window_size=3, horizon=2)\nprint(\\\"X:\\\", X)\nprint(\\\"y:\\\", y)\n```\n\n---\n\n**Q10: How would you implement walk-forward validation for time series?**\n\n```python\ndef walk_forward_validation(data, model_fn, window_size=100, test_size=50):\n    \\\"\\\"\\\"\n    Perform walk-forward validation.\n    \n    Args:\n        data: Time series data\n        model_fn: Function that fits model and returns predictions\n        window_size: Minimum training window size\n        test_size: Size of each test set\n    \n    Returns:\n        List of (predictions, actuals) tuples\n    \\\"\\\"\\\"\n    results = []\n    \n    for i in range(window_size, len(data) - test_size + 1, test_size):\n        # Split data\n        train = data[:i]\n        test = data[i:i + test_size]\n        \n        # Fit model and predict\n        predictions = model_fn(train, len(test))\n        \n        # Store results\n        results.append((predictions, test))\n    \n    return results\n\n# Example usage\ndef simple_model(train, n_steps):\n    # Simple: forecast mean of last 10 values\n    return np.full(n_steps, train[-10:].mean())\n\nresults = walk_forward_validation(ts_full.values, simple_model)\nprint(f\\\"Number of folds: {len(results)}\\\")\n```\n\n---\n\n## Summary\n\nThis notebook covered:\n\n✅ **Time Series Fundamentals**: Components, stationarity, ACF/PACF, decomposition\n✅ **Classical Methods**: Moving averages, exponential smoothing, ARIMA, SARIMA\n✅ **Deep Learning**: LSTM, 1D CNN, Transformers\n✅ **Best Practices**: Cross-validation, evaluation metrics, production considerations\n\n**Key Takeaways:**\n\n1. **Always check for stationarity** before applying statistical methods\n2. **Start simple**: Baseline models (naive, moving average) establish minimum performance\n3. **Classical methods often work well** with proper parameter tuning\n4. **Deep learning shines** with large datasets and complex patterns\n5. **Proper validation is critical**: Use time series split, not random k-fold\n6. **Multiple metrics**: MAE, RMSE, MAPE tell different stories\n7. **Domain knowledge**: Understanding the data source is as important as the model\n\n**Next Steps:**\n\n- Try these methods on real-world datasets (stock prices, weather, sales)\n- Experiment with hyperparameter tuning\n- Implement ensemble methods (combining multiple models)\n- Explore multivariate time series forecasting\n- Study probabilistic forecasting (prediction intervals)\n\n---\n\n**Happy Forecasting! 📈**\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def calculate_all_metrics(y_true, y_pred):\n    \"\"\"\n    Calculate all common time series forecasting metrics.\n    \"\"\"\n    # MAE\n    mae = mean_absolute_error(y_true, y_pred)\n    \n    # RMSE\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    \n    # MAPE (avoid division by zero)\n    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-10))) * 100\n    \n    # sMAPE\n    smape = np.mean(2.0 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-10)) * 100\n    \n    # MASE (using naive forecast as baseline)\n    naive_forecast_mae = np.mean(np.abs(np.diff(y_true)))\n    mase = mae / (naive_forecast_mae + 1e-10)\n    \n    # R-squared\n    r2 = r2_score(y_true, y_pred)\n    \n    return {\n        'MAE': mae,\n        'RMSE': rmse,\n        'MAPE': mape,\n        'sMAPE': smape,\n        'MASE': mase,\n        'R²': r2\n    }\n\n# Calculate metrics for best model\nbest_model_name = model_metrics_sorted[0][0]\nbest_forecast = models_comparison[best_model_name][0]\n\nmetrics = calculate_all_metrics(test.values, best_forecast)\n\nprint(f\\\"\\\\nDetailed Metrics for Best Model ({best_model_name}):\\\")\nprint(\\\"=\\\"*50)\nfor metric, value in metrics.items():\n    if metric == 'R²':\n        print(f\\\"{metric:<15} {value:.4f}\\\")\n    elif metric in ['MAPE', 'sMAPE']:\n        print(f\\\"{metric:<15} {value:.2f}%\\\")\n    else:\n        print(f\\\"{metric:<15} {value:.4f}\\\")\nprint(\\\"=\\\"*50)\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 6.2 Evaluation Metrics for Time Series\n\nDifferent metrics capture different aspects of forecast quality:\n\n**1. Mean Absolute Error (MAE):**\n$$\\text{MAE} = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|$$\n- **Pros**: Easy to interpret, same units as data, robust to outliers\n- **Cons**: Doesn't penalize large errors heavily\n\n**2. Root Mean Squared Error (RMSE):**\n$$\\text{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}$$\n- **Pros**: Penalizes large errors, same units as data\n- **Cons**: Sensitive to outliers\n\n**3. Mean Absolute Percentage Error (MAPE):**\n$$\\text{MAPE} = \\frac{100\\%}{n}\\sum_{i=1}^{n}\\left|\\frac{y_i - \\hat{y}_i}{y_i}\\right|$$\n- **Pros**: Scale-independent, easy to interpret (percentage)\n- **Cons**: Undefined when $y_i = 0$, asymmetric (penalizes over-forecasts more)\n\n**4. Symmetric MAPE (sMAPE):**\n$$\\text{sMAPE} = \\frac{100\\%}{n}\\sum_{i=1}^{n}\\frac{|y_i - \\hat{y}_i|}{(|y_i| + |\\hat{y}_i|)/2}$$\n- **Pros**: Symmetric, bounded between 0% and 200%\n\n**5. Mean Absolute Scaled Error (MASE):**\n$$\\text{MASE} = \\frac{\\text{MAE}}{\\frac{1}{n-1}\\sum_{i=2}^{n}|y_i - y_{i-1}|}$$\n- **Pros**: Scale-independent, works for zero values, symmetric\n- **Cons**: Less intuitive\n\n**When to use which:**\n- **MAE**: General purpose, when all errors are equally important\n- **RMSE**: When large errors are particularly undesirable\n- **MAPE**: When relative errors matter (e.g., financial forecasting)\n- **MASE**: When comparing across different time series\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.model_selection import TimeSeriesSplit\n\n# Demonstrate time series cross-validation\nn_splits = 5\ntscv = TimeSeriesSplit(n_splits=n_splits)\n\nplt.figure(figsize=(15, 8))\n\nfor i, (train_index, test_index) in enumerate(tscv.split(ts_full)):\n    # Get train and test data\n    train_fold = ts_full.iloc[train_index]\n    test_fold = ts_full.iloc[test_index]\n    \n    # Plot\n    plt.subplot(n_splits, 1, i+1)\n    plt.plot(train_fold.index, np.ones(len(train_fold)) * (i+1), 'b-', linewidth=8, label='Train' if i == 0 else '')\n    plt.plot(test_fold.index, np.ones(len(test_fold)) * (i+1), 'r-', linewidth=8, label='Test' if i == 0 else '')\n    plt.ylim(0.5, 1.5)\n    plt.yticks([])\n    plt.ylabel(f'Fold {i+1}', rotation=0, labelpad=20)\n    \n    if i == 0:\n        plt.legend(loc='upper right')\n    if i == n_splits - 1:\n        plt.xlabel('Time')\n\nplt.suptitle('Time Series Cross-Validation (Walk-Forward)', fontsize=14, fontweight='bold', y=0.995)\nplt.tight_layout()\nplt.show()\n\nprint(f\\\"Number of splits: {n_splits}\\\")\nprint(f\\\"Total data points: {len(ts_full)}\\\")\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 6. Best Practices & Production\n\n### 6.1 Time Series Cross-Validation\n\n**Standard cross-validation does NOT work for time series** because it violates temporal order.\n\n**Time Series Split (Walk-Forward Validation):**\n```\nTraining Set 1   | Test Set 1\nTraining Set 1 + Test Set 1   | Test Set 2\nTraining Set 1 + Test Set 1 + Test Set 2   | Test Set 3\n...\n```\n\nThis maintains temporal order and prevents data leakage.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Compare all models\nmodels_comparison = {\n    'Simple ES': (ses_forecast, mean_absolute_error(test.values, ses_forecast)),\n    'Double ES': (des_forecast, mean_absolute_error(test.values, des_forecast)),\n    'Triple ES': (tes_forecast, mean_absolute_error(test.values, tes_forecast)),\n    'ARIMA': (arima_forecast.values, arima_mae),\n    'SARIMA': (sarima_forecast.values, sarima_mae),\n    'LSTM': (lstm_forecast, lstm_mae),\n    '1D CNN': (cnn_forecast, cnn_mae),\n    'Transformer': (transformer_forecast, transformer_mae)\n}\n\n# Plot all forecasts\nplt.figure(figsize=(18, 10))\n\n# Top plot: All forecasts\nplt.subplot(2, 1, 1)\nplt.plot(train.index[-100:], train.values[-100:], label='Train (last 100)', alpha=0.5, color='gray')\nplt.plot(test.index, test.values, label='Actual', color='black', linewidth=3, alpha=0.8)\n\ncolors = plt.cm.tab10(np.linspace(0, 1, len(models_comparison)))\nfor (name, (forecast, _)), color in zip(models_comparison.items(), colors):\n    plt.plot(test.index, forecast, label=name, linestyle='--', alpha=0.7, color=color)\n\nplt.axvline(test.index[0], color='red', linestyle=':', alpha=0.3)\nplt.title('All Models Comparison - Forecasts', fontsize=14, fontweight='bold')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.legend(loc='upper left', ncol=3)\nplt.grid(True, alpha=0.3)\n\n# Bottom plot: MAE comparison\nplt.subplot(2, 1, 2)\nmodel_names = list(models_comparison.keys())\nmaes = [models_comparison[name][1] for name in model_names]\n\nbars = plt.bar(model_names, maes, color=colors, alpha=0.7, edgecolor='black')\nplt.xlabel('Model')\nplt.ylabel('MAE')\nplt.title('Model Performance Comparison (MAE - Lower is Better)', fontsize=14, fontweight='bold')\nplt.xticks(rotation=45, ha='right')\nplt.grid(True, alpha=0.3, axis='y')\n\n# Add value labels on bars\nfor bar, mae in zip(bars, maes):\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2., height,\n             f'{mae:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n# Print detailed comparison table\nprint(\\\"\\\\n\\\" + \\\"=\\\"*80)\nprint(\\\" \\\" * 25 + \\\"MODEL PERFORMANCE COMPARISON\\\")\nprint(\\\"=\\\"*80)\nprint(f\\\"{'Model':<20} {'MAE':<12} {'RMSE':<12} {'Rank':<10}\\\")\\\nprint(\\\"-\\\"*80)\n\n# Calculate RMSE for all models\nmodel_metrics = []\nfor name, (forecast, mae) in models_comparison.items():\n    rmse = np.sqrt(mean_squared_error(test.values, forecast))\n    model_metrics.append((name, mae, rmse))\n\n# Sort by MAE\nmodel_metrics_sorted = sorted(model_metrics, key=lambda x: x[1])\n\nfor rank, (name, mae, rmse) in enumerate(model_metrics_sorted, 1):\n    marker = \\\" ← BEST\\\" if rank == 1 else \\\"\\\"\n    print(f\\\"{name:<20} {mae:<12.4f} {rmse:<12.4f} {rank:<10}{marker}\\\")\n\nprint(\\\"=\\\"*80)\n\n# Statistical summary\nprint(f\\\"\\\\nBest Model: {model_metrics_sorted[0][0]}\\\")\nprint(f\\\"Worst Model: {model_metrics_sorted[-1][0]}\\\")\nimprovement = ((model_metrics_sorted[-1][1] - model_metrics_sorted[0][1]) / model_metrics_sorted[-1][1] * 100)\nprint(f\\\"Improvement from worst to best: {improvement:.2f}%\\\")\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4.5 Model Comparison\n\nLet's compare all the models we've trained on the same test set.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class PositionalEncoding(nn.Module):\n    \"\"\"\n    Positional encoding for Transformer to inject sequence order information.\n    \"\"\"\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        \n        # Create positional encoding matrix\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n        self.register_buffer('pe', pe)\n    \n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Tensor of shape (batch_size, seq_len, d_model)\n        \"\"\"\n        return x + self.pe[:, :x.size(1), :]\n\nclass TransformerForecaster(nn.Module):\n    \"\"\"\n    Transformer model for time series forecasting.\n    \"\"\"\n    def __init__(self, input_size=1, d_model=64, nhead=4, num_layers=2, \n                 dim_feedforward=256, dropout=0.1, output_size=1):\n        \"\"\"\n        Args:\n            input_size: Number of input features\n            d_model: Dimension of model\n            nhead: Number of attention heads\n            num_layers: Number of transformer layers\n            dim_feedforward: Dimension of feedforward network\n            dropout: Dropout probability\n            output_size: Number of output values\n        \"\"\"\n        super(TransformerForecaster, self).__init__()\n        \n        # Input projection\n        self.input_projection = nn.Linear(input_size, d_model)\n        \n        # Positional encoding\n        self.pos_encoder = PositionalEncoding(d_model)\n        \n        # Transformer encoder\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True\n        )\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        \n        # Output layers\n        self.fc = nn.Linear(d_model, output_size)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, seq_len, input_size)\n        \n        Returns:\n            Output tensor of shape (batch_size, output_size)\n        \"\"\"\n        # Project input to d_model dimension\n        x = self.input_projection(x)  # (batch, seq_len, d_model)\n        \n        # Add positional encoding\n        x = self.pos_encoder(x)\n        \n        # Transformer encoding\n        x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n        \n        # Use the last time step for prediction\n        x = x[:, -1, :]  # (batch, d_model)\n        \n        # Output projection\n        x = self.dropout(x)\n        x = self.fc(x)  # (batch, output_size)\n        \n        return x\n\n# Initialize Transformer model\ntransformer_model = TransformerForecaster(\n    input_size=1,\n    d_model=64,\n    nhead=4,\n    num_layers=2,\n    dim_feedforward=256,\n    dropout=0.1,\n    output_size=horizon\n).to(device)\n\noptimizer_transformer = optim.Adam(transformer_model.parameters(), lr=0.001)\nscheduler_transformer = optim.lr_scheduler.ReduceLROnPlateau(optimizer_transformer, mode='min', factor=0.5, patience=5)\n\nprint(transformer_model)\nprint(f\"\\\\nTotal parameters: {sum(p.numel() for p in transformer_model.parameters()):,}\")\n\n# Train Transformer model\nprint(\"\\\\nTraining Transformer model...\")\ntrain_losses_transformer, val_losses_transformer = train_model(\n    transformer_model, train_loader, val_loader,\n    criterion, optimizer_transformer, scheduler_transformer, epochs=100\n)\n\n# Forecast with Transformer\ntransformer_forecast = forecast_lstm(transformer_model, last_train_window, len(test), scaler)\n\n# Calculate errors\ntransformer_mae = mean_absolute_error(test.values, transformer_forecast)\ntransformer_rmse = np.sqrt(mean_squared_error(test.values, transformer_forecast))\n\nprint(f\"\\\\nTransformer Forecast Accuracy:\")\nprint(f\"MAE:  {transformer_mae:.4f}\")\nprint(f\"RMSE: {transformer_rmse:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4.4 Transformer for Time Series\n\n**Transformers** use self-attention mechanisms to capture dependencies regardless of distance.\n\n**Self-Attention Mechanism:**\n\n$$\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n$$\n\nwhere:\n- $Q$ = Query matrix\n- $K$ = Key matrix\n- $V$ = Value matrix\n- $d_k$ = Dimension of key vectors\n\n**Multi-Head Attention:**\n$$\n\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O\n$$\n$$\n\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n$$\n\n**Advantages for Time Series:**\n- Captures long-range dependencies efficiently\n- Parallel processing (unlike RNNs)\n- Attention weights provide interpretability\n- Positional encoding maintains temporal order",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class CNN1DForecaster(nn.Module):\n    \"\"\"\n    1D CNN model for time series forecasting.\n    \"\"\"\n    def __init__(self, input_size=1, num_filters=64, kernel_size=3, output_size=1):\n        \"\"\"\n        Args:\n            input_size: Number of features per time step\n            num_filters: Number of filters in conv layers\n            kernel_size: Size of convolutional kernel\n            output_size: Number of output values\n        \"\"\"\n        super(CNN1DForecaster, self).__init__()\n        \n        # Convolutional layers\n        self.conv1 = nn.Conv1d(input_size, num_filters, kernel_size, padding=kernel_size//2)\n        self.conv2 = nn.Conv1d(num_filters, num_filters*2, kernel_size, padding=kernel_size//2)\n        self.conv3 = nn.Conv1d(num_filters*2, num_filters*4, kernel_size, padding=kernel_size//2)\n        \n        # Pooling and activation\n        self.pool = nn.MaxPool1d(2)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.3)\n        \n        # Global average pooling\n        self.global_pool = nn.AdaptiveAvgPool1d(1)\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(num_filters*4, 128)\n        self.fc2 = nn.Linear(128, output_size)\n    \n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, seq_len, input_size)\n        \n        Returns:\n            Output tensor of shape (batch_size, output_size)\n        \"\"\"\n        # Transpose for Conv1d: (batch, features, seq_len)\n        x = x.transpose(1, 2)\n        \n        # Conv blocks\n        x = self.relu(self.conv1(x))\n        x = self.pool(x)\n        x = self.dropout(x)\n        \n        x = self.relu(self.conv2(x))\n        x = self.pool(x)\n        x = self.dropout(x)\n        \n        x = self.relu(self.conv3(x))\n        \n        # Global pooling\n        x = self.global_pool(x)  # (batch, channels, 1)\n        x = x.squeeze(-1)  # (batch, channels)\n        \n        # Fully connected\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        \n        return x\n\n# Initialize CNN model\ncnn_model = CNN1DForecaster(\n    input_size=1,\n    num_filters=64,\n    kernel_size=5,\n    output_size=horizon\n).to(device)\n\noptimizer_cnn = optim.Adam(cnn_model.parameters(), lr=0.001)\nscheduler_cnn = optim.lr_scheduler.ReduceLROnPlateau(optimizer_cnn, mode='min', factor=0.5, patience=5)\n\nprint(cnn_model)\nprint(f\"\\\\nTotal parameters: {sum(p.numel() for p in cnn_model.parameters()):,}\")\n\n# Train CNN model\nprint(\"\\\\nTraining 1D CNN model...\")\ntrain_losses_cnn, val_losses_cnn = train_model(\n    cnn_model, train_loader, val_loader,\n    criterion, optimizer_cnn, scheduler_cnn, epochs=100\n)\n\n# Forecast with CNN\ncnn_forecast = forecast_lstm(cnn_model, last_train_window, len(test), scaler)\n\n# Calculate errors\ncnn_mae = mean_absolute_error(test.values, cnn_forecast)\ncnn_rmse = np.sqrt(mean_squared_error(test.values, cnn_forecast))\n\nprint(f\"\\\\n1D CNN Forecast Accuracy:\")\nprint(f\"MAE:  {cnn_mae:.4f}\")\nprint(f\"RMSE: {cnn_rmse:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4.3 1D CNN for Time Series\n\n**1D Convolutional Neural Networks** can capture local patterns and are computationally efficient.\n\n**1D Convolution:**\n$$y_i = \\sum_{k=0}^{K-1} w_k \\cdot x_{i+k} + b$$\n\n**Advantages:**\n- Fast training compared to RNNs\n- Parallel processing (no sequential dependency)\n- Good at detecting local patterns\n- Parameter sharing through convolutional filters\n\n**Architecture:**\n```\nInput → Conv1D → ReLU → MaxPool → Conv1D → ReLU → MaxPool → Flatten → Dense → Output\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Multi-step ahead forecasting with LSTM\ndef forecast_lstm(model, last_sequence, steps, scaler):\n    \"\"\"\n    Perform multi-step ahead forecasting using trained LSTM model.\n    \n    Args:\n        model: Trained LSTM model\n        last_sequence: Last window_size observations (scaled)\n        steps: Number of steps to forecast\n        scaler: Scaler used for normalization\n    \n    Returns:\n        Array of forecasted values (original scale)\n    \"\"\"\n    model.eval()\n    forecasts = []\n    current_seq = torch.FloatTensor(last_sequence).unsqueeze(0).unsqueeze(-1).to(device)\n    \n    with torch.no_grad():\n        for _ in range(steps):\n            # Predict next value\n            pred = model(current_seq)\n            forecasts.append(pred.cpu().numpy()[0, 0])\n            \n            # Update sequence: remove first, append prediction\n            current_seq = torch.cat([\n                current_seq[:, 1:, :],\n                pred.unsqueeze(1).unsqueeze(-1)\n            ], dim=1)\n    \n    # Inverse transform to original scale\n    forecasts = np.array(forecasts).reshape(-1, 1)\n    forecasts = scaler.inverse_transform(forecasts).flatten()\n    \n    return forecasts\n\n# Get last window from training data\nlast_train_window = train_scaled[-window_size:]\n\n# Forecast test period\nlstm_forecast = forecast_lstm(lstm_model, last_train_window, len(test), scaler)\n\n# Plot results\nplt.figure(figsize=(15, 6))\nplt.plot(train.index, train.values, label='Training Data', alpha=0.7)\nplt.plot(test.index, test.values, label='Actual', color='black', linewidth=2)\nplt.plot(test.index, lstm_forecast, label='LSTM Forecast', linestyle='--', linewidth=2, color='purple')\nplt.axvline(test.index[0], color='red', linestyle=':', alpha=0.5, label='Forecast Start')\nplt.title('LSTM Time Series Forecasting')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Calculate errors\nlstm_mae = mean_absolute_error(test.values, lstm_forecast)\nlstm_rmse = np.sqrt(mean_squared_error(test.values, lstm_forecast))\n\nprint(f\"\\\\nLSTM Forecast Accuracy:\")\nprint(f\"MAE:  {lstm_mae:.4f}\")\nprint(f\"RMSE: {lstm_rmse:.4f}\")\n\nprint(f\"\\\\nComparison with SARIMA:\")\nprint(f\"SARIMA MAE: {sarima_mae:.4f}\")\nprint(f\"LSTM MAE:   {lstm_mae:.4f}\")\nimprovement = ((sarima_mae - lstm_mae) / sarima_mae * 100)\nprint(f\"Improvement: {improvement:.2f}%\\\" if improvement > 0 else f\\\"Degradation: {-improvement:.2f}%\\\")\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Training loop\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=50):\n    \"\"\"\n    Train the time series forecasting model.\n    \"\"\"\n    train_losses = []\n    val_losses = []\n    best_val_loss = float('inf')\n    patience_counter = 0\n    early_stop_patience = 10\n    \n    for epoch in range(epochs):\n        # Training phase\n        model.train()\n        train_loss = 0.0\n        \n        for batch_x, batch_y in train_loader:\n            # Move to device and add feature dimension\n            batch_x = batch_x.unsqueeze(-1).to(device)  # (batch, seq_len, 1)\n            batch_y = batch_y.to(device)\n            \n            # Forward pass\n            outputs = optimizer.zero_grad()\n            outputs = model(batch_x)\n            loss = criterion(outputs, batch_y)\n            \n            # Backward pass\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            \n            train_loss += loss.item()\n        \n        train_loss /= len(train_loader)\n        train_losses.append(train_loss)\n        \n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        \n        with torch.no_grad():\n            for batch_x, batch_y in val_loader:\n                batch_x = batch_x.unsqueeze(-1).to(device)\n                batch_y = batch_y.to(device)\n                \n                outputs = model(batch_x)\n                loss = criterion(outputs, batch_y)\n                val_loss += loss.item()\n        \n        val_loss /= len(val_loader)\n        val_losses.append(val_loss)\n        \n        # Learning rate scheduling\n        scheduler.step(val_loss)\n        \n        # Early stopping\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            # Save best model\n            torch.save(model.state_dict(), 'best_lstm_model.pth')\n        else:\n            patience_counter += 1\n        \n        if (epoch + 1) % 10 == 0:\n            print(f'Epoch [{epoch+1}/{epochs}] - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n        \n        if patience_counter >= early_stop_patience:\n            print(f'Early stopping at epoch {epoch+1}')\n            break\n    \n    # Load best model\n    model.load_state_dict(torch.load('best_lstm_model.pth'))\n    \n    return train_losses, val_losses\n\n# Train the model\nprint(\"Training LSTM model...\")\ntrain_losses, val_losses = train_model(\n    lstm_model, train_loader, val_loader, \n    criterion, optimizer, scheduler, epochs=100\n)\n\n# Plot training history\nplt.figure(figsize=(12, 4))\nplt.plot(train_losses, label='Train Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss (MSE)')\nplt.title('LSTM Training History')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(f\"\\\\nBest validation loss: {min(val_losses):.6f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class LSTMForecaster(nn.Module):\n    \"\"\"\n    LSTM model for time series forecasting.\n    \"\"\"\n    def __init__(self, input_size=1, hidden_size=64, num_layers=2, dropout=0.2, output_size=1):\n        \"\"\"\n        Args:\n            input_size: Number of features per time step\n            hidden_size: Number of LSTM hidden units\n            num_layers: Number of LSTM layers\n            dropout: Dropout probability\n            output_size: Number of output values (forecast horizon)\n        \"\"\"\n        super(LSTMForecaster, self).__init__()\n        \n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        # LSTM layers\n        self.lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            dropout=dropout if num_layers > 1 else 0,\n            batch_first=True\n        )\n        \n        # Fully connected output layer\n        self.fc = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, seq_len, input_size)\n        \n        Returns:\n            Output tensor of shape (batch_size, output_size)\n        \"\"\"\n        # x shape: (batch_size, seq_len, input_size)\n        \n        # LSTM forward pass\n        # lstm_out shape: (batch_size, seq_len, hidden_size)\n        # h_n shape: (num_layers, batch_size, hidden_size)\n        lstm_out, (h_n, c_n) = self.lstm(x)\n        \n        # Use the last hidden state for prediction\n        # h_n[-1] shape: (batch_size, hidden_size)\n        out = self.fc(h_n[-1])\n        \n        return out\n\n# Initialize model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nlstm_model = LSTMForecaster(\n    input_size=1,\n    hidden_size=64,\n    num_layers=2,\n    dropout=0.2,\n    output_size=horizon\n).to(device)\n\n# Training configuration\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n\nprint(lstm_model)\nprint(f\"\\\\nTotal parameters: {sum(p.numel() for p in lstm_model.parameters()):,}\")\nprint(f\"Training on device: {device}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4.2 LSTM for Time Series\n\n**Long Short-Term Memory (LSTM)** networks are designed to handle sequential data and can learn long-term dependencies.\n\n**LSTM Cell Equations:**\n\n$$\n\\begin{align}\nf_t &= \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f) \\quad \\text{(Forget gate)} \\\\\ni_t &= \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i) \\quad \\text{(Input gate)} \\\\\n\\tilde{C}_t &= \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C) \\quad \\text{(Candidate cell state)} \\\\\nC_t &= f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t \\quad \\text{(Cell state)} \\\\\no_t &= \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o) \\quad \\text{(Output gate)} \\\\\nh_t &= o_t \\odot \\tanh(C_t) \\quad \\text{(Hidden state)}\n\\end{align}\n$$\n\nwhere:\n- $\\sigma$ is the sigmoid function\n- $\\odot$ is element-wise multiplication\n- $f_t, i_t, o_t$ are forget, input, and output gates\n- $C_t$ is the cell state (long-term memory)\n- $h_t$ is the hidden state (short-term memory)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class TimeSeriesDataset(Dataset):\n    \"\"\"\n    PyTorch Dataset for time series with sliding window.\n    \"\"\"\n    def __init__(self, data, window_size=20, horizon=1):\n        \"\"\"\n        Args:\n            data: Time series data (numpy array or pandas Series)\n            window_size: Number of past time steps to use as input\n            horizon: Number of future time steps to predict\n        \"\"\"\n        self.data = data if isinstance(data, np.ndarray) else data.values\n        self.window_size = window_size\n        self.horizon = horizon\n        \n        # Create sequences\n        self.X, self.y = self._create_sequences()\n    \n    def _create_sequences(self):\n        \"\"\"Create input-output sequences using sliding window.\"\"\"\n        X, y = [], []\n        \n        for i in range(len(self.data) - self.window_size - self.horizon + 1):\n            X.append(self.data[i:i + self.window_size])\n            y.append(self.data[i + self.window_size:i + self.window_size + self.horizon])\n        \n        return np.array(X), np.array(y)\n    \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        return (\n            torch.FloatTensor(self.X[idx]),\n            torch.FloatTensor(self.y[idx])\n        )\n\n# Prepare data for deep learning\nscaler = MinMaxScaler()\ntrain_scaled = scaler.fit_transform(train.values.reshape(-1, 1)).flatten()\ntest_scaled = scaler.transform(test.values.reshape(-1, 1)).flatten()\n\n# Create datasets\nwindow_size = 30\nhorizon = 1\n\ntrain_dataset = TimeSeriesDataset(train_scaled, window_size=window_size, horizon=horizon)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n# Create a validation set from the last 20% of training data\nval_size = int(0.2 * len(train_dataset))\ntrain_size = len(train_dataset) - val_size\ntrain_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\nval_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n\nprint(f\"Training samples: {len(train_subset)}\")\nprint(f\"Validation samples: {len(val_subset)}\")\nprint(f\"Window size: {window_size}\")\nprint(f\"Forecast horizon: {horizon}\")\nprint(f\"\\\\nSample input shape: {train_dataset[0][0].shape}\")\nprint(f\"Sample output shape: {train_dataset[0][1].shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 4. Deep Learning for Time Series\n\nDeep learning methods can capture complex patterns and non-linear relationships in time series data.\n\n### 4.1 Time Series Dataset Preparation\n\nFor supervised learning, we need to convert time series into input-output pairs using a **sliding window** approach.\n\n**Example:**\n```\nOriginal series: [1, 2, 3, 4, 5, 6, 7, 8]\nWindow size = 3, horizon = 1\n\nX                 y\n[1, 2, 3]    →    4\n[2, 3, 4]    →    5\n[3, 4, 5]    →    6\n[4, 5, 6]    →    7\n[5, 6, 7]    →    8\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Analysis & Forecasting: Complete Guide\n",
    "\n",
    "**Comprehensive coverage from classical methods to modern deep learning approaches**\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Time Series Fundamentals](#1-time-series-fundamentals)\n",
    "   - Components of time series\n",
    "   - Stationarity and differencing\n",
    "   - ACF/PACF analysis\n",
    "   - Decomposition methods\n",
    "\n",
    "2. [Classical Statistical Methods](#2-classical-statistical-methods)\n",
    "   - Moving Averages\n",
    "   - Exponential Smoothing (SES, DES, Holt-Winters)\n",
    "   - ARIMA Models\n",
    "   - SARIMA (Seasonal ARIMA)\n",
    "\n",
    "3. [Modern Statistical Methods](#3-modern-statistical-methods)\n",
    "   - State Space Models\n",
    "   - Prophet (Facebook)\n",
    "   - Vector Autoregression (VAR)\n",
    "\n",
    "4. [Deep Learning for Time Series](#4-deep-learning-for-time-series)\n",
    "   - RNN/LSTM/GRU\n",
    "   - 1D CNNs\n",
    "   - Transformers for Time Series\n",
    "   - Temporal Convolutional Networks (TCN)\n",
    "\n",
    "5. [Advanced Topics](#5-advanced-topics)\n",
    "   - Multivariate Time Series\n",
    "   - Anomaly Detection\n",
    "   - Probabilistic Forecasting\n",
    "   - Transfer Learning\n",
    "\n",
    "6. [Best Practices & Production](#6-best-practices-production)\n",
    "   - Cross-validation strategies\n",
    "   - Evaluation metrics\n",
    "   - Feature engineering\n",
    "\n",
    "7. [Interview Questions](#7-interview-questions)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Time series specific\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Time Series Fundamentals\n",
    "\n",
    "### 1.1 What is a Time Series?\n",
    "\n",
    "A **time series** is a sequence of data points indexed in time order. Each observation is associated with a specific time point.\n",
    "\n",
    "**Mathematical Definition:**\n",
    "\n",
    "$$Y = \\{y_t : t \\in T\\}$$\n",
    "\n",
    "where:\n",
    "- $y_t$ is the observation at time $t$\n",
    "- $T$ is the index set (usually integers or datetime)\n",
    "\n",
    "### 1.2 Components of Time Series\n",
    "\n",
    "Any time series can be decomposed into:\n",
    "\n",
    "1. **Trend ($T_t$)**: Long-term increase or decrease\n",
    "2. **Seasonality ($S_t$)**: Regular, periodic fluctuations\n",
    "3. **Cyclic ($C_t$)**: Non-periodic fluctuations\n",
    "4. **Irregular/Noise ($I_t$)**: Random variation\n",
    "\n",
    "**Additive Model:**\n",
    "$$y_t = T_t + S_t + C_t + I_t$$\n",
    "\n",
    "**Multiplicative Model:**\n",
    "$$y_t = T_t \\times S_t \\times C_t \\times I_t$$\n",
    "\n",
    "### 1.3 Stationarity\n",
    "\n",
    "A time series is **stationary** if its statistical properties (mean, variance, autocorrelation) are constant over time.\n",
    "\n",
    "**Weak Stationarity** (2nd order stationarity):\n",
    "1. $E[y_t] = \\mu$ (constant mean)\n",
    "2. $Var(y_t) = \\sigma^2$ (constant variance)\n",
    "3. $Cov(y_t, y_{t-k})$ depends only on lag $k$, not on $t$\n",
    "\n",
    "**Why is stationarity important?**\n",
    "- Most statistical forecasting methods assume stationarity\n",
    "- Non-stationary data can lead to spurious relationships\n",
    "- Makes the data easier to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic time series with different components\n",
    "def generate_time_series(n=500, trend=True, seasonal=True, noise_level=1.0):\n",
    "    \"\"\"\n",
    "    Generate synthetic time series with trend, seasonality, and noise.\n",
    "    \n",
    "    Args:\n",
    "        n: Number of time points\n",
    "        trend: Whether to include trend component\n",
    "        seasonal: Whether to include seasonal component\n",
    "        noise_level: Standard deviation of noise\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Generated time series\n",
    "    \"\"\"\n",
    "    t = np.arange(n)\n",
    "    \n",
    "    # Trend component (linear)\n",
    "    trend_component = 0.05 * t if trend else np.zeros(n)\n",
    "    \n",
    "    # Seasonal component (annual + quarterly)\n",
    "    seasonal_component = 0\n",
    "    if seasonal:\n",
    "        seasonal_component = (\n",
    "            10 * np.sin(2 * np.pi * t / 50) +  # Annual\n",
    "            5 * np.sin(2 * np.pi * t / 12.5)   # Quarterly\n",
    "        )\n",
    "    \n",
    "    # Noise component\n",
    "    noise = np.random.normal(0, noise_level, n)\n",
    "    \n",
    "    # Combine components\n",
    "    series = trend_component + seasonal_component + noise\n",
    "    \n",
    "    # Create time index\n",
    "    dates = pd.date_range(start='2020-01-01', periods=n, freq='D')\n",
    "    return pd.Series(series, index=dates)\n",
    "\n",
    "# Generate different types of time series\n",
    "ts_stationary = generate_time_series(500, trend=False, seasonal=False, noise_level=1.0)\n",
    "ts_trend = generate_time_series(500, trend=True, seasonal=False, noise_level=1.0)\n",
    "ts_seasonal = generate_time_series(500, trend=False, seasonal=True, noise_level=1.0)\n",
    "ts_full = generate_time_series(500, trend=True, seasonal=True, noise_level=1.0)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "ts_stationary.plot(ax=axes[0, 0], title='Stationary (White Noise)')\n",
    "axes[0, 0].set_ylabel('Value')\n",
    "\n",
    "ts_trend.plot(ax=axes[0, 1], title='Trend Only')\n",
    "axes[0, 1].set_ylabel('Value')\n",
    "\n",
    "ts_seasonal.plot(ax=axes[1, 0], title='Seasonal Only')\n",
    "axes[1, 0].set_ylabel('Value')\n",
    "\n",
    "ts_full.plot(ax=axes[1, 1], title='Trend + Seasonal + Noise')\n",
    "axes[1, 1].set_ylabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Generated 4 types of time series\")\n",
    "print(f\"Series length: {len(ts_full)}\")\n",
    "print(f\"Date range: {ts_full.index[0]} to {ts_full.index[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Testing for Stationarity: Augmented Dickey-Fuller Test\n",
    "\n",
    "The **ADF test** tests the null hypothesis that a unit root is present (i.e., the series is non-stationary).\n",
    "\n",
    "**Hypotheses:**\n",
    "- $H_0$: The series has a unit root (non-stationary)\n",
    "- $H_1$: The series has no unit root (stationary)\n",
    "\n",
    "**Test Equation:**\n",
    "$$\\Delta y_t = \\alpha + \\beta t + \\gamma y_{t-1} + \\sum_{i=1}^{p} \\delta_i \\Delta y_{t-i} + \\epsilon_t$$\n",
    "\n",
    "**Decision Rule:**\n",
    "- If p-value < 0.05: Reject $H_0$ (series is stationary)\n",
    "- If p-value >= 0.05: Fail to reject $H_0$ (series is non-stationary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adf_test(series, name=''):\n",
    "    \"\"\"\n",
    "    Perform Augmented Dickey-Fuller test for stationarity.\n",
    "    \n",
    "    Args:\n",
    "        series: Time series data\n",
    "        name: Name of the series for display\n",
    "    \n",
    "    Returns:\n",
    "        dict: Test results\n",
    "    \"\"\"\n",
    "    result = adfuller(series.dropna(), autolag='AIC')\n",
    "    \n",
    "    output = {\n",
    "        'test_statistic': result[0],\n",
    "        'p_value': result[1],\n",
    "        'lags_used': result[2],\n",
    "        'n_observations': result[3],\n",
    "        'critical_values': result[4],\n",
    "        'is_stationary': result[1] < 0.05\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ADF Test Results for {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Test Statistic:      {output['test_statistic']:.6f}\")\n",
    "    print(f\"P-value:             {output['p_value']:.6f}\")\n",
    "    print(f\"Lags Used:           {output['lags_used']}\")\n",
    "    print(f\"Number of Obs:       {output['n_observations']}\")\n",
    "    print(f\"\\nCritical Values:\")\n",
    "    for key, value in output['critical_values'].items():\n",
    "        print(f\"  {key}: {value:.3f}\")\n",
    "    \n",
    "    if output['is_stationary']:\n",
    "        print(f\"\\n✓ Series IS stationary (p-value < 0.05)\")\n",
    "    else:\n",
    "        print(f\"\\n✗ Series is NOT stationary (p-value >= 0.05)\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Test all series\n",
    "adf_test(ts_stationary, 'White Noise (Expected: Stationary)')\n",
    "adf_test(ts_trend, 'Trend Series (Expected: Non-Stationary)')\n",
    "adf_test(ts_seasonal, 'Seasonal Series')\n",
    "adf_test(ts_full, 'Full Series (Expected: Non-Stationary)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Making Time Series Stationary: Differencing\n",
    "\n",
    "**Differencing** is the transformation that computes the difference between consecutive observations.\n",
    "\n",
    "**First-order differencing:**\n",
    "$$\\Delta y_t = y_t - y_{t-1}$$\n",
    "\n",
    "**Second-order differencing:**\n",
    "$$\\Delta^2 y_t = \\Delta y_t - \\Delta y_{t-1} = y_t - 2y_{t-1} + y_{t-2}$$\n",
    "\n",
    "**Seasonal differencing (period $s$):**\n",
    "$$\\Delta_s y_t = y_t - y_{t-s}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply differencing to make non-stationary series stationary\n",
    "def make_stationary(series, max_diff=3):\n",
    "    \"\"\"\n",
    "    Apply differencing until series becomes stationary.\n",
    "    \n",
    "    Args:\n",
    "        series: Time series to transform\n",
    "        max_diff: Maximum number of differences to try\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (stationary_series, num_differences)\n",
    "    \"\"\"\n",
    "    diff_series = series.copy()\n",
    "    \n",
    "    for d in range(max_diff + 1):\n",
    "        result = adfuller(diff_series.dropna(), autolag='AIC')\n",
    "        p_value = result[1]\n",
    "        \n",
    "        print(f\"Difference order {d}: p-value = {p_value:.6f}\", end='')\n",
    "        \n",
    "        if p_value < 0.05:\n",
    "            print(\" ✓ STATIONARY\")\n",
    "            return diff_series, d\n",
    "        else:\n",
    "            print(\" - Not stationary\")\n",
    "            if d < max_diff:\n",
    "                diff_series = diff_series.diff()\n",
    "    \n",
    "    return diff_series, max_diff\n",
    "\n",
    "print(\"Making trend series stationary:\")\n",
    "ts_trend_stationary, d_trend = make_stationary(ts_trend)\n",
    "\n",
    "print(\"\\nMaking full series stationary:\")\n",
    "ts_full_stationary, d_full = make_stationary(ts_full)\n",
    "\n",
    "# Visualize original vs differenced\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 8))\n",
    "\n",
    "ts_trend.plot(ax=axes[0, 0], title='Original Trend Series')\n",
    "axes[0, 0].set_ylabel('Value')\n",
    "\n",
    "ts_trend_stationary.plot(ax=axes[0, 1], title=f'After {d_trend} Difference(s)')\n",
    "axes[0, 1].set_ylabel('Value')\n",
    "\n",
    "ts_full.plot(ax=axes[1, 0], title='Original Full Series')\n",
    "axes[1, 0].set_ylabel('Value')\n",
    "\n",
    "ts_full_stationary.plot(ax=axes[1, 1], title=f'After {d_full} Difference(s)')\n",
    "axes[1, 1].set_ylabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Autocorrelation Function (ACF) and Partial Autocorrelation (PACF)\n",
    "\n",
    "**Autocorrelation** measures the correlation between a time series and a lagged version of itself.\n",
    "\n",
    "**ACF at lag $k$:**\n",
    "$$\\rho_k = \\frac{Cov(y_t, y_{t-k})}{Var(y_t)} = \\frac{\\sum_{t=k+1}^{n}(y_t - \\bar{y})(y_{t-k} - \\bar{y})}{\\sum_{t=1}^{n}(y_t - \\bar{y})^2}$$\n",
    "\n",
    "**PACF at lag $k$:**\n",
    "The correlation between $y_t$ and $y_{t-k}$ after removing the effect of intermediate lags.\n",
    "\n",
    "**Why are ACF/PACF important?**\n",
    "- Identify patterns in time series\n",
    "- Determine order of ARIMA models:\n",
    "  - ACF cuts off after lag $q$ → MA($q$)\n",
    "  - PACF cuts off after lag $p$ → AR($p$)\n",
    "  - Both decay gradually → ARMA($p$, $q$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acf_pacf(series, lags=40, title=''):\n",
    "    \"\"\"\n",
    "    Plot ACF and PACF for a time series.\n",
    "    \n",
    "    Args:\n",
    "        series: Time series data\n",
    "        lags: Number of lags to plot\n",
    "        title: Title for the plots\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 4))\n",
    "    \n",
    "    # ACF plot\n",
    "    plot_acf(series.dropna(), lags=lags, ax=axes[0])\n",
    "    axes[0].set_title(f'ACF - {title}')\n",
    "    axes[0].set_xlabel('Lag')\n",
    "    axes[0].set_ylabel('Autocorrelation')\n",
    "    \n",
    "    # PACF plot\n",
    "    plot_pacf(series.dropna(), lags=lags, ax=axes[1])\n",
    "    axes[1].set_title(f'PACF - {title}')\n",
    "    axes[1].set_xlabel('Lag')\n",
    "    axes[1].set_ylabel('Partial Autocorrelation')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for different series\n",
    "plot_acf_pacf(ts_stationary, title='White Noise')\n",
    "plot_acf_pacf(ts_seasonal, title='Seasonal Series')\n",
    "plot_acf_pacf(ts_full_stationary.dropna(), title='Differenced Full Series')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Time Series Decomposition\n",
    "\n",
    "Decomposition separates a time series into its constituent components.\n",
    "\n",
    "**Methods:**\n",
    "1. **Additive Decomposition**: $y_t = T_t + S_t + R_t$\n",
    "   - Use when seasonal variation is roughly constant over time\n",
    "\n",
    "2. **Multiplicative Decomposition**: $y_t = T_t \\times S_t \\times R_t$\n",
    "   - Use when seasonal variation increases with the level of the series\n",
    "\n",
    "**Classical Decomposition Algorithm:**\n",
    "1. Estimate trend using moving average\n",
    "2. Remove trend to get detrended series\n",
    "3. Estimate seasonal component by averaging detrended values for each season\n",
    "4. Remainder = Original - Trend - Seasonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose time series\n",
    "def decompose_series(series, model='additive', period=50):\n",
    "    \"\"\"\n",
    "    Decompose time series into trend, seasonal, and residual components.\n",
    "    \n",
    "    Args:\n",
    "        series: Time series to decompose\n",
    "        model: 'additive' or 'multiplicative'\n",
    "        period: Period of seasonal component\n",
    "    \n",
    "    Returns:\n",
    "        Decomposition object\n",
    "    \"\"\"\n",
    "    result = seasonal_decompose(series, model=model, period=period)\n",
    "    \n",
    "    # Plot decomposition\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(15, 10))\n",
    "    \n",
    "    result.observed.plot(ax=axes[0])\n",
    "    axes[0].set_ylabel('Observed')\n",
    "    axes[0].set_title(f'Time Series Decomposition ({model.capitalize()})')\n",
    "    \n",
    "    result.trend.plot(ax=axes[1])\n",
    "    axes[1].set_ylabel('Trend')\n",
    "    \n",
    "    result.seasonal.plot(ax=axes[2])\n",
    "    axes[2].set_ylabel('Seasonal')\n",
    "    \n",
    "    result.resid.plot(ax=axes[3])\n",
    "    axes[3].set_ylabel('Residual')\n",
    "    axes[3].set_xlabel('Time')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\nDecomposition Statistics:\")\n",
    "    print(f\"Trend strength:    {1 - result.resid.var() / (result.trend + result.resid).var():.4f}\")\n",
    "    print(f\"Seasonal strength: {1 - result.resid.var() / (result.seasonal + result.resid).var():.4f}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Decompose the full series\n",
    "decomposition = decompose_series(ts_full, model='additive', period=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Classical Statistical Methods\n",
    "\n",
    "### 2.1 Moving Averages\n",
    "\n",
    "**Simple Moving Average (SMA):**\n",
    "$$\\hat{y}_{t+1} = \\frac{1}{k}\\sum_{i=0}^{k-1} y_{t-i}$$\n",
    "\n",
    "**Weighted Moving Average (WMA):**\n",
    "$$\\hat{y}_{t+1} = \\sum_{i=0}^{k-1} w_i y_{t-i}, \\quad \\sum_{i=0}^{k-1} w_i = 1$$\n",
    "\n",
    "**Properties:**\n",
    "- Simple to implement and understand\n",
    "- Good for smoothing noisy data\n",
    "- Lag behind trends\n",
    "- All past observations weighted equally (SMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovingAverage:\n",
    "    \"\"\"\n",
    "    Simple and Weighted Moving Average forecaster.\n",
    "    \"\"\"\n",
    "    def __init__(self, window=10, weights=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            window: Size of the moving window\n",
    "            weights: Optional weights for WMA (must sum to 1)\n",
    "        \"\"\"\n",
    "        self.window = window\n",
    "        self.weights = weights\n",
    "        \n",
    "        if weights is not None:\n",
    "            assert len(weights) == window, \"Weights length must equal window size\"\n",
    "            assert abs(sum(weights) - 1.0) < 1e-6, \"Weights must sum to 1\"\n",
    "    \n",
    "    def fit(self, series):\n",
    "        \"\"\"Store the series for forecasting.\"\"\"\n",
    "        self.series = series.copy()\n",
    "        return self\n",
    "    \n",
    "    def predict(self, steps=1):\n",
    "        \"\"\"\n",
    "        Forecast future values.\n",
    "        \n",
    "        Args:\n",
    "            steps: Number of steps ahead to forecast\n",
    "        \n",
    "        Returns:\n",
    "            Array of forecasted values\n",
    "        \"\"\"\n",
    "        forecasts = []\n",
    "        series = self.series.values.copy()\n",
    "        \n",
    "        for _ in range(steps):\n",
    "            # Get last 'window' observations\n",
    "            recent = series[-self.window:]\n",
    "            \n",
    "            # Calculate forecast\n",
    "            if self.weights is None:\n",
    "                # Simple moving average\n",
    "                forecast = np.mean(recent)\n",
    "            else:\n",
    "                # Weighted moving average\n",
    "                forecast = np.sum(recent * self.weights[::-1])  # Reverse weights for recency\n",
    "            \n",
    "            forecasts.append(forecast)\n",
    "            series = np.append(series, forecast)\n",
    "        \n",
    "        return np.array(forecasts)\n",
    "\n",
    "# Test SMA vs WMA\n",
    "window = 20\n",
    "\n",
    "# Simple MA\n",
    "sma = MovingAverage(window=window)\n",
    "sma.fit(ts_full[:-50])\n",
    "sma_forecast = sma.predict(steps=50)\n",
    "\n",
    "# Weighted MA (exponentially decaying weights)\n",
    "weights = np.exp(np.linspace(-2, 0, window))\n",
    "weights = weights / weights.sum()\n",
    "wma = MovingAverage(window=window, weights=weights)\n",
    "wma.fit(ts_full[:-50])\n",
    "wma_forecast = wma.predict(steps=50)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(ts_full.index[:-50], ts_full.values[:-50], label='Training Data', alpha=0.7)\n",
    "plt.plot(ts_full.index[-50:], ts_full.values[-50:], label='Actual', color='black', linewidth=2)\n",
    "plt.plot(ts_full.index[-50:], sma_forecast, label=f'SMA (window={window})', linestyle='--')\n",
    "plt.plot(ts_full.index[-50:], wma_forecast, label=f'WMA (window={window})', linestyle='--')\n",
    "plt.axvline(ts_full.index[-50], color='red', linestyle=':', alpha=0.5, label='Forecast Start')\n",
    "plt.title('Moving Average Forecasting')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Calculate errors\n",
    "actual = ts_full.values[-50:]\n",
    "print(f\"\\nSMA - MAE: {mean_absolute_error(actual, sma_forecast):.4f}\")\n",
    "print(f\"SMA - RMSE: {np.sqrt(mean_squared_error(actual, sma_forecast)):.4f}\")\n",
    "print(f\"\\nWMA - MAE: {mean_absolute_error(actual, wma_forecast):.4f}\")\n",
    "print(f\"WMA - RMSE: {np.sqrt(mean_squared_error(actual, wma_forecast)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Exponential Smoothing\n",
    "\n",
    "Exponential smoothing methods weight recent observations more heavily.\n",
    "\n",
    "#### 2.2.1 Simple Exponential Smoothing (SES)\n",
    "\n",
    "For data with no trend or seasonality:\n",
    "\n",
    "$$\\hat{y}_{t+1} = \\alpha y_t + (1-\\alpha)\\hat{y}_t = \\hat{y}_t + \\alpha(y_t - \\hat{y}_t)$$\n",
    "\n",
    "where $0 \\leq \\alpha \\leq 1$ is the smoothing parameter.\n",
    "\n",
    "**Expanding the recursion:**\n",
    "$$\\hat{y}_{t+1} = \\alpha \\sum_{i=0}^{t-1} (1-\\alpha)^i y_{t-i} + (1-\\alpha)^t \\hat{y}_1$$\n",
    "\n",
    "#### 2.2.2 Double Exponential Smoothing (Holt's Method)\n",
    "\n",
    "For data with trend but no seasonality:\n",
    "\n",
    "**Level equation:**\n",
    "$$\\ell_t = \\alpha y_t + (1-\\alpha)(\\ell_{t-1} + b_{t-1})$$\n",
    "\n",
    "**Trend equation:**\n",
    "$$b_t = \\beta(\\ell_t - \\ell_{t-1}) + (1-\\beta)b_{t-1}$$\n",
    "\n",
    "**Forecast:**\n",
    "$$\\hat{y}_{t+h} = \\ell_t + h b_t$$\n",
    "\n",
    "#### 2.2.3 Triple Exponential Smoothing (Holt-Winters)\n",
    "\n",
    "For data with trend AND seasonality:\n",
    "\n",
    "**Additive seasonality:**\n",
    "$$\\ell_t = \\alpha (y_t - s_{t-m}) + (1-\\alpha)(\\ell_{t-1} + b_{t-1})$$\n",
    "$$b_t = \\beta(\\ell_t - \\ell_{t-1}) + (1-\\beta)b_{t-1}$$\n",
    "$$s_t = \\gamma(y_t - \\ell_t) + (1-\\gamma)s_{t-m}$$\n",
    "$$\\hat{y}_{t+h} = \\ell_t + h b_t + s_{t+h-m}$$\n",
    "\n",
    "where $m$ is the seasonal period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExponentialSmoothing:\n",
    "    \"\"\"\n",
    "    Implementation of Simple, Double, and Triple Exponential Smoothing.\n",
    "    \"\"\"\n",
    "    def __init__(self, method='simple', alpha=0.3, beta=0.1, gamma=0.1, seasonal_period=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            method: 'simple', 'double', or 'triple'\n",
    "            alpha: Smoothing parameter for level (0 < alpha < 1)\n",
    "            beta: Smoothing parameter for trend (0 < beta < 1)\n",
    "            gamma: Smoothing parameter for seasonality (0 < gamma < 1)\n",
    "            seasonal_period: Period of seasonality for triple smoothing\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.seasonal_period = seasonal_period\n",
    "    \n",
    "    def fit(self, series):\n",
    "        \"\"\"Fit the model to the time series.\"\"\"\n",
    "        self.series = series.values\n",
    "        n = len(self.series)\n",
    "        \n",
    "        if self.method == 'simple':\n",
    "            # Simple exponential smoothing\n",
    "            self.level = np.zeros(n)\n",
    "            self.level[0] = self.series[0]\n",
    "            \n",
    "            for t in range(1, n):\n",
    "                self.level[t] = self.alpha * self.series[t] + (1 - self.alpha) * self.level[t-1]\n",
    "        \n",
    "        elif self.method == 'double':\n",
    "            # Double exponential smoothing (Holt's method)\n",
    "            self.level = np.zeros(n)\n",
    "            self.trend = np.zeros(n)\n",
    "            \n",
    "            # Initialize\n",
    "            self.level[0] = self.series[0]\n",
    "            self.trend[0] = self.series[1] - self.series[0]\n",
    "            \n",
    "            for t in range(1, n):\n",
    "                self.level[t] = (\n",
    "                    self.alpha * self.series[t] + \n",
    "                    (1 - self.alpha) * (self.level[t-1] + self.trend[t-1])\n",
    "                )\n",
    "                self.trend[t] = (\n",
    "                    self.beta * (self.level[t] - self.level[t-1]) +\n",
    "                    (1 - self.beta) * self.trend[t-1]\n",
    "                )\n",
    "        \n",
    "        elif self.method == 'triple':\n",
    "            # Triple exponential smoothing (Holt-Winters)\n",
    "            if self.seasonal_period is None:\n",
    "                raise ValueError(\"seasonal_period must be specified for triple smoothing\")\n",
    "            \n",
    "            m = self.seasonal_period\n",
    "            self.level = np.zeros(n)\n",
    "            self.trend = np.zeros(n)\n",
    "            self.seasonal = np.zeros(n)\n",
    "            \n",
    "            # Initialize level and trend\n",
    "            self.level[0] = np.mean(self.series[:m])\n",
    "            self.trend[0] = (np.mean(self.series[m:2*m]) - np.mean(self.series[:m])) / m\n",
    "            \n",
    "            # Initialize seasonal components\n",
    "            for i in range(m):\n",
    "                self.seasonal[i] = self.series[i] - self.level[0]\n",
    "            \n",
    "            # Update components\n",
    "            for t in range(1, n):\n",
    "                if t >= m:\n",
    "                    self.level[t] = (\n",
    "                        self.alpha * (self.series[t] - self.seasonal[t-m]) +\n",
    "                        (1 - self.alpha) * (self.level[t-1] + self.trend[t-1])\n",
    "                    )\n",
    "                    self.trend[t] = (\n",
    "                        self.beta * (self.level[t] - self.level[t-1]) +\n",
    "                        (1 - self.beta) * self.trend[t-1]\n",
    "                    )\n",
    "                    self.seasonal[t] = (\n",
    "                        self.gamma * (self.series[t] - self.level[t]) +\n",
    "                        (1 - self.gamma) * self.seasonal[t-m]\n",
    "                    )\n",
    "                else:\n",
    "                    self.level[t] = self.level[0]\n",
    "                    self.trend[t] = self.trend[0]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, steps=1):\n",
    "        \"\"\"\n",
    "        Forecast future values.\n",
    "        \n",
    "        Args:\n",
    "            steps: Number of steps ahead to forecast\n",
    "        \n",
    "        Returns:\n",
    "            Array of forecasted values\n",
    "        \"\"\"\n",
    "        forecasts = np.zeros(steps)\n",
    "        \n",
    "        if self.method == 'simple':\n",
    "            # Forecast is just the last level\n",
    "            forecasts[:] = self.level[-1]\n",
    "        \n",
    "        elif self.method == 'double':\n",
    "            # Forecast includes trend\n",
    "            for h in range(1, steps + 1):\n",
    "                forecasts[h-1] = self.level[-1] + h * self.trend[-1]\n",
    "        \n",
    "        elif self.method == 'triple':\n",
    "            # Forecast includes trend and seasonality\n",
    "            m = self.seasonal_period\n",
    "            for h in range(1, steps + 1):\n",
    "                seasonal_idx = -m + ((h - 1) % m)\n",
    "                forecasts[h-1] = self.level[-1] + h * self.trend[-1] + self.seasonal[seasonal_idx]\n",
    "        \n",
    "        return forecasts\n",
    "\n",
    "# Test all three methods\n",
    "train_size = len(ts_full) - 50\n",
    "train = ts_full[:train_size]\n",
    "test = ts_full[train_size:]\n",
    "\n",
    "# Simple ES\n",
    "ses = ExponentialSmoothing(method='simple', alpha=0.3)\n",
    "ses.fit(train)\n",
    "ses_forecast = ses.predict(steps=50)\n",
    "\n",
    "# Double ES\n",
    "des = ExponentialSmoothing(method='double', alpha=0.3, beta=0.1)\n",
    "des.fit(train)\n",
    "des_forecast = des.predict(steps=50)\n",
    "\n",
    "# Triple ES\n",
    "tes = ExponentialSmoothing(method='triple', alpha=0.3, beta=0.1, gamma=0.1, seasonal_period=50)\n",
    "tes.fit(train)\n",
    "tes_forecast = tes.predict(steps=50)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(train.index, train.values, label='Training Data', alpha=0.7)\n",
    "plt.plot(test.index, test.values, label='Actual', color='black', linewidth=2)\n",
    "plt.plot(test.index, ses_forecast, label='Simple ES', linestyle='--')\n",
    "plt.plot(test.index, des_forecast, label='Double ES (Holt)', linestyle='--')\n",
    "plt.plot(test.index, tes_forecast, label='Triple ES (Holt-Winters)', linestyle='--')\n",
    "plt.axvline(test.index[0], color='red', linestyle=':', alpha=0.5, label='Forecast Start')\n",
    "plt.title('Exponential Smoothing Methods Comparison')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Calculate errors\n",
    "actual = test.values\n",
    "print(\"\\nForecast Accuracy:\")\n",
    "print(f\"Simple ES    - MAE: {mean_absolute_error(actual, ses_forecast):.4f}, RMSE: {np.sqrt(mean_squared_error(actual, ses_forecast)):.4f}\")\n",
    "print(f\"Double ES    - MAE: {mean_absolute_error(actual, des_forecast):.4f}, RMSE: {np.sqrt(mean_squared_error(actual, des_forecast)):.4f}\")\n",
    "print(f\"Triple ES    - MAE: {mean_absolute_error(actual, tes_forecast):.4f}, RMSE: {np.sqrt(mean_squared_error(actual, tes_forecast)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 ARIMA Models\n",
    "\n",
    "**ARIMA(p, d, q)** = AutoRegressive Integrated Moving Average\n",
    "\n",
    "- **p**: Order of autoregressive part\n",
    "- **d**: Degree of differencing\n",
    "- **q**: Order of moving average part\n",
    "\n",
    "#### AR(p) - Autoregressive Model\n",
    "\n",
    "$$y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\cdots + \\phi_p y_{t-p} + \\epsilon_t$$\n",
    "\n",
    "The current value is a linear combination of past values.\n",
    "\n",
    "#### MA(q) - Moving Average Model\n",
    "\n",
    "$$y_t = c + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\cdots + \\theta_q \\epsilon_{t-q}$$\n",
    "\n",
    "The current value is a linear combination of past forecast errors.\n",
    "\n",
    "#### ARMA(p, q)\n",
    "\n",
    "$$y_t = c + \\phi_1 y_{t-1} + \\cdots + \\phi_p y_{t-p} + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\cdots + \\theta_q \\epsilon_{t-q}$$\n",
    "\n",
    "#### ARIMA(p, d, q)\n",
    "\n",
    "ARMA model applied to the differenced series:\n",
    "$$\\Delta^d y_t = \\text{ARMA}(p, q)$$\n",
    "\n",
    "**How to select p, d, q:**\n",
    "1. **d**: Number of differences needed to make series stationary (ADF test)\n",
    "2. **p**: Look at PACF plot - where it cuts off\n",
    "3. **q**: Look at ACF plot - where it cuts off\n",
    "4. Use AIC/BIC for model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_arima(series, max_p=5, max_d=2, max_q=5):\n",
    "    \"\"\"\n",
    "    Find the best ARIMA order using AIC.\n",
    "    \n",
    "    Args:\n",
    "        series: Time series data\n",
    "        max_p, max_d, max_q: Maximum orders to try\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Best (p, d, q) order and the fitted model\n",
    "    \"\"\"\n",
    "    best_aic = np.inf\n",
    "    best_order = None\n",
    "    best_model = None\n",
    "    \n",
    "    print(\"Searching for best ARIMA model...\")\n",
    "    print(f\"{'Order':<15} {'AIC':<12} {'BIC':<12}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Grid search\n",
    "    results = []\n",
    "    for p in range(max_p + 1):\n",
    "        for d in range(max_d + 1):\n",
    "            for q in range(max_q + 1):\n",
    "                try:\n",
    "                    model = ARIMA(series, order=(p, d, q))\n",
    "                    fitted = model.fit()\n",
    "                    \n",
    "                    aic = fitted.aic\n",
    "                    bic = fitted.bic\n",
    "                    results.append(((p, d, q), aic, bic))\n",
    "                    \n",
    "                    if aic < best_aic:\n",
    "                        best_aic = aic\n",
    "                        best_order = (p, d, q)\n",
    "                        best_model = fitted\n",
    "                    \n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    # Print top 5 models\n",
    "    results.sort(key=lambda x: x[1])\n",
    "    for i, (order, aic, bic) in enumerate(results[:5]):\n",
    "        marker = \"<-- BEST\" if order == best_order else \"\"\n",
    "        print(f\"{str(order):<15} {aic:<12.2f} {bic:<12.2f} {marker}\")\n",
    "    \n",
    "    return best_order, best_model\n",
    "\n",
    "# Find best ARIMA model\n",
    "best_order, best_arima = find_best_arima(train, max_p=3, max_d=2, max_q=3)\n",
    "\n",
    "print(f\"\\n\\nBest ARIMA order: {best_order}\")\n",
    "print(f\"\\nModel Summary:\")\n",
    "print(best_arima.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast with ARIMA\n",
    "arima_forecast = best_arima.forecast(steps=50)\n",
    "\n",
    "# Get confidence intervals\n",
    "forecast_result = best_arima.get_forecast(steps=50)\n",
    "forecast_df = forecast_result.summary_frame(alpha=0.05)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(train.index, train.values, label='Training Data', alpha=0.7)\n",
    "plt.plot(test.index, test.values, label='Actual', color='black', linewidth=2)\n",
    "plt.plot(test.index, arima_forecast, label=f'ARIMA{best_order}', linestyle='--', linewidth=2)\n",
    "\n",
    "# Plot confidence intervals\n",
    "plt.fill_between(test.index, \n",
    "                 forecast_df['mean_ci_lower'].values, \n",
    "                 forecast_df['mean_ci_upper'].values, \n",
    "                 alpha=0.2, label='95% CI')\n",
    "\n",
    "plt.axvline(test.index[0], color='red', linestyle=':', alpha=0.5, label='Forecast Start')\n",
    "plt.title(f'ARIMA{best_order} Forecasting with Confidence Intervals')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Model diagnostics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 8))\n",
    "best_arima.plot_diagnostics(fig=fig)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate errors\n",
    "arima_mae = mean_absolute_error(test.values, arima_forecast)\n",
    "arima_rmse = np.sqrt(mean_squared_error(test.values, arima_forecast))\n",
    "\n",
    "print(f\"\\nARIMA{best_order} Forecast Accuracy:\")\n",
    "print(f\"MAE:  {arima_mae:.4f}\")\n",
    "print(f\"RMSE: {arima_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 SARIMA - Seasonal ARIMA\n",
    "\n",
    "**SARIMA(p, d, q)(P, D, Q)s** extends ARIMA to handle seasonality.\n",
    "\n",
    "- **(p, d, q)**: Non-seasonal components\n",
    "- **(P, D, Q)s**: Seasonal components with period s\n",
    "\n",
    "**Full SARIMA equation:**\n",
    "\n",
    "$$\\phi_p(B) \\Phi_P(B^s) \\Delta^d \\Delta_s^D y_t = \\theta_q(B) \\Theta_Q(B^s) \\epsilon_t$$\n",
    "\n",
    "where:\n",
    "- $B$ is the backshift operator: $B y_t = y_{t-1}$\n",
    "- $\\phi_p(B)$ is the non-seasonal AR polynomial\n",
    "- $\\Phi_P(B^s)$ is the seasonal AR polynomial\n",
    "- $\\theta_q(B)$ is the non-seasonal MA polynomial\n",
    "- $\\Theta_Q(B^s)$ is the seasonal MA polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit SARIMA model\n",
    "# We know our data has seasonality with period 50\n",
    "seasonal_order = (1, 1, 1, 50)  # (P, D, Q, s)\n",
    "order = (1, 1, 1)  # (p, d, q)\n",
    "\n",
    "print(f\"Fitting SARIMA{order}x{seasonal_order} model...\")\n",
    "sarima_model = SARIMAX(train, order=order, seasonal_order=seasonal_order)\n",
    "sarima_fitted = sarima_model.fit(disp=False)\n",
    "\n",
    "print(\"\\nSARIMA Model Summary:\")\n",
    "print(sarima_fitted.summary())\n",
    "\n",
    "# Forecast\n",
    "sarima_forecast = sarima_fitted.forecast(steps=50)\n",
    "sarima_forecast_obj = sarima_fitted.get_forecast(steps=50)\n",
    "sarima_forecast_df = sarima_forecast_obj.summary_frame(alpha=0.05)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(train.index, train.values, label='Training Data', alpha=0.7)\n",
    "plt.plot(test.index, test.values, label='Actual', color='black', linewidth=2)\n",
    "plt.plot(test.index, sarima_forecast, label=f'SARIMA{order}x{seasonal_order}', \n",
    "         linestyle='--', linewidth=2, color='green')\n",
    "\n",
    "# Plot confidence intervals\n",
    "plt.fill_between(test.index, \n",
    "                 sarima_forecast_df['mean_ci_lower'].values, \n",
    "                 sarima_forecast_df['mean_ci_upper'].values, \n",
    "                 alpha=0.2, color='green', label='95% CI')\n",
    "\n",
    "plt.axvline(test.index[0], color='red', linestyle=':', alpha=0.5, label='Forecast Start')\n",
    "plt.title(f'SARIMA Forecasting with Seasonal Components')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Calculate errors\n",
    "sarima_mae = mean_absolute_error(test.values, sarima_forecast)\n",
    "sarima_rmse = np.sqrt(mean_squared_error(test.values, sarima_forecast))\n",
    "\n",
    "print(f\"\\nSARIMA Forecast Accuracy:\")\n",
    "print(f\"MAE:  {sarima_mae:.4f}\")\n",
    "print(f\"RMSE: {sarima_rmse:.4f}\")\n",
    "\n",
    "print(f\"\\nComparison with ARIMA:\")\n",
    "print(f\"ARIMA MAE:  {arima_mae:.4f}\")\n",
    "print(f\"SARIMA MAE: {sarima_mae:.4f}\")\n",
    "print(f\"Improvement: {((arima_mae - sarima_mae) / arima_mae * 100):.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}