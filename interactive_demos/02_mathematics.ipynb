{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“ Mathematics for Machine Learning\n",
    "\n",
    "Welcome to the mathematical foundations of ML! This notebook covers essential mathematical concepts with interactive examples.\n",
    "\n",
    "**Learning Goals:**\n",
    "- Master linear algebra fundamentals\n",
    "- Understand calculus for optimization\n",
    "- Apply probability theory\n",
    "- Visualize mathematical concepts\n",
    "\n",
    "**Sources:**\n",
    "- \"Mathematics for Machine Learning\" - Deisenroth, Faisal, Ong (2020)\n",
    "- \"Deep Learning\" - Goodfellow, Bengio, Courville, Chapter 2-4\n",
    "- \"Pattern Recognition and Machine Learning\" - Bishop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import linalg\n",
    "import pandas as pd\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "print(\"âœ… Libraries loaded successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¢ Part 1: Linear Algebra - The Language of ML\n",
    "\n",
    "Linear algebra is fundamental to ML. Every dataset, model, and transformation uses vectors and matrices!\n",
    "\n",
    "### Why Linear Algebra Matters:\n",
    "- **Data representation**: Each data point is a vector\n",
    "- **Model parameters**: Weights are matrices\n",
    "- **Transformations**: Matrix operations transform data\n",
    "- **Efficiency**: Vectorization speeds up computation\n",
    "\n",
    "**Source:** \"Mathematics for Machine Learning\" Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Vectors: Building Blocks of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectors represent data points\n",
    "# Example: A house with [size, bedrooms, age]\n",
    "house_1 = np.array([1200, 3, 5])  # 1200 sqft, 3 bedrooms, 5 years old\n",
    "house_2 = np.array([1800, 4, 2])  # 1800 sqft, 4 bedrooms, 2 years old\n",
    "\n",
    "print(\"ðŸ  House 1 (vector):\", house_1)\n",
    "print(\"ðŸ  House 2 (vector):\", house_2)\n",
    "print(\"\\nðŸ“ Vector properties:\")\n",
    "print(f\"  Dimension: {house_1.shape}\")\n",
    "print(f\"  Length (L2 norm): {np.linalg.norm(house_1):.2f}\")\n",
    "\n",
    "# Vector operations\n",
    "print(\"\\nâž• Vector Addition (combining features):\")\n",
    "total = house_1 + house_2\n",
    "print(f\"  Sum: {total}\")\n",
    "\n",
    "print(\"\\nâœ–ï¸ Scalar Multiplication (scaling):\")\n",
    "scaled = 2 * house_1\n",
    "print(f\"  2 Ã— house_1 = {scaled}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Dot Product (similarity measure):\")\n",
    "similarity = np.dot(house_1, house_2)\n",
    "print(f\"  house_1 Â· house_2 = {similarity}\")\n",
    "print(\"  â†’ Larger dot product = more similar houses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing vectors in 2D\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Simple 2D vectors for visualization\n",
    "v1 = np.array([3, 2])\n",
    "v2 = np.array([1, 3])\n",
    "\n",
    "# Plot 1: Individual vectors\n",
    "axes[0].quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, color='blue', width=0.008, label='v1')\n",
    "axes[0].quiver(0, 0, v2[0], v2[1], angles='xy', scale_units='xy', scale=1, color='red', width=0.008, label='v2')\n",
    "axes[0].set_xlim(-1, 5)\n",
    "axes[0].set_ylim(-1, 5)\n",
    "axes[0].set_aspect('equal')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "axes[0].set_title('Original Vectors')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "\n",
    "# Plot 2: Vector addition\n",
    "v_sum = v1 + v2\n",
    "axes[1].quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, color='blue', width=0.008, alpha=0.5, label='v1')\n",
    "axes[1].quiver(v1[0], v1[1], v2[0], v2[1], angles='xy', scale_units='xy', scale=1, color='red', width=0.008, alpha=0.5, label='v2')\n",
    "axes[1].quiver(0, 0, v_sum[0], v_sum[1], angles='xy', scale_units='xy', scale=1, color='green', width=0.01, label='v1 + v2')\n",
    "axes[1].set_xlim(-1, 5)\n",
    "axes[1].set_ylim(-1, 6)\n",
    "axes[1].set_aspect('equal')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "axes[1].set_title('Vector Addition')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('y')\n",
    "\n",
    "# Plot 3: Scalar multiplication\n",
    "v_scaled = 1.5 * v1\n",
    "axes[2].quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, color='blue', width=0.008, alpha=0.5, label='v1')\n",
    "axes[2].quiver(0, 0, v_scaled[0], v_scaled[1], angles='xy', scale_units='xy', scale=1, color='purple', width=0.01, label='1.5 Ã— v1')\n",
    "axes[2].set_xlim(-1, 6)\n",
    "axes[2].set_ylim(-1, 5)\n",
    "axes[2].set_aspect('equal')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].legend()\n",
    "axes[2].set_title('Scalar Multiplication')\n",
    "axes[2].set_xlabel('x')\n",
    "axes[2].set_ylabel('y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¡ Key Insight: Vector operations preserve geometric relationships!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Matrices: Organizing Data and Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix: Collection of data points (each row = one data point)\n",
    "houses = np.array([\n",
    "    [1200, 3, 5],   # House 1\n",
    "    [1800, 4, 2],   # House 2\n",
    "    [1500, 3, 8],   # House 3\n",
    "    [2200, 5, 1],   # House 4\n",
    "    [1000, 2, 10]   # House 5\n",
    "])\n",
    "\n",
    "print(\"ðŸ˜ï¸ Housing Dataset (Matrix):\")\n",
    "print(houses)\n",
    "print(f\"\\nðŸ“ Shape: {houses.shape} (5 houses, 3 features)\")\n",
    "print(f\"\\nðŸ“Š Column means (average per feature):\")\n",
    "print(f\"  Avg size: {houses[:, 0].mean():.0f} sqft\")\n",
    "print(f\"  Avg bedrooms: {houses[:, 1].mean():.1f}\")\n",
    "print(f\"  Avg age: {houses[:, 2].mean():.1f} years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix operations in ML\n",
    "print(\"ðŸ”§ Common Matrix Operations in ML:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Transpose (swap rows and columns)\n",
    "print(\"\\n1ï¸âƒ£ Transpose (features Ã— samples):\")\n",
    "houses_T = houses.T\n",
    "print(f\"   Original shape: {houses.shape}\")\n",
    "print(f\"   Transposed shape: {houses_T.shape}\")\n",
    "print(houses_T)\n",
    "\n",
    "# 2. Matrix-vector multiplication (applying weights)\n",
    "print(\"\\n2ï¸âƒ£ Matrix-Vector Multiplication (prediction):\")\n",
    "weights = np.array([0.1, 50, -10])  # $ per sqft, $ per bedroom, $ per year\n",
    "prices = houses @ weights  # @ is matrix multiplication\n",
    "print(f\"   Weights: {weights}\")\n",
    "print(f\"   Predicted prices: {prices}\")\n",
    "print(\"   â†’ This is how linear regression makes predictions!\")\n",
    "\n",
    "# 3. Matrix-matrix multiplication\n",
    "print(\"\\n3ï¸âƒ£ Matrix-Matrix Multiplication (neural network layer):\")\n",
    "W = np.random.randn(3, 2)  # Weight matrix (3 input features â†’ 2 hidden units)\n",
    "hidden = houses @ W\n",
    "print(f\"   Input shape: {houses.shape}\")\n",
    "print(f\"   Weight shape: {W.shape}\")\n",
    "print(f\"   Output shape: {hidden.shape}\")\n",
    "print(\"   â†’ This is a single layer in a neural network!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing matrix transformations\n",
    "print(\"ðŸŽ¨ Visualizing Matrix Transformations\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a simple 2D dataset (square)\n",
    "square = np.array([\n",
    "    [0, 0],\n",
    "    [1, 0],\n",
    "    [1, 1],\n",
    "    [0, 1],\n",
    "    [0, 0]  # Close the square\n",
    "])\n",
    "\n",
    "# Different transformation matrices\n",
    "transformations = {\n",
    "    'Scaling': np.array([[2, 0], [0, 2]]),\n",
    "    'Rotation (45Â°)': np.array([[np.cos(np.pi/4), -np.sin(np.pi/4)],\n",
    "                                [np.sin(np.pi/4), np.cos(np.pi/4)]]),\n",
    "    'Shear': np.array([[1, 0.5], [0, 1]]),\n",
    "    'Reflection': np.array([[-1, 0], [0, 1]])\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, matrix) in enumerate(transformations.items()):\n",
    "    # Apply transformation\n",
    "    transformed = square @ matrix.T\n",
    "    \n",
    "    # Plot\n",
    "    axes[idx].plot(square[:, 0], square[:, 1], 'b-o', linewidth=2, \n",
    "                   markersize=8, label='Original', alpha=0.5)\n",
    "    axes[idx].plot(transformed[:, 0], transformed[:, 1], 'r-o', \n",
    "                   linewidth=2, markersize=8, label='Transformed')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].set_aspect('equal')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].set_title(f'{name}\\n{matrix[0]} \\n{matrix[1]}')\n",
    "    axes[idx].set_xlim(-2, 3)\n",
    "    axes[idx].set_ylim(-2, 3)\n",
    "    axes[idx].axhline(y=0, color='k', linewidth=0.5)\n",
    "    axes[idx].axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Every transformation in ML is a matrix multiplication!\")\n",
    "print(\"   - PCA: Rotation to new axes\")\n",
    "print(\"   - Neural networks: Series of transformations\")\n",
    "print(\"   - Image processing: Convolution matrices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Eigenvalues & Eigenvectors: Understanding Data Structure\n",
    "\n",
    "**Intuition:** Eigenvectors show the \"principal directions\" of a transformation, and eigenvalues show how much stretching happens in those directions.\n",
    "\n",
    "**ML Applications:**\n",
    "- PCA (Principal Component Analysis)\n",
    "- Covariance matrices\n",
    "- Spectral clustering\n",
    "\n",
    "**Source:** \"Mathematics for Machine Learning\" Chapter 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate correlated data\n",
    "np.random.seed(42)\n",
    "mean = [0, 0]\n",
    "cov = [[3, 1.5], [1.5, 1]]  # Covariance matrix\n",
    "data = np.random.multivariate_normal(mean, cov, 300)\n",
    "\n",
    "# Compute eigenvalues and eigenvectors of covariance matrix\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov)\n",
    "\n",
    "print(\"ðŸ” Eigenanalysis of Data:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nCovariance Matrix:\")\n",
    "print(cov)\n",
    "print(f\"\\nðŸ“Š Eigenvalues: {eigenvalues}\")\n",
    "print(f\"   â†’ These show variance in principal directions\")\n",
    "print(f\"\\nðŸ§­ Eigenvectors:\")\n",
    "print(eigenvectors)\n",
    "print(f\"   â†’ These show the principal directions\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(data[:, 0], data[:, 1], alpha=0.5, label='Data points')\n",
    "\n",
    "# Plot eigenvectors scaled by eigenvalues\n",
    "origin = np.array([[0, 0], [0, 0]])\n",
    "for i in range(2):\n",
    "    eigvec = eigenvectors[:, i] * np.sqrt(eigenvalues[i]) * 2\n",
    "    plt.quiver(0, 0, eigvec[0], eigvec[1], \n",
    "               angles='xy', scale_units='xy', scale=1,\n",
    "               color=['red', 'blue'][i], width=0.01,\n",
    "               label=f'Eigenvector {i+1} (Î»={eigenvalues[i]:.2f})')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.legend()\n",
    "plt.title('Principal Component Analysis (PCA)\\nEigenvectors show directions of maximum variance')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ PCA finds these eigenvectors to reduce dimensionality!\")\n",
    "print(\"   The first eigenvector (red) captures most variance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Part 2: Calculus - The Language of Optimization\n",
    "\n",
    "Machine learning is optimization! We use calculus to find the best model parameters.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Derivatives**: Rate of change (slopes)\n",
    "- **Gradients**: Direction of steepest ascent\n",
    "- **Optimization**: Finding minimum/maximum values\n",
    "- **Gradient Descent**: The core ML training algorithm\n",
    "\n",
    "**Source:** \"Deep Learning\" - Goodfellow et al., Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Derivatives: Understanding Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Cost function for a simple model\n",
    "def cost_function(w):\n",
    "    \"\"\"Simple quadratic cost function: (w - 3)^2 + 5\"\"\"\n",
    "    return (w - 3)**2 + 5\n",
    "\n",
    "def derivative(w):\n",
    "    \"\"\"Derivative of cost function: 2(w - 3)\"\"\"\n",
    "    return 2 * (w - 3)\n",
    "\n",
    "# Visualize function and its derivative\n",
    "w_values = np.linspace(-2, 8, 100)\n",
    "cost_values = cost_function(w_values)\n",
    "deriv_values = derivative(w_values)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Cost function\n",
    "ax1.plot(w_values, cost_values, 'b-', linewidth=2, label='Cost function')\n",
    "ax1.plot(3, 5, 'ro', markersize=15, label='Minimum (w=3)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(fontsize=12)\n",
    "ax1.set_xlabel('Weight (w)', fontsize=12)\n",
    "ax1.set_ylabel('Cost', fontsize=12)\n",
    "ax1.set_title('Cost Function: Goal is to find minimum', fontsize=14)\n",
    "\n",
    "# Plot 2: Derivative (slope)\n",
    "ax2.plot(w_values, deriv_values, 'g-', linewidth=2, label='Derivative (slope)')\n",
    "ax2.axhline(y=0, color='r', linestyle='--', linewidth=2, label='Zero derivative = minimum')\n",
    "ax2.plot(3, 0, 'ro', markersize=15)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(fontsize=12)\n",
    "ax2.set_xlabel('Weight (w)', fontsize=12)\n",
    "ax2.set_ylabel('Derivative', fontsize=12)\n",
    "ax2.set_title('Derivative: Shows direction to move', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Key Insights:\")\n",
    "print(\"  â€¢ Derivative > 0 â†’ Function increasing â†’ Move left (decrease w)\")\n",
    "print(\"  â€¢ Derivative < 0 â†’ Function decreasing â†’ Move right (increase w)\")\n",
    "print(\"  â€¢ Derivative = 0 â†’ At minimum/maximum â†’ Stop!\")\n",
    "print(\"\\nðŸ’¡ This is the foundation of gradient descent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Gradient Descent: Training ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement gradient descent from scratch\n",
    "def gradient_descent(starting_point, learning_rate, num_iterations):\n",
    "    \"\"\"\n",
    "    Find minimum of cost function using gradient descent\n",
    "    \n",
    "    Parameters:\n",
    "    - starting_point: Initial weight value\n",
    "    - learning_rate: Step size (how far to move)\n",
    "    - num_iterations: Number of steps to take\n",
    "    \"\"\"\n",
    "    w = starting_point\n",
    "    history = [w]\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Calculate gradient (derivative)\n",
    "        grad = derivative(w)\n",
    "        \n",
    "        # Update weight: move opposite to gradient\n",
    "        w = w - learning_rate * grad\n",
    "        history.append(w)\n",
    "        \n",
    "        if i < 5 or i == num_iterations - 1:\n",
    "            print(f\"Iteration {i+1}: w={w:.4f}, cost={cost_function(w):.4f}, gradient={grad:.4f}\")\n",
    "    \n",
    "    return w, history\n",
    "\n",
    "print(\"ðŸŽ¯ Gradient Descent in Action!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nStarting from w=7 (far from optimal w=3)...\\n\")\n",
    "\n",
    "final_w, history = gradient_descent(starting_point=7, learning_rate=0.1, num_iterations=20)\n",
    "\n",
    "print(f\"\\nâœ… Converged to w={final_w:.4f} (optimal is w=3.0)\")\n",
    "print(f\"   Final cost: {cost_function(final_w):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient descent path\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Path on cost function\n",
    "ax1 = fig.add_subplot(121)\n",
    "w_values = np.linspace(-1, 8, 100)\n",
    "ax1.plot(w_values, cost_function(w_values), 'b-', linewidth=2, label='Cost function')\n",
    "\n",
    "# Plot gradient descent steps\n",
    "history_costs = [cost_function(w) for w in history]\n",
    "ax1.plot(history, history_costs, 'ro-', linewidth=2, markersize=8, \n",
    "         label='Gradient descent path', alpha=0.7)\n",
    "ax1.plot(history[0], history_costs[0], 'go', markersize=15, label='Start')\n",
    "ax1.plot(history[-1], history_costs[-1], 'r*', markersize=20, label='End')\n",
    "\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.set_xlabel('Weight (w)', fontsize=12)\n",
    "ax1.set_ylabel('Cost', fontsize=12)\n",
    "ax1.set_title('Gradient Descent Path', fontsize=14)\n",
    "\n",
    "# Plot 2: Cost over iterations\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.plot(range(len(history_costs)), history_costs, 'b-o', linewidth=2, markersize=6)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlabel('Iteration', fontsize=12)\n",
    "ax2.set_ylabel('Cost', fontsize=12)\n",
    "ax2.set_title('Cost Reduction Over Time', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ This is EXACTLY how neural networks learn!\")\n",
    "print(\"   1. Compute gradient (derivative of cost w.r.t. weights)\")\n",
    "print(\"   2. Update weights in opposite direction of gradient\")\n",
    "print(\"   3. Repeat until convergence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Partial Derivatives & Gradients: Multiple Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D cost function (2 weights)\n",
    "def cost_3d(w1, w2):\n",
    "    \"\"\"Bowl-shaped cost function\"\"\"\n",
    "    return (w1 - 2)**2 + (w2 - 1)**2 + 5\n",
    "\n",
    "def gradient_3d(w1, w2):\n",
    "    \"\"\"Gradient vector [âˆ‚C/âˆ‚w1, âˆ‚C/âˆ‚w2]\"\"\"\n",
    "    dw1 = 2 * (w1 - 2)\n",
    "    dw2 = 2 * (w2 - 1)\n",
    "    return np.array([dw1, dw2])\n",
    "\n",
    "# Visualize 3D cost surface\n",
    "w1_range = np.linspace(-1, 5, 50)\n",
    "w2_range = np.linspace(-2, 4, 50)\n",
    "W1, W2 = np.meshgrid(w1_range, w2_range)\n",
    "Z = cost_3d(W1, W2)\n",
    "\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "\n",
    "# 3D surface plot\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "surf = ax1.plot_surface(W1, W2, Z, cmap='viridis', alpha=0.8)\n",
    "ax1.set_xlabel('w1')\n",
    "ax1.set_ylabel('w2')\n",
    "ax1.set_zlabel('Cost')\n",
    "ax1.set_title('3D Cost Surface')\n",
    "fig.colorbar(surf, ax=ax1, shrink=0.5)\n",
    "\n",
    "# Contour plot\n",
    "ax2 = fig.add_subplot(132)\n",
    "contour = ax2.contour(W1, W2, Z, levels=20, cmap='viridis')\n",
    "ax2.plot(2, 1, 'r*', markersize=20, label='Minimum (2, 1)')\n",
    "ax2.set_xlabel('w1')\n",
    "ax2.set_ylabel('w2')\n",
    "ax2.set_title('Contour Plot (Top View)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "fig.colorbar(contour, ax=ax2)\n",
    "\n",
    "# Gradient vectors\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.contour(W1, W2, Z, levels=20, cmap='viridis', alpha=0.3)\n",
    "\n",
    "# Sample points and their gradients\n",
    "sample_points = np.array([[4, 3], [0, 2], [3, -1], [1, 0]])\n",
    "for point in sample_points:\n",
    "    grad = gradient_3d(point[0], point[1])\n",
    "    # Normalize for visualization\n",
    "    grad_norm = grad / (np.linalg.norm(grad) + 1e-8) * 0.5\n",
    "    ax3.arrow(point[0], point[1], -grad_norm[0], -grad_norm[1],\n",
    "             head_width=0.2, head_length=0.15, fc='red', ec='red', linewidth=2)\n",
    "    ax3.plot(point[0], point[1], 'ro', markersize=8)\n",
    "\n",
    "ax3.plot(2, 1, 'g*', markersize=20, label='Minimum')\n",
    "ax3.set_xlabel('w1')\n",
    "ax3.set_ylabel('w2')\n",
    "ax3.set_title('Gradient Vectors\\n(Red arrows point toward minimum)')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_xlim(-1, 5)\n",
    "ax3.set_ylim(-2, 4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ§­ Gradient Points Downhill!\")\n",
    "print(\"  â€¢ Gradient = [âˆ‚C/âˆ‚w1, âˆ‚C/âˆ‚w2] = vector of partial derivatives\")\n",
    "print(\"  â€¢ Points in direction of steepest ascent\")\n",
    "print(\"  â€¢ We move OPPOSITE to gradient (downhill) to minimize cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ² Part 3: Probability & Statistics\n",
    "\n",
    "ML deals with uncertainty! Probability theory helps us model and reason about randomness.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Probability distributions\n",
    "- Expected values\n",
    "- Variance and standard deviation\n",
    "- Bayes' theorem\n",
    "\n",
    "**Source:** \"Pattern Recognition and Machine Learning\" - Bishop, Chapter 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common probability distributions in ML\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# 1. Normal/Gaussian Distribution (most important!)\n",
    "x = np.linspace(-5, 5, 100)\n",
    "for mu, sigma in [(0, 1), (0, 0.5), (0, 2)]:\n",
    "    y = (1/(sigma * np.sqrt(2*np.pi))) * np.exp(-0.5*((x-mu)/sigma)**2)\n",
    "    axes[0,0].plot(x, y, linewidth=2, label=f'Î¼={mu}, Ïƒ={sigma}')\n",
    "axes[0,0].set_title('Normal Distribution\\n(Most features, errors, noise)')\n",
    "axes[0,0].set_xlabel('x')\n",
    "axes[0,0].set_ylabel('Probability Density')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Bernoulli Distribution (binary outcomes)\n",
    "p_values = [0.3, 0.5, 0.7]\n",
    "x_pos = np.arange(2)\n",
    "width = 0.25\n",
    "for i, p in enumerate(p_values):\n",
    "    axes[0,1].bar(x_pos + i*width, [1-p, p], width, label=f'p={p}', alpha=0.7)\n",
    "axes[0,1].set_title('Bernoulli Distribution\\n(Binary classification)')\n",
    "axes[0,1].set_xlabel('Outcome')\n",
    "axes[0,1].set_ylabel('Probability')\n",
    "axes[0,1].set_xticks(x_pos + width)\n",
    "axes[0,1].set_xticklabels(['0 (Failure)', '1 (Success)'])\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Uniform Distribution\n",
    "a, b = -2, 2\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = np.where((x >= a) & (x <= b), 1/(b-a), 0)\n",
    "axes[0,2].plot(x, y, linewidth=2)\n",
    "axes[0,2].fill_between(x, y, alpha=0.3)\n",
    "axes[0,2].set_title('Uniform Distribution\\n(Random initialization)')\n",
    "axes[0,2].set_xlabel('x')\n",
    "axes[0,2].set_ylabel('Probability Density')\n",
    "axes[0,2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Exponential Distribution\n",
    "x = np.linspace(0, 5, 100)\n",
    "for lam in [0.5, 1, 2]:\n",
    "    y = lam * np.exp(-lam * x)\n",
    "    axes[1,0].plot(x, y, linewidth=2, label=f'Î»={lam}')\n",
    "axes[1,0].set_title('Exponential Distribution\\n(Time between events)')\n",
    "axes[1,0].set_xlabel('x')\n",
    "axes[1,0].set_ylabel('Probability Density')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Poisson Distribution\n",
    "x = np.arange(0, 15)\n",
    "for lam in [1, 4, 8]:\n",
    "    y = (lam**x * np.exp(-lam)) / np.array([np.math.factorial(i) for i in x])\n",
    "    axes[1,1].plot(x, y, 'o-', linewidth=2, label=f'Î»={lam}')\n",
    "axes[1,1].set_title('Poisson Distribution\\n(Count data)')\n",
    "axes[1,1].set_xlabel('k (number of events)')\n",
    "axes[1,1].set_ylabel('Probability')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Beta Distribution\n",
    "from scipy.stats import beta\n",
    "x = np.linspace(0, 1, 100)\n",
    "for a, b in [(0.5, 0.5), (2, 2), (5, 2)]:\n",
    "    y = beta.pdf(x, a, b)\n",
    "    axes[1,2].plot(x, y, linewidth=2, label=f'Î±={a}, Î²={b}')\n",
    "axes[1,2].set_title('Beta Distribution\\n(Probabilities as outputs)')\n",
    "axes[1,2].set_xlabel('x')\n",
    "axes[1,2].set_ylabel('Probability Density')\n",
    "axes[1,2].legend()\n",
    "axes[1,2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Distribution Use Cases in ML:\")\n",
    "print(\"  â€¢ Normal: Feature distributions, weight initialization, errors\")\n",
    "print(\"  â€¢ Bernoulli: Binary classification outputs\")\n",
    "print(\"  â€¢ Uniform: Random weight initialization\")\n",
    "print(\"  â€¢ Exponential: Time series, survival analysis\")\n",
    "print(\"  â€¢ Poisson: Count predictions (traffic, customers)\")\n",
    "print(\"  â€¢ Beta: Bayesian inference, probability priors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Bayes' Theorem: The Foundation of Probabilistic ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical example: Medical diagnosis\n",
    "print(\"ðŸ¥ Bayes' Theorem in Action: Medical Diagnosis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Given information\n",
    "P_disease = 0.01  # 1% of population has disease (prior)\n",
    "P_positive_given_disease = 0.95  # Test is 95% sensitive (true positive rate)\n",
    "P_positive_given_healthy = 0.10  # Test has 10% false positive rate\n",
    "\n",
    "# Calculate P(positive) using law of total probability\n",
    "P_positive = (P_positive_given_disease * P_disease + \n",
    "              P_positive_given_healthy * (1 - P_disease))\n",
    "\n",
    "# Bayes' theorem: P(disease | positive test)\n",
    "P_disease_given_positive = (P_positive_given_disease * P_disease) / P_positive\n",
    "\n",
    "print(f\"\\nðŸ“Š Given Information:\")\n",
    "print(f\"  â€¢ Disease prevalence: {P_disease:.1%}\")\n",
    "print(f\"  â€¢ Test sensitivity: {P_positive_given_disease:.1%}\")\n",
    "print(f\"  â€¢ False positive rate: {P_positive_given_healthy:.1%}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Bayes' Theorem Result:\")\n",
    "print(f\"  If you test positive, probability of having disease: {P_disease_given_positive:.1%}\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Surprising insight:\")\n",
    "print(f\"  Even with a positive test, only {P_disease_given_positive:.1%} chance of disease!\")\n",
    "print(f\"  Why? The disease is rare (prior probability is low)\")\n",
    "\n",
    "# Visualize Bayes' theorem\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Visualization 1: Tree diagram\n",
    "ax1 = axes[0]\n",
    "ax1.text(0.1, 0.5, 'Population', fontsize=14, fontweight='bold', ha='center')\n",
    "ax1.arrow(0.15, 0.5, 0.15, 0.2, head_width=0.02, head_length=0.05, fc='blue', ec='blue')\n",
    "ax1.arrow(0.15, 0.5, 0.15, -0.2, head_width=0.02, head_length=0.05, fc='green', ec='green')\n",
    "\n",
    "ax1.text(0.35, 0.75, f'Disease\\n{P_disease:.1%}', fontsize=12, ha='center', \n",
    "         bbox=dict(boxstyle='round', facecolor='lightblue'))\n",
    "ax1.text(0.35, 0.25, f'Healthy\\n{1-P_disease:.1%}', fontsize=12, ha='center',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightgreen'))\n",
    "\n",
    "ax1.arrow(0.45, 0.75, 0.15, 0.05, head_width=0.02, head_length=0.03, fc='red', ec='red')\n",
    "ax1.arrow(0.45, 0.25, 0.15, 0.05, head_width=0.02, head_length=0.03, fc='orange', ec='orange')\n",
    "\n",
    "ax1.text(0.7, 0.82, f'Test +\\n{P_positive_given_disease:.1%}', fontsize=11, ha='center',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightcoral'))\n",
    "ax1.text(0.7, 0.32, f'Test +\\n{P_positive_given_healthy:.1%}', fontsize=11, ha='center',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightyellow'))\n",
    "\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Probability Tree', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Visualization 2: Bar chart\n",
    "ax2 = axes[1]\n",
    "categories = ['Prior\\nP(Disease)', 'Likelihood\\nP(+|Disease)', 'Posterior\\nP(Disease|+)']\n",
    "values = [P_disease, P_positive_given_disease, P_disease_given_positive]\n",
    "colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
    "\n",
    "bars = ax2.bar(categories, values, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "for bar, value in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "             f'{value:.1%}', ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "\n",
    "ax2.set_ylabel('Probability', fontsize=12)\n",
    "ax2.set_title('Bayes\\' Theorem: Updating Beliefs', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "ax2.set_ylim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ§  ML Applications of Bayes' Theorem:\")\n",
    "print(\"  â€¢ Naive Bayes classifier\")\n",
    "print(\"  â€¢ Bayesian neural networks\")\n",
    "print(\"  â€¢ Spam filters\")\n",
    "print(\"  â€¢ Reinforcement learning (belief updates)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ® Interactive Exercises\n",
    "\n",
    "Now it's your turn! Try these exercises to solidify your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Vector Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this exercise\n",
    "\n",
    "# Given two vectors representing houses:\n",
    "house_a = np.array([2000, 4, 3])  # [sqft, bedrooms, age]\n",
    "house_b = np.array([1500, 3, 7])\n",
    "\n",
    "# Task 1: Calculate the Euclidean distance between houses\n",
    "# Hint: Use np.linalg.norm() or compute sqrt(sum((a-b)^2))\n",
    "distance = None  # YOUR CODE HERE\n",
    "\n",
    "# Task 2: Normalize house_a to unit length\n",
    "# Hint: Divide by the vector's norm\n",
    "house_a_normalized = None  # YOUR CODE HERE\n",
    "\n",
    "# Task 3: Calculate cosine similarity\n",
    "# Hint: (a Â· b) / (||a|| ||b||)\n",
    "cosine_similarity = None  # YOUR CODE HERE\n",
    "\n",
    "# Check your answers\n",
    "print(\"âœ… Solutions:\")\n",
    "print(f\"Distance: {distance}\")\n",
    "print(f\"Normalized house_a: {house_a_normalized}\")\n",
    "print(f\"Cosine similarity: {cosine_similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this exercise\n",
    "\n",
    "# Dataset: 4 samples, 3 features\n",
    "X = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9],\n",
    "    [10, 11, 12]\n",
    "])\n",
    "\n",
    "# Weight matrix: 3 inputs -> 2 outputs\n",
    "W = np.array([\n",
    "    [0.1, 0.2],\n",
    "    [0.3, 0.4],\n",
    "    [0.5, 0.6]\n",
    "])\n",
    "\n",
    "# Task 1: Multiply X and W to get predictions\n",
    "# What should be the output shape?\n",
    "predictions = None  # YOUR CODE HERE\n",
    "\n",
    "# Task 2: Calculate the mean of each feature (column) in X\n",
    "feature_means = None  # YOUR CODE HERE\n",
    "\n",
    "# Task 3: Center the data by subtracting means\n",
    "X_centered = None  # YOUR CODE HERE\n",
    "\n",
    "print(\"âœ… Solutions:\")\n",
    "print(f\"Predictions shape: {predictions.shape if predictions is not None else 'Not computed'}\")\n",
    "print(f\"Feature means: {feature_means}\")\n",
    "print(f\"Centered data:\\n{X_centered}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Implement Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement gradient descent for a different function\n",
    "\n",
    "def new_cost_function(w):\n",
    "    \"\"\"Cost function: w^3 - 2w^2 + 5\"\"\"\n",
    "    return w**3 - 2*w**2 + 5\n",
    "\n",
    "def new_derivative(w):\n",
    "    \"\"\"Derivative: 3w^2 - 4w\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return None\n",
    "\n",
    "def gradient_descent_exercise(start, lr, iterations):\n",
    "    \"\"\"\n",
    "    Implement gradient descent\n",
    "    \n",
    "    Parameters:\n",
    "    - start: starting weight\n",
    "    - lr: learning rate\n",
    "    - iterations: number of steps\n",
    "    \n",
    "    Returns:\n",
    "    - final weight\n",
    "    - history of weights\n",
    "    \"\"\"\n",
    "    w = start\n",
    "    history = [w]\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        # YOUR CODE HERE\n",
    "        # 1. Calculate gradient\n",
    "        # 2. Update weight\n",
    "        # 3. Append to history\n",
    "        pass\n",
    "    \n",
    "    return w, history\n",
    "\n",
    "# Test your implementation\n",
    "final, hist = gradient_descent_exercise(start=2.0, lr=0.01, iterations=100)\n",
    "print(f\"Final weight: {final}\")\n",
    "print(f\"Final cost: {new_cost_function(final)}\")\n",
    "\n",
    "# Visualize\n",
    "if hist is not None and len(hist) > 1:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot cost function\n",
    "    plt.subplot(121)\n",
    "    w_vals = np.linspace(-1, 3, 100)\n",
    "    plt.plot(w_vals, new_cost_function(w_vals), 'b-', linewidth=2)\n",
    "    plt.plot(hist, [new_cost_function(w) for w in hist], 'ro-', alpha=0.6)\n",
    "    plt.xlabel('w')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.title('Gradient Descent Path')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot convergence\n",
    "    plt.subplot(122)\n",
    "    plt.plot([new_cost_function(w) for w in hist], 'b-o')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.title('Cost vs Iteration')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Summary & Next Steps\n",
    "\n",
    "### âœ… What You've Learned:\n",
    "\n",
    "**Linear Algebra:**\n",
    "- Vectors represent data points\n",
    "- Matrices organize datasets and transformations\n",
    "- Eigenvalues/eigenvectors reveal data structure\n",
    "- Matrix operations are the foundation of neural networks\n",
    "\n",
    "**Calculus:**\n",
    "- Derivatives measure rate of change\n",
    "- Gradients point uphill (we go opposite direction)\n",
    "- Gradient descent is the core ML training algorithm\n",
    "- Chain rule enables backpropagation\n",
    "\n",
    "**Probability:**\n",
    "- Distributions model uncertainty\n",
    "- Bayes' theorem updates beliefs with evidence\n",
    "- Expected values guide decision making\n",
    "- Variance measures spread/uncertainty\n",
    "\n",
    "### ðŸ“š Key Formulas:\n",
    "\n",
    "1. **Dot Product**: $\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i} a_i b_i$\n",
    "\n",
    "2. **Matrix Multiplication**: $(AB)_{ij} = \\sum_{k} A_{ik} B_{kj}$\n",
    "\n",
    "3. **Gradient Descent**: $w_{t+1} = w_t - \\alpha \\nabla f(w_t)$\n",
    "\n",
    "4. **Bayes' Theorem**: $P(A|B) = \\frac{P(B|A) P(A)}{P(B)}$\n",
    "\n",
    "### ðŸš€ Next Steps:\n",
    "\n",
    "1. **[Statistics Fundamentals](03_statistics.ipynb)** - Dive deeper into statistical inference\n",
    "2. **[Data Processing](04_data_processing.ipynb)** - Learn to clean and transform data\n",
    "3. **[Classical ML](05_classical_ml.ipynb)** - Apply math to real algorithms\n",
    "\n",
    "### ðŸ“– Recommended Reading:\n",
    "\n",
    "- \"Mathematics for Machine Learning\" - Deisenroth, Faisal, Ong\n",
    "- \"Deep Learning\" - Goodfellow, Bengio, Courville (Chapters 2-4)\n",
    "- 3Blue1Brown videos on Linear Algebra and Calculus\n",
    "- Khan Academy: Linear Algebra, Calculus, Probability\n",
    "\n",
    "### ðŸ’ª Challenge Problems:\n",
    "\n",
    "1. Implement PCA from scratch using eigendecomposition\n",
    "2. Code a simple neural network using only NumPy\n",
    "3. Derive and implement backpropagation\n",
    "4. Build a Naive Bayes classifier\n",
    "\n",
    "**Remember**: Mathematics is the language of ML. Master these foundations, and everything else becomes easier! ðŸŽ¯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
