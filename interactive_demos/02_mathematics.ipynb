{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìê Mathematics for Machine Learning\n",
    "\n",
    "Welcome to the mathematical foundations of ML! This notebook covers essential mathematical concepts with interactive examples.\n",
    "\n",
    "**Learning Goals:**\n",
    "- Master linear algebra fundamentals\n",
    "- Understand calculus for optimization\n",
    "- Apply probability theory\n",
    "- Visualize mathematical concepts\n",
    "\n",
    "**Sources:**\n",
    "- \"Mathematics for Machine Learning\" - Deisenroth, Faisal, Ong (2020)\n",
    "- \"Deep Learning\" - Goodfellow, Bengio, Courville, Chapter 2-4\n",
    "- \"Pattern Recognition and Machine Learning\" - Bishop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import linalg\n",
    "import pandas as pd\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¢ Part 1: Linear Algebra - The Language of ML\n",
    "\n",
    "Linear algebra is fundamental to ML. Every dataset, model, and transformation uses vectors and matrices!\n",
    "\n",
    "### Why Linear Algebra Matters:\n",
    "- **Data representation**: Each data point is a vector\n",
    "- **Model parameters**: Weights are matrices\n",
    "- **Transformations**: Matrix operations transform data\n",
    "- **Efficiency**: Vectorization speeds up computation\n",
    "\n",
    "**Source:** \"Mathematics for Machine Learning\" Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Vectors: Building Blocks of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectors represent data points\n",
    "# Example: A house with [size, bedrooms, age]\n",
    "house_1 = np.array([1200, 3, 5])  # 1200 sqft, 3 bedrooms, 5 years old\n",
    "house_2 = np.array([1800, 4, 2])  # 1800 sqft, 4 bedrooms, 2 years old\n",
    "\n",
    "print(\"üè† House 1 (vector):\", house_1)\n",
    "print(\"üè† House 2 (vector):\", house_2)\n",
    "print(\"\\nüìè Vector properties:\")\n",
    "print(f\"  Dimension: {house_1.shape}\")\n",
    "print(f\"  Length (L2 norm): {np.linalg.norm(house_1):.2f}\")\n",
    "\n",
    "# Vector operations\n",
    "print(\"\\n‚ûï Vector Addition (combining features):\")\n",
    "total = house_1 + house_2\n",
    "print(f\"  Sum: {total}\")\n",
    "\n",
    "print(\"\\n‚úñÔ∏è Scalar Multiplication (scaling):\")\n",
    "scaled = 2 * house_1\n",
    "print(f\"  2 √ó house_1 = {scaled}\")\n",
    "\n",
    "print(\"\\nüìä Dot Product (similarity measure):\")\n",
    "similarity = np.dot(house_1, house_2)\n",
    "print(f\"  house_1 ¬∑ house_2 = {similarity}\")\n",
    "print(\"  ‚Üí Larger dot product = more similar houses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing vectors in 2D\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Simple 2D vectors for visualization\n",
    "v1 = np.array([3, 2])\n",
    "v2 = np.array([1, 3])\n",
    "\n",
    "# Plot 1: Individual vectors\n",
    "axes[0].quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, color='blue', width=0.008, label='v1')\n",
    "axes[0].quiver(0, 0, v2[0], v2[1], angles='xy', scale_units='xy', scale=1, color='red', width=0.008, label='v2')\n",
    "axes[0].set_xlim(-1, 5)\n",
    "axes[0].set_ylim(-1, 5)\n",
    "axes[0].set_aspect('equal')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "axes[0].set_title('Original Vectors')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "\n",
    "# Plot 2: Vector addition\n",
    "v_sum = v1 + v2\n",
    "axes[1].quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, color='blue', width=0.008, alpha=0.5, label='v1')\n",
    "axes[1].quiver(v1[0], v1[1], v2[0], v2[1], angles='xy', scale_units='xy', scale=1, color='red', width=0.008, alpha=0.5, label='v2')\n",
    "axes[1].quiver(0, 0, v_sum[0], v_sum[1], angles='xy', scale_units='xy', scale=1, color='green', width=0.01, label='v1 + v2')\n",
    "axes[1].set_xlim(-1, 5)\n",
    "axes[1].set_ylim(-1, 6)\n",
    "axes[1].set_aspect('equal')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "axes[1].set_title('Vector Addition')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('y')\n",
    "\n",
    "# Plot 3: Scalar multiplication\n",
    "v_scaled = 1.5 * v1\n",
    "axes[2].quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, color='blue', width=0.008, alpha=0.5, label='v1')\n",
    "axes[2].quiver(0, 0, v_scaled[0], v_scaled[1], angles='xy', scale_units='xy', scale=1, color='purple', width=0.01, label='1.5 √ó v1')\n",
    "axes[2].set_xlim(-1, 6)\n",
    "axes[2].set_ylim(-1, 5)\n",
    "axes[2].set_aspect('equal')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].legend()\n",
    "axes[2].set_title('Scalar Multiplication')\n",
    "axes[2].set_xlabel('x')\n",
    "axes[2].set_ylabel('y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Key Insight: Vector operations preserve geometric relationships!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Matrices: Organizing Data and Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix: Collection of data points (each row = one data point)\n",
    "houses = np.array([\n",
    "    [1200, 3, 5],   # House 1\n",
    "    [1800, 4, 2],   # House 2\n",
    "    [1500, 3, 8],   # House 3\n",
    "    [2200, 5, 1],   # House 4\n",
    "    [1000, 2, 10]   # House 5\n",
    "])\n",
    "\n",
    "print(\"üèòÔ∏è Housing Dataset (Matrix):\")\n",
    "print(houses)\n",
    "print(f\"\\nüìê Shape: {houses.shape} (5 houses, 3 features)\")\n",
    "print(f\"\\nüìä Column means (average per feature):\")\n",
    "print(f\"  Avg size: {houses[:, 0].mean():.0f} sqft\")\n",
    "print(f\"  Avg bedrooms: {houses[:, 1].mean():.1f}\")\n",
    "print(f\"  Avg age: {houses[:, 2].mean():.1f} years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix operations in ML\n",
    "print(\"üîß Common Matrix Operations in ML:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Transpose (swap rows and columns)\n",
    "print(\"\\n1Ô∏è‚É£ Transpose (features √ó samples):\")\n",
    "houses_T = houses.T\n",
    "print(f\"   Original shape: {houses.shape}\")\n",
    "print(f\"   Transposed shape: {houses_T.shape}\")\n",
    "print(houses_T)\n",
    "\n",
    "# 2. Matrix-vector multiplication (applying weights)\n",
    "print(\"\\n2Ô∏è‚É£ Matrix-Vector Multiplication (prediction):\")\n",
    "weights = np.array([0.1, 50, -10])  # $ per sqft, $ per bedroom, $ per year\n",
    "prices = houses @ weights  # @ is matrix multiplication\n",
    "print(f\"   Weights: {weights}\")\n",
    "print(f\"   Predicted prices: {prices}\")\n",
    "print(\"   ‚Üí This is how linear regression makes predictions!\")\n",
    "\n",
    "# 3. Matrix-matrix multiplication\n",
    "print(\"\\n3Ô∏è‚É£ Matrix-Matrix Multiplication (neural network layer):\")\n",
    "W = np.random.randn(3, 2)  # Weight matrix (3 input features ‚Üí 2 hidden units)\n",
    "hidden = houses @ W\n",
    "print(f\"   Input shape: {houses.shape}\")\n",
    "print(f\"   Weight shape: {W.shape}\")\n",
    "print(f\"   Output shape: {hidden.shape}\")\n",
    "print(\"   ‚Üí This is a single layer in a neural network!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing matrix transformations\n",
    "print(\"üé® Visualizing Matrix Transformations\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a simple 2D dataset (square)\n",
    "square = np.array([\n",
    "    [0, 0],\n",
    "    [1, 0],\n",
    "    [1, 1],\n",
    "    [0, 1],\n",
    "    [0, 0]  # Close the square\n",
    "])\n",
    "\n",
    "# Different transformation matrices\n",
    "transformations = {\n",
    "    'Scaling': np.array([[2, 0], [0, 2]]),\n",
    "    'Rotation (45¬∞)': np.array([[np.cos(np.pi/4), -np.sin(np.pi/4)],\n",
    "                                [np.sin(np.pi/4), np.cos(np.pi/4)]]),\n",
    "    'Shear': np.array([[1, 0.5], [0, 1]]),\n",
    "    'Reflection': np.array([[-1, 0], [0, 1]])\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, matrix) in enumerate(transformations.items()):\n",
    "    # Apply transformation\n",
    "    transformed = square @ matrix.T\n",
    "    \n",
    "    # Plot\n",
    "    axes[idx].plot(square[:, 0], square[:, 1], 'b-o', linewidth=2, \n",
    "                   markersize=8, label='Original', alpha=0.5)\n",
    "    axes[idx].plot(transformed[:, 0], transformed[:, 1], 'r-o', \n",
    "                   linewidth=2, markersize=8, label='Transformed')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].set_aspect('equal')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].set_title(f'{name}\\n{matrix[0]} \\n{matrix[1]}')\n",
    "    axes[idx].set_xlim(-2, 3)\n",
    "    axes[idx].set_ylim(-2, 3)\n",
    "    axes[idx].axhline(y=0, color='k', linewidth=0.5)\n",
    "    axes[idx].axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Every transformation in ML is a matrix multiplication!\")\n",
    "print(\"   - PCA: Rotation to new axes\")\n",
    "print(\"   - Neural networks: Series of transformations\")\n",
    "print(\"   - Image processing: Convolution matrices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Eigenvalues & Eigenvectors: Understanding Data Structure\n",
    "\n",
    "**Intuition:** Eigenvectors show the \"principal directions\" of a transformation, and eigenvalues show how much stretching happens in those directions.\n",
    "\n",
    "**ML Applications:**\n",
    "- PCA (Principal Component Analysis)\n",
    "- Covariance matrices\n",
    "- Spectral clustering\n",
    "\n",
    "**Source:** \"Mathematics for Machine Learning\" Chapter 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate correlated data\n",
    "np.random.seed(42)\n",
    "mean = [0, 0]\n",
    "cov = [[3, 1.5], [1.5, 1]]  # Covariance matrix\n",
    "data = np.random.multivariate_normal(mean, cov, 300)\n",
    "\n",
    "# Compute eigenvalues and eigenvectors of covariance matrix\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov)\n",
    "\n",
    "print(\"üîç Eigenanalysis of Data:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nCovariance Matrix:\")\n",
    "print(cov)\n",
    "print(f\"\\nüìä Eigenvalues: {eigenvalues}\")\n",
    "print(f\"   ‚Üí These show variance in principal directions\")\n",
    "print(f\"\\nüß≠ Eigenvectors:\")\n",
    "print(eigenvectors)\n",
    "print(f\"   ‚Üí These show the principal directions\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(data[:, 0], data[:, 1], alpha=0.5, label='Data points')\n",
    "\n",
    "# Plot eigenvectors scaled by eigenvalues\n",
    "origin = np.array([[0, 0], [0, 0]])\n",
    "for i in range(2):\n",
    "    eigvec = eigenvectors[:, i] * np.sqrt(eigenvalues[i]) * 2\n",
    "    plt.quiver(0, 0, eigvec[0], eigvec[1], \n",
    "               angles='xy', scale_units='xy', scale=1,\n",
    "               color=['red', 'blue'][i], width=0.01,\n",
    "               label=f'Eigenvector {i+1} (Œª={eigenvalues[i]:.2f})')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.legend()\n",
    "plt.title('Principal Component Analysis (PCA)\\nEigenvectors show directions of maximum variance')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° PCA finds these eigenvectors to reduce dimensionality!\")\n",
    "print(\"   The first eigenvector (red) captures most variance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Part 2: Calculus - The Language of Optimization\n",
    "\n",
    "Machine learning is optimization! We use calculus to find the best model parameters.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Derivatives**: Rate of change (slopes)\n",
    "- **Gradients**: Direction of steepest ascent\n",
    "- **Optimization**: Finding minimum/maximum values\n",
    "- **Gradient Descent**: The core ML training algorithm\n",
    "\n",
    "**Source:** \"Deep Learning\" - Goodfellow et al., Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Derivatives: Understanding Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Cost function for a simple model\n",
    "def cost_function(w):\n",
    "    \"\"\"Simple quadratic cost function: (w - 3)^2 + 5\"\"\"\n",
    "    return (w - 3)**2 + 5\n",
    "\n",
    "def derivative(w):\n",
    "    \"\"\"Derivative of cost function: 2(w - 3)\"\"\"\n",
    "    return 2 * (w - 3)\n",
    "\n",
    "# Visualize function and its derivative\n",
    "w_values = np.linspace(-2, 8, 100)\n",
    "cost_values = cost_function(w_values)\n",
    "deriv_values = derivative(w_values)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Cost function\n",
    "ax1.plot(w_values, cost_values, 'b-', linewidth=2, label='Cost function')\n",
    "ax1.plot(3, 5, 'ro', markersize=15, label='Minimum (w=3)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(fontsize=12)\n",
    "ax1.set_xlabel('Weight (w)', fontsize=12)\n",
    "ax1.set_ylabel('Cost', fontsize=12)\n",
    "ax1.set_title('Cost Function: Goal is to find minimum', fontsize=14)\n",
    "\n",
    "# Plot 2: Derivative (slope)\n",
    "ax2.plot(w_values, deriv_values, 'g-', linewidth=2, label='Derivative (slope)')\n",
    "ax2.axhline(y=0, color='r', linestyle='--', linewidth=2, label='Zero derivative = minimum')\n",
    "ax2.plot(3, 0, 'ro', markersize=15)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(fontsize=12)\n",
    "ax2.set_xlabel('Weight (w)', fontsize=12)\n",
    "ax2.set_ylabel('Derivative', fontsize=12)\n",
    "ax2.set_title('Derivative: Shows direction to move', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Key Insights:\")\n",
    "print(\"  ‚Ä¢ Derivative > 0 ‚Üí Function increasing ‚Üí Move left (decrease w)\")\n",
    "print(\"  ‚Ä¢ Derivative < 0 ‚Üí Function decreasing ‚Üí Move right (increase w)\")\n",
    "print(\"  ‚Ä¢ Derivative = 0 ‚Üí At minimum/maximum ‚Üí Stop!\")\n",
    "print(\"\\nüí° This is the foundation of gradient descent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Gradient Descent: Training ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement gradient descent from scratch\n",
    "def gradient_descent(starting_point, learning_rate, num_iterations):\n",
    "    \"\"\"\n",
    "    Find minimum of cost function using gradient descent\n",
    "    \n",
    "    Parameters:\n",
    "    - starting_point: Initial weight value\n",
    "    - learning_rate: Step size (how far to move)\n",
    "    - num_iterations: Number of steps to take\n",
    "    \"\"\"\n",
    "    w = starting_point\n",
    "    history = [w]\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Calculate gradient (derivative)\n",
    "        grad = derivative(w)\n",
    "        \n",
    "        # Update weight: move opposite to gradient\n",
    "        w = w - learning_rate * grad\n",
    "        history.append(w)\n",
    "        \n",
    "        if i < 5 or i == num_iterations - 1:\n",
    "            print(f\"Iteration {i+1}: w={w:.4f}, cost={cost_function(w):.4f}, gradient={grad:.4f}\")\n",
    "    \n",
    "    return w, history\n",
    "\n",
    "print(\"üéØ Gradient Descent in Action!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nStarting from w=7 (far from optimal w=3)...\\n\")\n",
    "\n",
    "final_w, history = gradient_descent(starting_point=7, learning_rate=0.1, num_iterations=20)\n",
    "\n",
    "print(f\"\\n‚úÖ Converged to w={final_w:.4f} (optimal is w=3.0)\")\n",
    "print(f\"   Final cost: {cost_function(final_w):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient descent path\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Path on cost function\n",
    "ax1 = fig.add_subplot(121)\n",
    "w_values = np.linspace(-1, 8, 100)\n",
    "ax1.plot(w_values, cost_function(w_values), 'b-', linewidth=2, label='Cost function')\n",
    "\n",
    "# Plot gradient descent steps\n",
    "history_costs = [cost_function(w) for w in history]\n",
    "ax1.plot(history, history_costs, 'ro-', linewidth=2, markersize=8, \n",
    "         label='Gradient descent path', alpha=0.7)\n",
    "ax1.plot(history[0], history_costs[0], 'go', markersize=15, label='Start')\n",
    "ax1.plot(history[-1], history_costs[-1], 'r*', markersize=20, label='End')\n",
    "\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.set_xlabel('Weight (w)', fontsize=12)\n",
    "ax1.set_ylabel('Cost', fontsize=12)\n",
    "ax1.set_title('Gradient Descent Path', fontsize=14)\n",
    "\n",
    "# Plot 2: Cost over iterations\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.plot(range(len(history_costs)), history_costs, 'b-o', linewidth=2, markersize=6)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlabel('Iteration', fontsize=12)\n",
    "ax2.set_ylabel('Cost', fontsize=12)\n",
    "ax2.set_title('Cost Reduction Over Time', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° This is EXACTLY how neural networks learn!\")\n",
    "print(\"   1. Compute gradient (derivative of cost w.r.t. weights)\")\n",
    "print(\"   2. Update weights in opposite direction of gradient\")\n",
    "print(\"   3. Repeat until convergence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Partial Derivatives & Gradients: Multiple Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D cost function (2 weights)\n",
    "def cost_3d(w1, w2):\n",
    "    \"\"\"Bowl-shaped cost function\"\"\"\n",
    "    return (w1 - 2)**2 + (w2 - 1)**2 + 5\n",
    "\n",
    "def gradient_3d(w1, w2):\n",
    "    \"\"\"Gradient vector [‚àÇC/‚àÇw1, ‚àÇC/‚àÇw2]\"\"\"\n",
    "    dw1 = 2 * (w1 - 2)\n",
    "    dw2 = 2 * (w2 - 1)\n",
    "    return np.array([dw1, dw2])\n",
    "\n",
    "# Visualize 3D cost surface\n",
    "w1_range = np.linspace(-1, 5, 50)\n",
    "w2_range = np.linspace(-2, 4, 50)\n",
    "W1, W2 = np.meshgrid(w1_range, w2_range)\n",
    "Z = cost_3d(W1, W2)\n",
    "\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "\n",
    "# 3D surface plot\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "surf = ax1.plot_surface(W1, W2, Z, cmap='viridis', alpha=0.8)\n",
    "ax1.set_xlabel('w1')\n",
    "ax1.set_ylabel('w2')\n",
    "ax1.set_zlabel('Cost')\n",
    "ax1.set_title('3D Cost Surface')\n",
    "fig.colorbar(surf, ax=ax1, shrink=0.5)\n",
    "\n",
    "# Contour plot\n",
    "ax2 = fig.add_subplot(132)\n",
    "contour = ax2.contour(W1, W2, Z, levels=20, cmap='viridis')\n",
    "ax2.plot(2, 1, 'r*', markersize=20, label='Minimum (2, 1)')\n",
    "ax2.set_xlabel('w1')\n",
    "ax2.set_ylabel('w2')\n",
    "ax2.set_title('Contour Plot (Top View)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "fig.colorbar(contour, ax=ax2)\n",
    "\n",
    "# Gradient vectors\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.contour(W1, W2, Z, levels=20, cmap='viridis', alpha=0.3)\n",
    "\n",
    "# Sample points and their gradients\n",
    "sample_points = np.array([[4, 3], [0, 2], [3, -1], [1, 0]])\n",
    "for point in sample_points:\n",
    "    grad = gradient_3d(point[0], point[1])\n",
    "    # Normalize for visualization\n",
    "    grad_norm = grad / (np.linalg.norm(grad) + 1e-8) * 0.5\n",
    "    ax3.arrow(point[0], point[1], -grad_norm[0], -grad_norm[1],\n",
    "             head_width=0.2, head_length=0.15, fc='red', ec='red', linewidth=2)\n",
    "    ax3.plot(point[0], point[1], 'ro', markersize=8)\n",
    "\n",
    "ax3.plot(2, 1, 'g*', markersize=20, label='Minimum')\n",
    "ax3.set_xlabel('w1')\n",
    "ax3.set_ylabel('w2')\n",
    "ax3.set_title('Gradient Vectors\\n(Red arrows point toward minimum)')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_xlim(-1, 5)\n",
    "ax3.set_ylim(-2, 4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üß≠ Gradient Points Downhill!\")\n",
    "print(\"  ‚Ä¢ Gradient = [‚àÇC/‚àÇw1, ‚àÇC/‚àÇw2] = vector of partial derivatives\")\n",
    "print(\"  ‚Ä¢ Points in direction of steepest ascent\")\n",
    "print(\"  ‚Ä¢ We move OPPOSITE to gradient (downhill) to minimize cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≤ Part 3: Probability & Statistics\n",
    "\n",
    "ML deals with uncertainty! Probability theory helps us model and reason about randomness.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Probability distributions\n",
    "- Expected values\n",
    "- Variance and standard deviation\n",
    "- Bayes' theorem\n",
    "\n",
    "**Source:** \"Pattern Recognition and Machine Learning\" - Bishop, Chapter 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common probability distributions in ML\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# 1. Normal/Gaussian Distribution (most important!)\n",
    "x = np.linspace(-5, 5, 100)\n",
    "for mu, sigma in [(0, 1), (0, 0.5), (0, 2)]:\n",
    "    y = (1/(sigma * np.sqrt(2*np.pi))) * np.exp(-0.5*((x-mu)/sigma)**2)\n",
    "    axes[0,0].plot(x, y, linewidth=2, label=f'Œº={mu}, œÉ={sigma}')\n",
    "axes[0,0].set_title('Normal Distribution\\n(Most features, errors, noise)')\n",
    "axes[0,0].set_xlabel('x')\n",
    "axes[0,0].set_ylabel('Probability Density')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Bernoulli Distribution (binary outcomes)\n",
    "p_values = [0.3, 0.5, 0.7]\n",
    "x_pos = np.arange(2)\n",
    "width = 0.25\n",
    "for i, p in enumerate(p_values):\n",
    "    axes[0,1].bar(x_pos + i*width, [1-p, p], width, label=f'p={p}', alpha=0.7)\n",
    "axes[0,1].set_title('Bernoulli Distribution\\n(Binary classification)')\n",
    "axes[0,1].set_xlabel('Outcome')\n",
    "axes[0,1].set_ylabel('Probability')\n",
    "axes[0,1].set_xticks(x_pos + width)\n",
    "axes[0,1].set_xticklabels(['0 (Failure)', '1 (Success)'])\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Uniform Distribution\n",
    "a, b = -2, 2\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = np.where((x >= a) & (x <= b), 1/(b-a), 0)\n",
    "axes[0,2].plot(x, y, linewidth=2)\n",
    "axes[0,2].fill_between(x, y, alpha=0.3)\n",
    "axes[0,2].set_title('Uniform Distribution\\n(Random initialization)')\n",
    "axes[0,2].set_xlabel('x')\n",
    "axes[0,2].set_ylabel('Probability Density')\n",
    "axes[0,2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Exponential Distribution\n",
    "x = np.linspace(0, 5, 100)\n",
    "for lam in [0.5, 1, 2]:\n",
    "    y = lam * np.exp(-lam * x)\n",
    "    axes[1,0].plot(x, y, linewidth=2, label=f'Œª={lam}')\n",
    "axes[1,0].set_title('Exponential Distribution\\n(Time between events)')\n",
    "axes[1,0].set_xlabel('x')\n",
    "axes[1,0].set_ylabel('Probability Density')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Poisson Distribution\n",
    "x = np.arange(0, 15)\n",
    "for lam in [1, 4, 8]:\n",
    "    y = (lam**x * np.exp(-lam)) / np.array([np.math.factorial(i) for i in x])\n",
    "    axes[1,1].plot(x, y, 'o-', linewidth=2, label=f'Œª={lam}')\n",
    "axes[1,1].set_title('Poisson Distribution\\n(Count data)')\n",
    "axes[1,1].set_xlabel('k (number of events)')\n",
    "axes[1,1].set_ylabel('Probability')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Beta Distribution\n",
    "from scipy.stats import beta\n",
    "x = np.linspace(0, 1, 100)\n",
    "for a, b in [(0.5, 0.5), (2, 2), (5, 2)]:\n",
    "    y = beta.pdf(x, a, b)\n",
    "    axes[1,2].plot(x, y, linewidth=2, label=f'Œ±={a}, Œ≤={b}')\n",
    "axes[1,2].set_title('Beta Distribution\\n(Probabilities as outputs)')\n",
    "axes[1,2].set_xlabel('x')\n",
    "axes[1,2].set_ylabel('Probability Density')\n",
    "axes[1,2].legend()\n",
    "axes[1,2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Distribution Use Cases in ML:\")\n",
    "print(\"  ‚Ä¢ Normal: Feature distributions, weight initialization, errors\")\n",
    "print(\"  ‚Ä¢ Bernoulli: Binary classification outputs\")\n",
    "print(\"  ‚Ä¢ Uniform: Random weight initialization\")\n",
    "print(\"  ‚Ä¢ Exponential: Time series, survival analysis\")\n",
    "print(\"  ‚Ä¢ Poisson: Count predictions (traffic, customers)\")\n",
    "print(\"  ‚Ä¢ Beta: Bayesian inference, probability priors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Bayes' Theorem: The Foundation of Probabilistic ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical example: Medical diagnosis\n",
    "print(\"üè• Bayes' Theorem in Action: Medical Diagnosis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Given information\n",
    "P_disease = 0.01  # 1% of population has disease (prior)\n",
    "P_positive_given_disease = 0.95  # Test is 95% sensitive (true positive rate)\n",
    "P_positive_given_healthy = 0.10  # Test has 10% false positive rate\n",
    "\n",
    "# Calculate P(positive) using law of total probability\n",
    "P_positive = (P_positive_given_disease * P_disease + \n",
    "              P_positive_given_healthy * (1 - P_disease))\n",
    "\n",
    "# Bayes' theorem: P(disease | positive test)\n",
    "P_disease_given_positive = (P_positive_given_disease * P_disease) / P_positive\n",
    "\n",
    "print(f\"\\nüìä Given Information:\")\n",
    "print(f\"  ‚Ä¢ Disease prevalence: {P_disease:.1%}\")\n",
    "print(f\"  ‚Ä¢ Test sensitivity: {P_positive_given_disease:.1%}\")\n",
    "print(f\"  ‚Ä¢ False positive rate: {P_positive_given_healthy:.1%}\")\n",
    "\n",
    "print(f\"\\nüéØ Bayes' Theorem Result:\")\n",
    "print(f\"  If you test positive, probability of having disease: {P_disease_given_positive:.1%}\")\n",
    "\n",
    "print(f\"\\nüí° Surprising insight:\")\n",
    "print(f\"  Even with a positive test, only {P_disease_given_positive:.1%} chance of disease!\")\n",
    "print(f\"  Why? The disease is rare (prior probability is low)\")\n",
    "\n",
    "# Visualize Bayes' theorem\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Visualization 1: Tree diagram\n",
    "ax1 = axes[0]\n",
    "ax1.text(0.1, 0.5, 'Population', fontsize=14, fontweight='bold', ha='center')\n",
    "ax1.arrow(0.15, 0.5, 0.15, 0.2, head_width=0.02, head_length=0.05, fc='blue', ec='blue')\n",
    "ax1.arrow(0.15, 0.5, 0.15, -0.2, head_width=0.02, head_length=0.05, fc='green', ec='green')\n",
    "\n",
    "ax1.text(0.35, 0.75, f'Disease\\n{P_disease:.1%}', fontsize=12, ha='center', \n",
    "         bbox=dict(boxstyle='round', facecolor='lightblue'))\n",
    "ax1.text(0.35, 0.25, f'Healthy\\n{1-P_disease:.1%}', fontsize=12, ha='center',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightgreen'))\n",
    "\n",
    "ax1.arrow(0.45, 0.75, 0.15, 0.05, head_width=0.02, head_length=0.03, fc='red', ec='red')\n",
    "ax1.arrow(0.45, 0.25, 0.15, 0.05, head_width=0.02, head_length=0.03, fc='orange', ec='orange')\n",
    "\n",
    "ax1.text(0.7, 0.82, f'Test +\\n{P_positive_given_disease:.1%}', fontsize=11, ha='center',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightcoral'))\n",
    "ax1.text(0.7, 0.32, f'Test +\\n{P_positive_given_healthy:.1%}', fontsize=11, ha='center',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightyellow'))\n",
    "\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Probability Tree', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Visualization 2: Bar chart\n",
    "ax2 = axes[1]\n",
    "categories = ['Prior\\nP(Disease)', 'Likelihood\\nP(+|Disease)', 'Posterior\\nP(Disease|+)']\n",
    "values = [P_disease, P_positive_given_disease, P_disease_given_positive]\n",
    "colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
    "\n",
    "bars = ax2.bar(categories, values, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "for bar, value in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "             f'{value:.1%}', ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "\n",
    "ax2.set_ylabel('Probability', fontsize=12)\n",
    "ax2.set_title('Bayes\\' Theorem: Updating Beliefs', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "ax2.set_ylim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüß† ML Applications of Bayes' Theorem:\")\n",
    "print(\"  ‚Ä¢ Naive Bayes classifier\")\n",
    "print(\"  ‚Ä¢ Bayesian neural networks\")\n",
    "print(\"  ‚Ä¢ Spam filters\")\n",
    "print(\"  ‚Ä¢ Reinforcement learning (belief updates)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéÆ Interactive Exercises\n",
    "\n",
    "Now it's your turn! Try these exercises to solidify your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Vector Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this exercise\n",
    "\n",
    "# Given two vectors representing houses:\n",
    "house_a = np.array([2000, 4, 3])  # [sqft, bedrooms, age]\n",
    "house_b = np.array([1500, 3, 7])\n",
    "\n",
    "# Task 1: Calculate the Euclidean distance between houses\n",
    "# Hint: Use np.linalg.norm() or compute sqrt(sum((a-b)^2))\n",
    "distance = None  # YOUR CODE HERE\n",
    "\n",
    "# Task 2: Normalize house_a to unit length\n",
    "# Hint: Divide by the vector's norm\n",
    "house_a_normalized = None  # YOUR CODE HERE\n",
    "\n",
    "# Task 3: Calculate cosine similarity\n",
    "# Hint: (a ¬∑ b) / (||a|| ||b||)\n",
    "cosine_similarity = None  # YOUR CODE HERE\n",
    "\n",
    "# Check your answers\n",
    "print(\"‚úÖ Solutions:\")\n",
    "print(f\"Distance: {distance}\")\n",
    "print(f\"Normalized house_a: {house_a_normalized}\")\n",
    "print(f\"Cosine similarity: {cosine_similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this exercise\n",
    "\n",
    "# Dataset: 4 samples, 3 features\n",
    "X = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9],\n",
    "    [10, 11, 12]\n",
    "])\n",
    "\n",
    "# Weight matrix: 3 inputs -> 2 outputs\n",
    "W = np.array([\n",
    "    [0.1, 0.2],\n",
    "    [0.3, 0.4],\n",
    "    [0.5, 0.6]\n",
    "])\n",
    "\n",
    "# Task 1: Multiply X and W to get predictions\n",
    "# What should be the output shape?\n",
    "predictions = None  # YOUR CODE HERE\n",
    "\n",
    "# Task 2: Calculate the mean of each feature (column) in X\n",
    "feature_means = None  # YOUR CODE HERE\n",
    "\n",
    "# Task 3: Center the data by subtracting means\n",
    "X_centered = None  # YOUR CODE HERE\n",
    "\n",
    "print(\"‚úÖ Solutions:\")\n",
    "print(f\"Predictions shape: {predictions.shape if predictions is not None else 'Not computed'}\")\n",
    "print(f\"Feature means: {feature_means}\")\n",
    "print(f\"Centered data:\\n{X_centered}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Implement Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement gradient descent for a different function\n",
    "\n",
    "def new_cost_function(w):\n",
    "    \"\"\"Cost function: w^3 - 2w^2 + 5\"\"\"\n",
    "    return w**3 - 2*w**2 + 5\n",
    "\n",
    "def new_derivative(w):\n",
    "    \"\"\"Derivative: 3w^2 - 4w\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return None\n",
    "\n",
    "def gradient_descent_exercise(start, lr, iterations):\n",
    "    \"\"\"\n",
    "    Implement gradient descent\n",
    "    \n",
    "    Parameters:\n",
    "    - start: starting weight\n",
    "    - lr: learning rate\n",
    "    - iterations: number of steps\n",
    "    \n",
    "    Returns:\n",
    "    - final weight\n",
    "    - history of weights\n",
    "    \"\"\"\n",
    "    w = start\n",
    "    history = [w]\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        # YOUR CODE HERE\n",
    "        # 1. Calculate gradient\n",
    "        # 2. Update weight\n",
    "        # 3. Append to history\n",
    "        pass\n",
    "    \n",
    "    return w, history\n",
    "\n",
    "# Test your implementation\n",
    "final, hist = gradient_descent_exercise(start=2.0, lr=0.01, iterations=100)\n",
    "print(f\"Final weight: {final}\")\n",
    "print(f\"Final cost: {new_cost_function(final)}\")\n",
    "\n",
    "# Visualize\n",
    "if hist is not None and len(hist) > 1:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot cost function\n",
    "    plt.subplot(121)\n",
    "    w_vals = np.linspace(-1, 3, 100)\n",
    "    plt.plot(w_vals, new_cost_function(w_vals), 'b-', linewidth=2)\n",
    "    plt.plot(hist, [new_cost_function(w) for w in hist], 'ro-', alpha=0.6)\n",
    "    plt.xlabel('w')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.title('Gradient Descent Path')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot convergence\n",
    "    plt.subplot(122)\n",
    "    plt.plot([new_cost_function(w) for w in hist], 'b-o')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.title('Cost vs Iteration')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Summary & Next Steps\n",
    "\n",
    "### ‚úÖ What You've Learned:\n",
    "\n",
    "**Linear Algebra:**\n",
    "- Vectors represent data points\n",
    "- Matrices organize datasets and transformations\n",
    "- Eigenvalues/eigenvectors reveal data structure\n",
    "- Matrix operations are the foundation of neural networks\n",
    "\n",
    "**Calculus:**\n",
    "- Derivatives measure rate of change\n",
    "- Gradients point uphill (we go opposite direction)\n",
    "- Gradient descent is the core ML training algorithm\n",
    "- Chain rule enables backpropagation\n",
    "\n",
    "**Probability:**\n",
    "- Distributions model uncertainty\n",
    "- Bayes' theorem updates beliefs with evidence\n",
    "- Expected values guide decision making\n",
    "- Variance measures spread/uncertainty\n",
    "\n",
    "### üìö Key Formulas:\n",
    "\n",
    "1. **Dot Product**: $\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i} a_i b_i$\n",
    "\n",
    "2. **Matrix Multiplication**: $(AB)_{ij} = \\sum_{k} A_{ik} B_{kj}$\n",
    "\n",
    "3. **Gradient Descent**: $w_{t+1} = w_t - \\alpha \\nabla f(w_t)$\n",
    "\n",
    "4. **Bayes' Theorem**: $P(A|B) = \\frac{P(B|A) P(A)}{P(B)}$\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "1. **[Statistics Fundamentals](03_statistics.ipynb)** - Dive deeper into statistical inference\n",
    "2. **[Data Processing](04_data_processing.ipynb)** - Learn to clean and transform data\n",
    "3. **[Classical ML](05_classical_ml.ipynb)** - Apply math to real algorithms\n",
    "\n",
    "### üìñ Recommended Reading:\n",
    "\n",
    "- \"Mathematics for Machine Learning\" - Deisenroth, Faisal, Ong\n",
    "- \"Deep Learning\" - Goodfellow, Bengio, Courville (Chapters 2-4)\n",
    "- 3Blue1Brown videos on Linear Algebra and Calculus\n",
    "- Khan Academy: Linear Algebra, Calculus, Probability\n",
    "\n",
    "### üí™ Challenge Problems:\n",
    "\n",
    "1. Implement PCA from scratch using eigendecomposition\n",
    "2. Code a simple neural network using only NumPy\n",
    "3. Derive and implement backpropagation\n",
    "4. Build a Naive Bayes classifier\n",
    "\n",
    "**Remember**: Mathematics is the language of ML. Master these foundations, and everything else becomes easier! üéØ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
