{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Classical Machine Learning Algorithms - Complete Guide\n",
    "\n",
    "Master the fundamental algorithms that power modern ML! This comprehensive guide covers theory, implementation, and interview preparation.\n",
    "\n",
    "**Learning Goals:**\n",
    "- Understand algorithm internals (not just sklearn!)\n",
    "- Know when to use each algorithm\n",
    "- Implement algorithms from scratch\n",
    "- Master hyperparameter tuning\n",
    "- Compare algorithms systematically\n",
    "- Prepare for technical interviews\n",
    "\n",
    "**Algorithms Covered:**\n",
    "1. **Linear Models:** Linear Regression, Ridge, Lasso, Logistic Regression\n",
    "2. **Tree-Based:** Decision Trees, Random Forest, Gradient Boosting, XGBoost\n",
    "3. **Instance-Based:** K-Nearest Neighbors (KNN)\n",
    "4. **Support Vector Machines:** SVM/SVR with different kernels\n",
    "5. **Naive Bayes:** Gaussian, Multinomial, Bernoulli\n",
    "6. **Ensemble Methods:** Bagging, Boosting, Stacking\n",
    "7. **Clustering:** K-Means, DBSCAN, Hierarchical\n",
    "8. **Dimensionality Reduction:** PCA, t-SNE, LDA\n",
    "\n",
    "**Interview Topics:**\n",
    "- Algorithm selection criteria\n",
    "- Bias-variance tradeoff\n",
    "- Overfitting prevention\n",
    "- Model interpretability\n",
    "- Computational complexity\n",
    "\n",
    "**Sources:**\n",
    "- \"The Elements of Statistical Learning\" - Hastie, Tibshirani, Friedman\n",
    "- \"Pattern Recognition and Machine Learning\" - Bishop\n",
    "- \"Hands-On Machine Learning\" - G√©ron (2019)\n",
    "- \"Introduction to Statistical Learning\" - James et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ALL necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve,\n",
    "    mean_squared_error, r2_score, mean_absolute_error\n",
    ")\n",
    "\n",
    "# Datasets\n",
    "from sklearn.datasets import (\n",
    "    make_classification, make_regression, make_blobs,\n",
    "    load_iris, load_breast_cancer, load_wine\n",
    ")\n",
    "\n",
    "# Classical ML Algorithms\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression, Ridge, Lasso, ElasticNet,\n",
    "    LogisticRegression, SGDClassifier\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, RandomForestRegressor,\n",
    "    GradientBoostingClassifier, GradientBoostingRegressor,\n",
    "    AdaBoostClassifier, BaggingClassifier, VotingClassifier, StackingClassifier\n",
    ")\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "sns.set_palette('husl')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All libraries loaded successfully!\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"Scikit-learn: {__import__('sklearn').__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Part 1: Linear Models - The Foundation\n",
    "\n",
    "**Interview Question:** *\"Explain linear regression and its assumptions.\"*\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Linear Regression:**\n",
    "- **Model:** $y = \\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n + \\epsilon$\n",
    "- **Goal:** Minimize squared errors (OLS - Ordinary Least Squares)\n",
    "- **Solution:** $\\beta = (X^T X)^{-1} X^T y$\n",
    "\n",
    "**Key Assumptions:**\n",
    "1. **Linearity:** Relationship between X and y is linear\n",
    "2. **Independence:** Observations are independent\n",
    "3. **Homoscedasticity:** Constant variance of errors\n",
    "4. **Normality:** Errors are normally distributed\n",
    "5. **No multicollinearity:** Features aren't highly correlated\n",
    "\n",
    "**Regularization Variants:**\n",
    "\n",
    "| Model | Loss Function | Use Case |\n",
    "|-------|---------------|----------|\n",
    "| **Linear Regression** | MSE | No regularization |\n",
    "| **Ridge (L2)** | MSE + Œ±||Œ≤||¬≤ | Multicollinearity, keep all features |\n",
    "| **Lasso (L1)** | MSE + Œ±||Œ≤|| | Feature selection, sparse solutions |\n",
    "| **ElasticNet** | MSE + Œ±‚ÇÅ||Œ≤|| + Œ±‚ÇÇ||Œ≤||¬≤ | Best of both worlds |\n",
    "\n",
    "**When to Use:**\n",
    "- ‚úÖ Fast training and prediction\n",
    "- ‚úÖ Interpretable coefficients\n",
    "- ‚úÖ Works well with many features (with regularization)\n",
    "- ‚ùå Cannot capture non-linear relationships (without feature engineering)\n",
    "- ‚ùå Sensitive to outliers\n",
    "\n",
    "**Source:** \"The Elements of Statistical Learning\" Chapter 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression from scratch\n",
    "print(\"üìê LINEAR REGRESSION FROM SCRATCH\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class LinearRegressionScratch:\n",
    "    \"\"\"\n",
    "    Linear Regression implemented from scratch using Normal Equation\n",
    "    \n",
    "    Formula: Œ≤ = (X^T X)^(-1) X^T y\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.coefficients = None\n",
    "        self.intercept = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit linear model using Normal Equation\n",
    "        \"\"\"\n",
    "        # Add bias term (column of ones)\n",
    "        X_with_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        \n",
    "        # Normal equation: Œ≤ = (X^T X)^(-1) X^T y\n",
    "        XtX = X_with_bias.T @ X_with_bias\n",
    "        Xty = X_with_bias.T @ y\n",
    "        \n",
    "        # Solve for coefficients\n",
    "        theta = np.linalg.solve(XtX, Xty)\n",
    "        \n",
    "        self.intercept = theta[0]\n",
    "        self.coefficients = theta[1:]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions\n",
    "        \"\"\"\n",
    "        return X @ self.coefficients + self.intercept\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate R¬≤ score\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        return 1 - (ss_res / ss_tot)\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X_reg, y_reg = make_regression(n_samples=200, n_features=1, noise=20, random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"\\nüîß Training Custom Implementation...\")\n",
    "model_scratch = LinearRegressionScratch()\n",
    "model_scratch.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nüìä Model Parameters:\")\n",
    "print(f\"  Intercept (Œ≤‚ÇÄ): {model_scratch.intercept:.4f}\")\n",
    "print(f\"  Coefficient (Œ≤‚ÇÅ): {model_scratch.coefficients[0]:.4f}\")\n",
    "print(f\"  Equation: y = {model_scratch.intercept:.2f} + {model_scratch.coefficients[0]:.2f}x\")\n",
    "\n",
    "# Compare with sklearn\n",
    "print(\"\\nüîß Training Sklearn Implementation...\")\n",
    "model_sklearn = LinearRegression()\n",
    "model_sklearn.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nüìä Sklearn Parameters:\")\n",
    "print(f\"  Intercept (Œ≤‚ÇÄ): {model_sklearn.intercept_:.4f}\")\n",
    "print(f\"  Coefficient (Œ≤‚ÇÅ): {model_sklearn.coef_[0]:.4f}\")\n",
    "\n",
    "# Evaluate both\n",
    "r2_scratch = model_scratch.score(X_test, y_test)\n",
    "r2_sklearn = model_sklearn.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\nüìà Performance Comparison:\")\n",
    "print(f\"  Custom R¬≤: {r2_scratch:.4f}\")\n",
    "print(f\"  Sklearn R¬≤: {r2_sklearn:.4f}\")\n",
    "print(f\"  Difference: {abs(r2_scratch - r2_sklearn):.6f}\")\n",
    "print(\"  ‚úÖ Implementations match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Linear Regression\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Plot 1: Scatter plot with regression line\n",
    "axes[0, 0].scatter(X_train, y_train, alpha=0.6, label='Training data', s=50)\n",
    "axes[0, 0].scatter(X_test, y_test, alpha=0.6, label='Test data', s=50, color='orange')\n",
    "\n",
    "# Plot regression line\n",
    "X_line = np.linspace(X_reg.min(), X_reg.max(), 100).reshape(-1, 1)\n",
    "y_line = model_sklearn.predict(X_line)\n",
    "axes[0, 0].plot(X_line, y_line, 'r-', linewidth=3, label='Regression line')\n",
    "\n",
    "axes[0, 0].set_xlabel('X', fontsize=12)\n",
    "axes[0, 0].set_ylabel('y', fontsize=12)\n",
    "axes[0, 0].set_title('Linear Regression Fit', fontweight='bold', fontsize=14)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Residuals plot\n",
    "y_pred_train = model_sklearn.predict(X_train)\n",
    "residuals = y_train - y_pred_train\n",
    "\n",
    "axes[0, 1].scatter(y_pred_train, residuals, alpha=0.6)\n",
    "axes[0, 1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Predicted Values', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Residuals', fontsize=12)\n",
    "axes[0, 1].set_title('Residuals Plot\\n(Should be randomly scattered)', fontweight='bold', fontsize=14)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Q-Q plot (check normality of residuals)\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[0, 2])\n",
    "axes[0, 2].set_title('Q-Q Plot\\n(Check normality assumption)', fontweight='bold', fontsize=14)\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Actual vs Predicted\n",
    "y_pred_test = model_sklearn.predict(X_test)\n",
    "axes[1, 0].scatter(y_test, y_pred_test, alpha=0.6)\n",
    "axes[1, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                'r--', linewidth=2, label='Perfect prediction')\n",
    "axes[1, 0].set_xlabel('Actual Values', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Predicted Values', fontsize=12)\n",
    "axes[1, 0].set_title(f'Actual vs Predicted\\nR¬≤ = {r2_sklearn:.4f}', fontweight='bold', fontsize=14)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Distribution of residuals\n",
    "axes[1, 1].hist(residuals, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[1, 1].axvline(residuals.mean(), color='red', linestyle='--', \n",
    "                   linewidth=2, label=f'Mean: {residuals.mean():.2f}')\n",
    "axes[1, 1].set_xlabel('Residual Value', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1, 1].set_title('Distribution of Residuals\\n(Should be normal)', fontweight='bold', fontsize=14)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Learning curve (if we had more samples)\n",
    "# Generate more data for learning curve\n",
    "X_large, y_large = make_regression(n_samples=1000, n_features=1, noise=20, random_state=42)\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    LinearRegression(), X_large, y_large, \n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    cv=5, scoring='r2'\n",
    ")\n",
    "\n",
    "train_mean = train_scores.mean(axis=1)\n",
    "train_std = train_scores.std(axis=1)\n",
    "val_mean = val_scores.mean(axis=1)\n",
    "val_std = val_scores.std(axis=1)\n",
    "\n",
    "axes[1, 2].plot(train_sizes, train_mean, label='Training score', linewidth=2)\n",
    "axes[1, 2].fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.3)\n",
    "axes[1, 2].plot(train_sizes, val_mean, label='Validation score', linewidth=2)\n",
    "axes[1, 2].fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.3)\n",
    "axes[1, 2].set_xlabel('Training Set Size', fontsize=12)\n",
    "axes[1, 2].set_ylabel('R¬≤ Score', fontsize=12)\n",
    "axes[1, 2].set_title('Learning Curve', fontweight='bold', fontsize=14)\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insights from Visualizations:\")\n",
    "print(\"  ‚Ä¢ Residuals plot: Random scatter = good, pattern = model is missing something\")\n",
    "print(\"  ‚Ä¢ Q-Q plot: Points on line = residuals are normal (assumption met)\")\n",
    "print(\"  ‚Ä¢ Actual vs Predicted: Points on diagonal = perfect predictions\")\n",
    "print(\"  ‚Ä¢ Residuals histogram: Should be bell-shaped (normal distribution)\")\n",
    "print(\"  ‚Ä¢ Learning curve: Gap between train/val = overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Regularization: Ridge, Lasso, ElasticNet\n",
    "\n",
    "**Interview Question:** *\"What's the difference between L1 and L2 regularization?\"*\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**L2 Regularization (Ridge):**\n",
    "- **Formula:** $\\min ||y - X\\beta||^2 + \\alpha ||\\beta||^2$\n",
    "- **Effect:** Shrinks coefficients toward zero (but never exactly zero)\n",
    "- **Use:** When you want to keep all features but reduce overfitting\n",
    "- **Properties:** Differentiable everywhere, unique solution\n",
    "\n",
    "**L1 Regularization (Lasso):**\n",
    "- **Formula:** $\\min ||y - X\\beta||^2 + \\alpha ||\\beta||$\n",
    "- **Effect:** Shrinks some coefficients to EXACTLY zero (feature selection)\n",
    "- **Use:** When you suspect only few features are important\n",
    "- **Properties:** Non-differentiable at zero, sparse solutions\n",
    "\n",
    "**Why L1 creates sparsity:**\n",
    "- L1 penalty has \"corners\" at axes ‚Üí optimization prefers axis-aligned solutions\n",
    "- L2 penalty is circular ‚Üí all directions treated equally\n",
    "\n",
    "**ElasticNet:**\n",
    "- **Formula:** $\\min ||y - X\\beta||^2 + \\alpha_1 ||\\beta|| + \\alpha_2 ||\\beta||^2$\n",
    "- **Use:** Combines benefits of both (sparse + grouped selection)\n",
    "\n",
    "**Interview Tip:** Draw the L1 (diamond) vs L2 (circle) constraint regions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Ridge, Lasso, ElasticNet\n",
    "print(\"üéØ REGULARIZATION COMPARISON: Ridge vs Lasso vs ElasticNet\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Generate data with correlated features\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "n_features = 20\n",
    "n_informative = 5  # Only 5 features are truly important\n",
    "\n",
    "X, y = make_regression(n_samples=n_samples, n_features=n_features, \n",
    "                       n_informative=n_informative, noise=10, random_state=42)\n",
    "\n",
    "# Add multicollinearity (some features highly correlated)\n",
    "X[:, 10] = X[:, 0] + np.random.randn(n_samples) * 0.1\n",
    "X[:, 11] = X[:, 1] + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize (important for regularization!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train different models with same alpha\n",
    "alpha = 1.0\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge (L2)': Ridge(alpha=alpha),\n",
    "    'Lasso (L1)': Lasso(alpha=alpha),\n",
    "    'ElasticNet': ElasticNet(alpha=alpha, l1_ratio=0.5)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"\\nüìä Training Models...\\n\")\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    train_score = model.score(X_train_scaled, y_train)\n",
    "    test_score = model.score(X_test_scaled, y_test)\n",
    "    \n",
    "    # Get coefficients\n",
    "    if hasattr(model, 'coef_'):\n",
    "        coef = model.coef_\n",
    "        n_nonzero = np.sum(np.abs(coef) > 1e-5)\n",
    "    else:\n",
    "        coef = None\n",
    "        n_nonzero = n_features\n",
    "    \n",
    "    results[name] = {\n",
    "        'train_r2': train_score,\n",
    "        'test_r2': test_score,\n",
    "        'coef': coef,\n",
    "        'n_nonzero': n_nonzero\n",
    "    }\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Train R¬≤: {train_score:.4f}\")\n",
    "    print(f\"  Test R¬≤: {test_score:.4f}\")\n",
    "    print(f\"  Non-zero coefficients: {n_nonzero}/{n_features}\")\n",
    "    print(f\"  Overfitting gap: {train_score - test_score:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nüí° Observations:\")\n",
    "print(\"  ‚Ä¢ Linear Regression: Likely overfits (high train, lower test)\")\n",
    "print(\"  ‚Ä¢ Ridge: Reduces overfitting, keeps all features\")\n",
    "print(\"  ‚Ä¢ Lasso: Sparse solution (many zeros), automatic feature selection\")\n",
    "print(\"  ‚Ä¢ ElasticNet: Balance between Ridge and Lasso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regularization effects\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Plot 1: Coefficient values comparison\n",
    "ax = axes[0, 0]\n",
    "x_pos = np.arange(n_features)\n",
    "width = 0.2\n",
    "\n",
    "for i, (name, result) in enumerate(list(results.items())[1:]):  # Skip Linear Regression\n",
    "    ax.bar(x_pos + i*width, result['coef'], width, label=name, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Feature Index', fontsize=12)\n",
    "ax.set_ylabel('Coefficient Value', fontsize=12)\n",
    "ax.set_title('Coefficient Comparison\\n(Notice Lasso zeros)', fontweight='bold', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: Number of non-zero coefficients\n",
    "ax = axes[0, 1]\n",
    "model_names = list(results.keys())\n",
    "n_nonzeros = [results[name]['n_nonzero'] for name in model_names]\n",
    "colors = ['gray', 'skyblue', 'coral', 'lightgreen']\n",
    "\n",
    "bars = ax.bar(model_names, n_nonzeros, color=colors, edgecolor='black', linewidth=2)\n",
    "ax.axhline(y=n_informative, color='red', linestyle='--', linewidth=2, \n",
    "           label=f'True informative: {n_informative}')\n",
    "ax.set_ylabel('Number of Non-zero Features', fontsize=12)\n",
    "ax.set_title('Feature Selection\\n(Lasso achieves sparsity)', fontweight='bold', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "for bar, val in zip(bars, n_nonzeros):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(val)}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 3: Train vs Test R¬≤ (overfitting detection)\n",
    "ax = axes[0, 2]\n",
    "train_scores = [results[name]['train_r2'] for name in model_names]\n",
    "test_scores = [results[name]['test_r2'] for name in model_names]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, train_scores, width, label='Train R¬≤', alpha=0.7, color='green')\n",
    "ax.bar(x + width/2, test_scores, width, label='Test R¬≤', alpha=0.7, color='orange')\n",
    "\n",
    "ax.set_ylabel('R¬≤ Score', fontsize=12)\n",
    "ax.set_title('Train vs Test Performance\\n(Gap = Overfitting)', fontweight='bold', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 4: Regularization path for Lasso\n",
    "ax = axes[1, 0]\n",
    "alphas = np.logspace(-3, 3, 100)\n",
    "coefs = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    lasso.fit(X_train_scaled, y_train)\n",
    "    coefs.append(lasso.coef_)\n",
    "\n",
    "coefs = np.array(coefs)\n",
    "\n",
    "for i in range(n_features):\n",
    "    ax.plot(alphas, coefs[:, i], alpha=0.6)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Alpha (Regularization Strength)', fontsize=12)\n",
    "ax.set_ylabel('Coefficient Value', fontsize=12)\n",
    "ax.set_title('Lasso Regularization Path\\n(Coefficients ‚Üí 0 as Œ± increases)', \n",
    "             fontweight='bold', fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Plot 5: Regularization path for Ridge\n",
    "ax = axes[1, 1]\n",
    "coefs_ridge = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train_scaled, y_train)\n",
    "    coefs_ridge.append(ridge.coef_)\n",
    "\n",
    "coefs_ridge = np.array(coefs_ridge)\n",
    "\n",
    "for i in range(n_features):\n",
    "    ax.plot(alphas, coefs_ridge[:, i], alpha=0.6)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Alpha (Regularization Strength)', fontsize=12)\n",
    "ax.set_ylabel('Coefficient Value', fontsize=12)\n",
    "ax.set_title('Ridge Regularization Path\\n(Shrinks but never zero)', \n",
    "             fontweight='bold', fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Plot 6: Geometric interpretation (2D case)\n",
    "ax = axes[1, 2]\n",
    "\n",
    "# Draw constraint regions\n",
    "theta = np.linspace(0, 2*np.pi, 100)\n",
    "t = 1\n",
    "\n",
    "# L2 (circle)\n",
    "l2_x = t * np.cos(theta)\n",
    "l2_y = t * np.sin(theta)\n",
    "ax.plot(l2_x, l2_y, 'b-', linewidth=3, label='L2 (Ridge)', alpha=0.7)\n",
    "ax.fill(l2_x, l2_y, alpha=0.2, color='blue')\n",
    "\n",
    "# L1 (diamond)\n",
    "l1_x = np.array([t, 0, -t, 0, t])\n",
    "l1_y = np.array([0, t, 0, -t, 0])\n",
    "ax.plot(l1_x, l1_y, 'r-', linewidth=3, label='L1 (Lasso)', alpha=0.7)\n",
    "ax.fill(l1_x, l1_y, alpha=0.2, color='red')\n",
    "\n",
    "# Draw contours of loss function\n",
    "x = np.linspace(-1.5, 1.5, 100)\n",
    "y = np.linspace(-1.5, 1.5, 100)\n",
    "X_grid, Y_grid = np.meshgrid(x, y)\n",
    "# Elliptical contours centered at (1, 0.5)\n",
    "Z = (X_grid - 1)**2 + 2*(Y_grid - 0.5)**2\n",
    "ax.contour(X_grid, Y_grid, Z, levels=10, alpha=0.4, colors='gray')\n",
    "\n",
    "# Mark optimal points\n",
    "ax.plot(1, 0.5, 'k*', markersize=20, label='OLS solution')\n",
    "ax.plot(0.7, 0.3, 'bo', markersize=12, label='Ridge solution')\n",
    "ax.plot(1, 0, 'ro', markersize=12, label='Lasso solution')\n",
    "\n",
    "ax.set_xlabel('Œ≤‚ÇÅ', fontsize=12)\n",
    "ax.set_ylabel('Œ≤‚ÇÇ', fontsize=12)\n",
    "ax.set_title('Geometric Interpretation\\n(L1 hits axes ‚Üí sparsity)', \n",
    "             fontweight='bold', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(-1.5, 1.5)\n",
    "ax.set_ylim(-1.5, 1.5)\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"  ‚Ä¢ Lasso creates EXACT zeros (sparse solution)\")\n",
    "print(\"  ‚Ä¢ Ridge shrinks all coefficients but keeps them all\")\n",
    "print(\"  ‚Ä¢ Geometric view: L1 diamond has corners ‚Üí hits axes ‚Üí zero coefficients\")\n",
    "print(\"  ‚Ä¢ Regularization path: Watch how coefficients change with Œ±\")\n",
    "\n",
    "print(\"\\nüéØ Interview Answer Template:\")\n",
    "print(\"  'L1 (Lasso) produces sparse solutions because its constraint region\")\n",
    "print(\"   has corners at the axes. When the loss function contours hit these\")\n",
    "print(\"   corners, some coefficients become exactly zero. L2 (Ridge) has a\")\n",
    "print(\"   circular constraint, so coefficients shrink smoothly but rarely hit\")\n",
    "print(\"   zero. Use Lasso for feature selection, Ridge when you want to keep\")\n",
    "print(\"   all features but reduce multicollinearity.'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Logistic Regression - Classification\n",
    "\n",
    "**Interview Question:** *\"Explain logistic regression. Why is it called 'regression' when it does classification?\"*\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Logistic Regression:**\n",
    "- **Model:** $P(y=1|x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta^T x)}}$ (sigmoid function)\n",
    "- **Loss:** Binary Cross-Entropy (Log Loss)\n",
    "- **Optimization:** Gradient descent (no closed-form solution)\n",
    "\n",
    "**Why called 'regression':**\n",
    "- Models continuous probability (0 to 1)\n",
    "- Uses linear regression framework with sigmoid transform\n",
    "- Predicts log-odds: $\\log(\\frac{p}{1-p}) = \\beta_0 + \\beta^T x$\n",
    "\n",
    "**Key Properties:**\n",
    "- Outputs calibrated probabilities\n",
    "- Linear decision boundary\n",
    "- Coefficients interpretable as log-odds ratios\n",
    "- Fast training and prediction\n",
    "\n",
    "**When to Use:**\n",
    "- ‚úÖ Need probability estimates\n",
    "- ‚úÖ Interpretability is important\n",
    "- ‚úÖ Linearly separable classes\n",
    "- ‚úÖ High-dimensional data (with regularization)\n",
    "- ‚ùå Complex non-linear boundaries\n",
    "\n",
    "**Multiclass:** Use softmax (multinomial logistic regression) or one-vs-rest\n",
    "\n",
    "**Source:** \"Pattern Recognition and Machine Learning\" Chapter 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression comprehensive example\n",
    "print(\"üéØ LOGISTIC REGRESSION - BINARY CLASSIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load breast cancer dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "print(f\"\\nüìä Dataset: {data.DESCR.split(chr(10))[0]}\")\n",
    "print(f\"  Samples: {X.shape[0]}\")\n",
    "print(f\"  Features: {X.shape[1]}\")\n",
    "print(f\"  Classes: {np.unique(y)} ({data.target_names})\")\n",
    "print(f\"  Class distribution: {np.bincount(y)}\")\n",
    "\n",
    "# Split and scale\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                      random_state=42, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train models with different regularization\n",
    "models_logreg = {\n",
    "    'No Regularization': LogisticRegression(penalty=None, max_iter=10000, random_state=42),\n",
    "    'L2 (Ridge)': LogisticRegression(penalty='l2', C=1.0, max_iter=10000, random_state=42),\n",
    "    'L1 (Lasso)': LogisticRegression(penalty='l1', C=1.0, solver='liblinear', \n",
    "                                      max_iter=10000, random_state=42),\n",
    "}\n",
    "\n",
    "print(\"\\nüîß Training Logistic Regression Models...\\n\")\n",
    "\n",
    "for name, model in models_logreg.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    # Count non-zero coefficients\n",
    "    n_nonzero = np.sum(np.abs(model.coef_) > 1e-5)\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Accuracy: {acc:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print(f\"  ROC-AUC: {auc:.4f}\")\n",
    "    print(f\"  Non-zero features: {n_nonzero}/{X.shape[1]}\")\n",
    "    print()\n",
    "\n",
    "# Get best model for visualization\n",
    "best_model = models_logreg['L2 (Ridge)']\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = np.abs(best_model.coef_[0])\n",
    "top_features_idx = np.argsort(feature_importance)[-10:][::-1]\n",
    "\n",
    "print(\"\\nüîù Top 10 Most Important Features:\")\n",
    "for idx in top_features_idx:\n",
    "    print(f\"  {data.feature_names[idx]:<30} : {best_model.coef_[0][idx]:>10.4f}\")\n",
    "\n",
    "print(\"\\nüí° Coefficient Interpretation:\")\n",
    "print(\"  Positive coefficient ‚Üí increases probability of malignant\")\n",
    "print(\"  Negative coefficient ‚Üí increases probability of benign\")\n",
    "print(\"  Magnitude ‚Üí importance of feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå≥ Part 2: Tree-Based Models - Non-Linear Power\n",
    "\n",
    "**Interview Question:** *\"Explain how a decision tree works and when to use it.\"*\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Decision Tree Algorithm:**\n",
    "1. Start with all data at root\n",
    "2. Find best feature and split point to maximize information gain\n",
    "3. Split data into left and right child nodes\n",
    "4. Recursively repeat until stopping criterion\n",
    "\n",
    "**Splitting Criteria:**\n",
    "- **Classification:** Gini impurity or Entropy (Information Gain)\n",
    "  - Gini: $G = 1 - \\sum_{i=1}^{n} p_i^2$\n",
    "  - Entropy: $H = -\\sum_{i=1}^{n} p_i \\log(p_i)$\n",
    "- **Regression:** MSE or MAE\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ No feature scaling needed\n",
    "- ‚úÖ Handles non-linear relationships\n",
    "- ‚úÖ Captures interactions automatically\n",
    "- ‚úÖ Interpretable (can visualize tree)\n",
    "- ‚úÖ Handles missing values\n",
    "- ‚úÖ Works with categorical and numerical features\n",
    "\n",
    "**Disadvantages:**\n",
    "- ‚ùå Prone to overfitting (high variance)\n",
    "- ‚ùå Unstable (small data change ‚Üí different tree)\n",
    "- ‚ùå Biased toward features with more levels\n",
    "- ‚ùå Creates axis-parallel boundaries only\n",
    "\n",
    "**Hyperparameters:**\n",
    "- `max_depth`: Maximum tree depth\n",
    "- `min_samples_split`: Minimum samples to split node\n",
    "- `min_samples_leaf`: Minimum samples in leaf\n",
    "- `max_features`: Features to consider for split\n",
    "\n",
    "**Solution to overfitting:** Use ensemble methods (Random Forest, Boosting)\n",
    "\n",
    "**Source:** \"The Elements of Statistical Learning\" Chapter 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree comprehensive example\n",
    "print(\"üå≥ DECISION TREE - FROM BASICS TO ADVANCED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Generate synthetic dataset with non-linear boundary\n",
    "np.random.seed(42)\n",
    "X, y = make_classification(n_samples=500, n_features=2, n_redundant=0,\n",
    "                           n_informative=2, n_clusters_per_class=2,\n",
    "                           random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"\\nüìä Dataset: Synthetic 2D Classification\")\n",
    "print(f\"  Training samples: {X_train.shape[0]}\")\n",
    "print(f\"  Test samples: {X_test.shape[0]}\")\n",
    "print(f\"  Features: {X_train.shape[1]}\")\n",
    "\n",
    "# Train trees with different depths\n",
    "depths = [1, 2, 3, 5, 10, None]  # None = unlimited\n",
    "trees = {}\n",
    "\n",
    "print(\"\\nüå≥ Training Decision Trees with Different Depths...\\n\")\n",
    "\n",
    "for depth in depths:\n",
    "    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    tree.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = tree.score(X_train, y_train)\n",
    "    test_acc = tree.score(X_test, y_test)\n",
    "    \n",
    "    trees[depth] = {\n",
    "        'model': tree,\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'n_nodes': tree.tree_.node_count,\n",
    "        'n_leaves': tree.tree_.n_leaves\n",
    "    }\n",
    "    \n",
    "    depth_str = f\"{depth}\" if depth is not None else \"Unlimited\"\n",
    "    print(f\"Depth {depth_str:>9}:\")\n",
    "    print(f\"  Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"  Test Accuracy:  {test_acc:.4f}\")\n",
    "    print(f\"  Nodes: {tree.tree_.node_count}, Leaves: {tree.tree_.n_leaves}\")\n",
    "    print(f\"  Overfitting: {train_acc - test_acc:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nüí° Observation: Deep trees overfit (perfect train, poor test)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision trees and decision boundaries\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "# Plot decision boundaries for different depths\n",
    "plot_depths = [1, 2, 5, None]\n",
    "h = 0.02  # mesh step size\n",
    "\n",
    "# Create mesh\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "for idx, depth in enumerate(plot_depths):\n",
    "    # Decision boundary\n",
    "    ax = plt.subplot(3, 4, idx + 1)\n",
    "    \n",
    "    tree = trees[depth]['model']\n",
    "    Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='RdYlBu', \n",
    "               edgecolors='black', s=30, alpha=0.7)\n",
    "    \n",
    "    depth_str = f\"{depth}\" if depth is not None else \"Unlimited\"\n",
    "    ax.set_title(f'Depth = {depth_str}\\nTest Acc: {trees[depth][\"test_acc\"]:.3f}',\n",
    "                fontweight='bold')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    \n",
    "    # Visualize tree structure\n",
    "    if depth in [1, 2]:  # Only for shallow trees (readable)\n",
    "        ax_tree = plt.subplot(3, 4, idx + 5)\n",
    "        plot_tree(tree, filled=True, ax=ax_tree, fontsize=8,\n",
    "                 class_names=['Class 0', 'Class 1'],\n",
    "                 feature_names=['Feature 1', 'Feature 2'])\n",
    "        ax_tree.set_title(f'Tree Structure (Depth={depth})', fontweight='bold')\n",
    "\n",
    "# Plot overfitting curve\n",
    "ax = plt.subplot(3, 4, 9)\n",
    "depths_plot = [d if d is not None else 20 for d in depths]\n",
    "train_accs = [trees[d]['train_acc'] for d in depths]\n",
    "test_accs = [trees[d]['test_acc'] for d in depths]\n",
    "\n",
    "ax.plot(depths_plot, train_accs, 'o-', label='Training Accuracy', linewidth=2, markersize=8)\n",
    "ax.plot(depths_plot, test_accs, 's-', label='Test Accuracy', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Max Depth', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Overfitting vs Tree Depth\\n(Gap shows overfitting)', fontweight='bold', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Feature importance (for full tree)\n",
    "ax = plt.subplot(3, 4, 10)\n",
    "full_tree = trees[None]['model']\n",
    "importances = full_tree.feature_importances_\n",
    "ax.barh(['Feature 1', 'Feature 2'], importances, color=['skyblue', 'coral'], edgecolor='black')\n",
    "ax.set_xlabel('Importance', fontsize=12)\n",
    "ax.set_title('Feature Importance\\n(Gini importance)', fontweight='bold', fontsize=14)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Model complexity (nodes vs depth)\n",
    "ax = plt.subplot(3, 4, 11)\n",
    "nodes = [trees[d]['n_nodes'] for d in depths]\n",
    "leaves = [trees[d]['n_leaves'] for d in depths]\n",
    "\n",
    "ax.plot(depths_plot, nodes, 'o-', label='Total Nodes', linewidth=2, markersize=8)\n",
    "ax.plot(depths_plot, leaves, 's-', label='Leaf Nodes', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Max Depth', fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_title('Tree Complexity\\n(More nodes = more complex)', fontweight='bold', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Summary comparison table\n",
    "ax = plt.subplot(3, 4, 12)\n",
    "ax.axis('off')\n",
    "\n",
    "table_data = [['Depth', 'Train Acc', 'Test Acc', 'Gap', 'Nodes']]\n",
    "for depth in depths:\n",
    "    d_str = str(depth) if depth is not None else 'Unl'\n",
    "    train = trees[depth]['train_acc']\n",
    "    test = trees[depth]['test_acc']\n",
    "    gap = train - test\n",
    "    nodes = trees[depth]['n_nodes']\n",
    "    table_data.append([d_str, f'{train:.3f}', f'{test:.3f}', f'{gap:.3f}', str(nodes)])\n",
    "\n",
    "table = ax.table(cellText=table_data, cellLoc='center', loc='center',\n",
    "                colWidths=[0.2, 0.2, 0.2, 0.2, 0.2])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(9)\n",
    "table.scale(1, 2)\n",
    "\n",
    "for i in range(len(table_data[0])):\n",
    "    table[(0, i)].set_facecolor('lightblue')\n",
    "    table[(0, i)].set_text_props(weight='bold')\n",
    "\n",
    "ax.set_title('Performance Summary', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"  ‚Ä¢ Shallow trees (depth 1-2): Underfit, simple boundaries\")\n",
    "print(\"  ‚Ä¢ Medium trees (depth 3-5): Good balance, generalize well\")\n",
    "print(\"  ‚Ä¢ Deep trees (unlimited): Overfit, complex boundaries\")\n",
    "print(\"  ‚Ä¢ Decision boundaries are axis-parallel (limitation of trees)\")\n",
    "\n",
    "print(\"\\nüéØ Interview Answer Template:\")\n",
    "print(\"  'Decision trees recursively split the feature space using greedy\")\n",
    "print(\"   algorithm to maximize information gain. At each node, we choose\")\n",
    "print(\"   the feature and threshold that best separates the classes. The main\")\n",
    "print(\"   challenge is overfitting - deep trees memorize training data. We\")\n",
    "print(\"   control this with max_depth, min_samples_split, and pruning. In\")\n",
    "print(\"   practice, ensemble methods like Random Forest work better.'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Quick Algorithm Comparison Matrix\n",
    "\n",
    "This table will help you choose the right algorithm in interviews:\n",
    "\n",
    "| Algorithm | Speed | Interpretability | Handles Non-linear | Scaling Needed | Handles Outliers | Feature Selection |\n",
    "|-----------|-------|------------------|-------------------|----------------|------------------|-------------------|\n",
    "| **Linear Regression** | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | ‚ùå | ‚úÖ | ‚ùå | With L1 |\n",
    "| **Logistic Regression** | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | ‚ùå | ‚úÖ | ‚ùå | With L1 |\n",
    "| **Decision Tree** | ‚ö°‚ö° | ‚≠ê‚≠ê | ‚úÖ | ‚ùå | ‚úÖ | ‚úÖ |\n",
    "| **Random Forest** | ‚ö° | ‚≠ê | ‚úÖ | ‚ùå | ‚úÖ | ‚úÖ |\n",
    "| **XGBoost** | ‚ö°‚ö° | ‚≠ê | ‚úÖ | ‚ùå | ‚úÖ | ‚úÖ |\n",
    "| **SVM** | ‚ö° | ‚≠ê | ‚úÖ (kernel) | ‚úÖ | ‚ùå | ‚ùå |\n",
    "| **KNN** | ‚ö° (predict slow) | ‚≠ê‚≠ê | ‚úÖ | ‚úÖ | ‚ùå | ‚ùå |\n",
    "| **Naive Bayes** | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê | ‚ùå | ‚ùå | ‚úÖ | ‚ùå |\n",
    "\n",
    "**Legend:** ‚ö° = Speed, ‚≠ê = Interpretability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
