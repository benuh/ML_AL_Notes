{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üëÅÔ∏è Computer Vision with Deep Learning\n",
    "\n",
    "## From CNNs to Modern Object Detection\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Convolutional Neural Networks (CNNs) from scratch\n",
    "- Image classification with modern architectures\n",
    "- Transfer learning and fine-tuning\n",
    "- Object detection (YOLO, Faster R-CNN)\n",
    "- Image segmentation\n",
    "- Visualization techniques\n",
    "\n",
    "**Prerequisites:** Deep Learning Fundamentals (Notebook 06)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Deep learning\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    import torchvision\n",
    "    from torchvision import transforms, models\n",
    "    TORCH_AVAILABLE = True\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch not installed. Install with: pip install torch torchvision\")\n",
    "    TORCH_AVAILABLE = False\n",
    "\n",
    "# Check for GPU\n",
    "if TORCH_AVAILABLE:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"‚úì Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding Convolutions\n",
    "\n",
    "### 1.1 2D Convolution from Scratch\n",
    "\n",
    "**Convolution operation:**\n",
    "```\n",
    "(I * K)[i, j] = Œ£_m Œ£_n I[i+m, j+n] ¬∑ K[m, n]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_numpy(image, kernel, stride=1, padding=0):\n",
    "    \"\"\"\n",
    "    2D convolution from scratch using NumPy.\n",
    "    \n",
    "    Args:\n",
    "        image: (H, W) input image\n",
    "        kernel: (K, K) convolution kernel\n",
    "        stride: Stride for convolution\n",
    "        padding: Zero padding\n",
    "    \n",
    "    Returns:\n",
    "        output: Convolved image\n",
    "    \"\"\"\n",
    "    # Add padding\n",
    "    if padding > 0:\n",
    "        image = np.pad(image, padding, mode='constant')\n",
    "    \n",
    "    H, W = image.shape\n",
    "    K = kernel.shape[0]\n",
    "    \n",
    "    # Output dimensions\n",
    "    out_H = (H - K) // stride + 1\n",
    "    out_W = (W - K) // stride + 1\n",
    "    \n",
    "    # Initialize output\n",
    "    output = np.zeros((out_H, out_W))\n",
    "    \n",
    "    # Perform convolution\n",
    "    for i in range(0, out_H):\n",
    "        for j in range(0, out_W):\n",
    "            # Extract region\n",
    "            region = image[i*stride:i*stride+K, j*stride:j*stride+K]\n",
    "            \n",
    "            # Element-wise multiplication and sum\n",
    "            output[i, j] = np.sum(region * kernel)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Example: Edge detection\n",
    "# Create sample image\n",
    "image = np.array([\n",
    "    [0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 1, 1, 0, 0],\n",
    "    [0, 0, 1, 1, 1, 0, 0],\n",
    "    [0, 0, 1, 1, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0],\n",
    "], dtype=np.float32)\n",
    "\n",
    "# Sobel edge detection kernels\n",
    "sobel_x = np.array([[-1, 0, 1],\n",
    "                    [-2, 0, 2],\n",
    "                    [-1, 0, 1]])\n",
    "\n",
    "sobel_y = np.array([[-1, -2, -1],\n",
    "                    [ 0,  0,  0],\n",
    "                    [ 1,  2,  1]])\n",
    "\n",
    "# Apply convolution\n",
    "edges_x = conv2d_numpy(image, sobel_x, stride=1, padding=1)\n",
    "edges_y = conv2d_numpy(image, sobel_y, stride=1, padding=1)\n",
    "edges = np.sqrt(edges_x**2 + edges_y**2)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "axes[0].imshow(image, cmap='gray')\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(edges_x, cmap='gray')\n",
    "axes[1].set_title('Horizontal Edges (Sobel X)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(edges_y, cmap='gray')\n",
    "axes[2].set_title('Vertical Edges (Sobel Y)')\n",
    "axes[2].axis('off')\n",
    "\n",
    "axes[3].imshow(edges, cmap='gray')\n",
    "axes[3].set_title('Combined Edges')\n",
    "axes[3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConvolution Output Shape:\", edges.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Common Convolution Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common kernels for image processing\n",
    "kernels = {\n",
    "    'Identity': np.array([[0, 0, 0],\n",
    "                          [0, 1, 0],\n",
    "                          [0, 0, 0]]),\n",
    "    \n",
    "    'Sharpen': np.array([[ 0, -1,  0],\n",
    "                         [-1,  5, -1],\n",
    "                         [ 0, -1,  0]]),\n",
    "    \n",
    "    'Blur': np.ones((3, 3)) / 9,\n",
    "    \n",
    "    'Edge Detection': np.array([[-1, -1, -1],\n",
    "                                [-1,  8, -1],\n",
    "                                [-1, -1, -1]]),\n",
    "    \n",
    "    'Emboss': np.array([[-2, -1, 0],\n",
    "                        [-1,  1, 1],\n",
    "                        [ 0,  1, 2]]),\n",
    "}\n",
    "\n",
    "# Apply all kernels to a sample image\n",
    "# Create more interesting sample image\n",
    "sample = np.zeros((15, 15))\n",
    "sample[3:7, 3:12] = 1\n",
    "sample[8:12, 3:7] = 1\n",
    "sample[8:12, 8:12] = 1\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "axes[0].imshow(sample, cmap='gray')\n",
    "axes[0].set_title('Original')\n",
    "axes[0].axis('off')\n",
    "\n",
    "for idx, (name, kernel) in enumerate(kernels.items(), 1):\n",
    "    result = conv2d_numpy(sample, kernel, padding=1)\n",
    "    axes[idx].imshow(result, cmap='gray')\n",
    "    axes[idx].set_title(name)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Building a CNN from Scratch\n",
    "\n",
    "### 2.1 Simple CNN for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    class SimpleCNN(nn.Module):\n",
    "        \"\"\"Simple CNN for digit recognition.\"\"\"\n",
    "        \n",
    "        def __init__(self):\n",
    "            super(SimpleCNN, self).__init__()\n",
    "            \n",
    "            # Convolutional layers\n",
    "            self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "            self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "            self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "            \n",
    "            # Batch normalization\n",
    "            self.bn1 = nn.BatchNorm2d(32)\n",
    "            self.bn2 = nn.BatchNorm2d(64)\n",
    "            self.bn3 = nn.BatchNorm2d(128)\n",
    "            \n",
    "            # Pooling\n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "            \n",
    "            # Fully connected layers\n",
    "            self.fc1 = nn.Linear(128 * 3 * 3, 256)\n",
    "            self.fc2 = nn.Linear(256, 10)\n",
    "            \n",
    "            # Dropout\n",
    "            self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            # Conv block 1: 28x28 -> 14x14\n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.pool(x)\n",
    "            \n",
    "            # Conv block 2: 14x14 -> 7x7\n",
    "            x = self.conv2(x)\n",
    "            x = self.bn2(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.pool(x)\n",
    "            \n",
    "            # Conv block 3: 7x7 -> 3x3\n",
    "            x = self.conv3(x)\n",
    "            x = self.bn3(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.pool(x)\n",
    "            \n",
    "            # Flatten\n",
    "            x = x.view(x.size(0), -1)\n",
    "            \n",
    "            # Fully connected\n",
    "            x = self.fc1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc2(x)\n",
    "            \n",
    "            return x\n",
    "    \n",
    "    # Create model\n",
    "    model = SimpleCNN()\n",
    "    \n",
    "    # Print architecture\n",
    "    print(model)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Test forward pass\n",
    "    dummy_input = torch.randn(4, 1, 28, 28)  # Batch of 4 images\n",
    "    output = model(dummy_input)\n",
    "    print(f\"\\nInput shape: {dummy_input.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "else:\n",
    "    print(\"PyTorch not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualizing Feature Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    def visualize_feature_maps(model, image, layer_num=1):\n",
    "        \"\"\"\n",
    "        Visualize feature maps from a convolutional layer.\n",
    "        \n",
    "        Args:\n",
    "            model: CNN model\n",
    "            image: Input image tensor (1, C, H, W)\n",
    "            layer_num: Which conv layer to visualize (1, 2, or 3)\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        # Forward pass and extract features\n",
    "        with torch.no_grad():\n",
    "            x = image\n",
    "            \n",
    "            # Go through layers up to desired layer\n",
    "            if layer_num >= 1:\n",
    "                x = model.conv1(x)\n",
    "                x = model.bn1(x)\n",
    "                x = F.relu(x)\n",
    "                if layer_num == 1:\n",
    "                    features = x\n",
    "                x = model.pool(x)\n",
    "            \n",
    "            if layer_num >= 2:\n",
    "                x = model.conv2(x)\n",
    "                x = model.bn2(x)\n",
    "                x = F.relu(x)\n",
    "                if layer_num == 2:\n",
    "                    features = x\n",
    "                x = model.pool(x)\n",
    "            \n",
    "            if layer_num >= 3:\n",
    "                x = model.conv3(x)\n",
    "                x = model.bn3(x)\n",
    "                x = F.relu(x)\n",
    "                if layer_num == 3:\n",
    "                    features = x\n",
    "        \n",
    "        # Plot feature maps\n",
    "        features = features.squeeze(0).cpu()  # Remove batch dim\n",
    "        num_features = min(16, features.shape[0])  # Plot first 16\n",
    "        \n",
    "        fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i in range(num_features):\n",
    "            axes[i].imshow(features[i], cmap='viridis')\n",
    "            axes[i].set_title(f'Filter {i+1}')\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        for i in range(num_features, 16):\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.suptitle(f'Feature Maps from Conv Layer {layer_num}', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Create a random input (you can replace with real image)\n",
    "    test_image = torch.randn(1, 1, 28, 28)\n",
    "    \n",
    "    # Visualize different layers\n",
    "    for layer in [1, 2, 3]:\n",
    "        visualize_feature_maps(model, test_image, layer_num=layer)\n",
    "else:\n",
    "    print(\"PyTorch not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Transfer Learning with Pre-trained Models\n",
    "\n",
    "### 3.1 Loading Pre-trained ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    # Load pre-trained ResNet\n",
    "    from torchvision.models import resnet50, ResNet50_Weights\n",
    "    \n",
    "    # Load with pre-trained weights\n",
    "    weights = ResNet50_Weights.DEFAULT\n",
    "    resnet = resnet50(weights=weights)\n",
    "    \n",
    "    # Set to eval mode\n",
    "    resnet.eval()\n",
    "    \n",
    "    # Get preprocessing transforms\n",
    "    preprocess = weights.transforms()\n",
    "    \n",
    "    print(\"ResNet-50 loaded successfully!\")\n",
    "    print(f\"\\nTotal parameters: {sum(p.numel() for p in resnet.parameters()):,}\")\n",
    "    \n",
    "    # Show architecture summary\n",
    "    print(\"\\nArchitecture:\")\n",
    "    for name, module in resnet.named_children():\n",
    "        print(f\"{name}: {module.__class__.__name__}\")\n",
    "else:\n",
    "    print(\"PyTorch not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Fine-tuning for Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    def create_transfer_learning_model(num_classes, freeze_backbone=True):\n",
    "        \"\"\"\n",
    "        Create transfer learning model from ResNet.\n",
    "        \n",
    "        Args:\n",
    "            num_classes: Number of classes for new task\n",
    "            freeze_backbone: Whether to freeze pre-trained weights\n",
    "        \n",
    "        Returns:\n",
    "            model: Modified ResNet model\n",
    "        \"\"\"\n",
    "        # Load pre-trained model\n",
    "        model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        \n",
    "        # Freeze backbone if requested\n",
    "        if freeze_backbone:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Replace final layer\n",
    "        num_features = model.fc.in_features\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    # Example: Create model for 10-class problem\n",
    "    transfer_model = create_transfer_learning_model(num_classes=10, freeze_backbone=True)\n",
    "    \n",
    "    # Count trainable parameters\n",
    "    trainable = sum(p.numel() for p in transfer_model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in transfer_model.parameters())\n",
    "    \n",
    "    print(f\"Total parameters: {total:,}\")\n",
    "    print(f\"Trainable parameters: {trainable:,}\")\n",
    "    print(f\"Frozen parameters: {total - trainable:,}\")\n",
    "    print(f\"\\nTrainable: {100 * trainable / total:.2f}%\")\n",
    "    \n",
    "    # Test forward pass\n",
    "    dummy = torch.randn(2, 3, 224, 224)\n",
    "    output = transfer_model(dummy)\n",
    "    print(f\"\\nOutput shape: {output.shape}\")  # (2, 10)\n",
    "else:\n",
    "    print(\"PyTorch not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Image Classification Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    def classify_image(model, image_path, class_names=None):\n",
    "        \"\"\"\n",
    "        Classify an image using pre-trained model.\n",
    "        \n",
    "        Args:\n",
    "            model: PyTorch model\n",
    "            image_path: Path to image file\n",
    "            class_names: List of class names (optional)\n",
    "        \n",
    "        Returns:\n",
    "            predictions: Top predictions with probabilities\n",
    "        \"\"\"\n",
    "        # Load and preprocess image\n",
    "        from PIL import Image\n",
    "        \n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Define transforms\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        # Transform image\n",
    "        img_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        # Make prediction\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(img_tensor)\n",
    "            probabilities = F.softmax(output, dim=1)\n",
    "        \n",
    "        # Get top 5 predictions\n",
    "        top5_prob, top5_indices = torch.topk(probabilities, 5)\n",
    "        top5_prob = top5_prob.squeeze().cpu().numpy()\n",
    "        top5_indices = top5_indices.squeeze().cpu().numpy()\n",
    "        \n",
    "        # Format results\n",
    "        results = []\n",
    "        for i, (idx, prob) in enumerate(zip(top5_indices, top5_prob)):\n",
    "            class_name = class_names[idx] if class_names else f\"Class {idx}\"\n",
    "            results.append((class_name, prob))\n",
    "        \n",
    "        # Visualize\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Show image\n",
    "        ax1.imshow(img)\n",
    "        ax1.set_title('Input Image')\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        # Show predictions\n",
    "        classes = [r[0] for r in results]\n",
    "        probs = [r[1] for r in results]\n",
    "        \n",
    "        y_pos = np.arange(len(classes))\n",
    "        ax2.barh(y_pos, probs)\n",
    "        ax2.set_yticks(y_pos)\n",
    "        ax2.set_yticklabels(classes)\n",
    "        ax2.invert_yaxis()\n",
    "        ax2.set_xlabel('Probability')\n",
    "        ax2.set_title('Top 5 Predictions')\n",
    "        ax2.set_xlim([0, 1])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    print(\"Image classification function defined.\")\n",
    "    print(\"\\nUsage:\")\n",
    "    print(\"results = classify_image(resnet, 'path/to/image.jpg')\")\n",
    "else:\n",
    "    print(\"PyTorch not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Data Augmentation\n",
    "\n",
    "### 4.1 Standard Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    # Define augmentation pipeline\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(\n",
    "            brightness=0.2,\n",
    "            contrast=0.2,\n",
    "            saturation=0.2,\n",
    "            hue=0.1\n",
    "        ),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "    ])\n",
    "    \n",
    "    # Validation transforms (no augmentation)\n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "    ])\n",
    "    \n",
    "    # Visualize augmentations\n",
    "    def show_augmentations(image_path, transform, num_augmentations=8):\n",
    "        \"\"\"Show multiple augmented versions of an image.\"\"\"\n",
    "        from PIL import Image\n",
    "        \n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        # Original\n",
    "        axes[0].imshow(img)\n",
    "        axes[0].set_title('Original')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Augmented versions\n",
    "        for i in range(1, num_augmentations):\n",
    "            # Apply transform (need to remove normalization for visualization)\n",
    "            aug_transform = transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=15),\n",
    "                transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
    "            ])\n",
    "            \n",
    "            aug_img = aug_transform(img)\n",
    "            axes[i].imshow(aug_img)\n",
    "            axes[i].set_title(f'Augmented {i}')\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"Data augmentation transforms defined.\")\n",
    "    print(\"\\nTo visualize:\")\n",
    "    print(\"show_augmentations('path/to/image.jpg', train_transforms)\")\n",
    "else:\n",
    "    print(\"PyTorch not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Training Loop\n",
    "\n",
    "### 5.1 Complete Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "        \"\"\"Train for one epoch.\"\"\"\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        from tqdm import tqdm\n",
    "        pbar = tqdm(train_loader, desc='Training')\n",
    "        \n",
    "        for inputs, targets in pbar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': running_loss / total,\n",
    "                'acc': 100. * correct / total\n",
    "            })\n",
    "        \n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = correct / total\n",
    "        \n",
    "        return epoch_loss, epoch_acc\n",
    "    \n",
    "    def validate(model, val_loader, criterion, device):\n",
    "        \"\"\"Validate model.\"\"\"\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        val_loss = running_loss / total\n",
    "        val_acc = correct / total\n",
    "        \n",
    "        return val_loss, val_acc\n",
    "    \n",
    "    def train_model(model, train_loader, val_loader, epochs=10, lr=0.001, device='cuda'):\n",
    "        \"\"\"Complete training pipeline.\"\"\"\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "        \n",
    "        # Training history\n",
    "        history = {\n",
    "            'train_loss': [], 'train_acc': [],\n",
    "            'val_loss': [], 'val_acc': []\n",
    "        }\n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_acc = train_one_epoch(\n",
    "                model, train_loader, criterion, optimizer, device\n",
    "            )\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "            \n",
    "            # Update scheduler\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Save history\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_acc'].append(val_acc)\n",
    "            \n",
    "            # Save best model\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                torch.save(model.state_dict(), 'best_model.pth')\n",
    "                print(f\"‚úì New best model saved (val_acc: {val_acc:.4f})\")\n",
    "            \n",
    "            # Print epoch summary\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "            print(f\"LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    print(\"Training pipeline defined.\")\n",
    "    print(\"\\nUsage:\")\n",
    "    print(\"history = train_model(model, train_loader, val_loader, epochs=10)\")\n",
    "else:\n",
    "    print(\"PyTorch not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Visualizing Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training and validation metrics.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "    ax1.plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', fontsize=12)\n",
    "    ax1.set_title('Training and Validation Loss', fontsize=14)\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2.plot(epochs, [100*acc for acc in history['train_acc']], \n",
    "             'b-', label='Train Acc', linewidth=2)\n",
    "    ax2.plot(epochs, [100*acc for acc in history['val_acc']], \n",
    "             'r-', label='Val Acc', linewidth=2)\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax2.set_title('Training and Validation Accuracy', fontsize=14)\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print best results\n",
    "    best_val_acc = max(history['val_acc'])\n",
    "    best_epoch = history['val_acc'].index(best_val_acc) + 1\n",
    "    \n",
    "    print(f\"\\nBest Validation Accuracy: {100*best_val_acc:.2f}% (Epoch {best_epoch})\")\n",
    "\n",
    "print(\"Plotting function defined.\")\n",
    "print(\"Usage: plot_training_history(history)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Grad-CAM Visualization\n",
    "\n",
    "### 6.1 Class Activation Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    class GradCAM:\n",
    "        \"\"\"Gradient-weighted Class Activation Mapping.\"\"\"\n",
    "        \n",
    "        def __init__(self, model, target_layer):\n",
    "            self.model = model\n",
    "            self.target_layer = target_layer\n",
    "            self.gradients = None\n",
    "            self.activations = None\n",
    "            \n",
    "            # Register hooks\n",
    "            target_layer.register_forward_hook(self.save_activation)\n",
    "            target_layer.register_backward_hook(self.save_gradient)\n",
    "        \n",
    "        def save_activation(self, module, input, output):\n",
    "            self.activations = output.detach()\n",
    "        \n",
    "        def save_gradient(self, module, grad_input, grad_output):\n",
    "            self.gradients = grad_output[0].detach()\n",
    "        \n",
    "        def generate_cam(self, input_image, target_class=None):\n",
    "            \"\"\"\n",
    "            Generate Grad-CAM heatmap.\n",
    "            \n",
    "            Args:\n",
    "                input_image: Input tensor (1, C, H, W)\n",
    "                target_class: Target class index (None = predicted class)\n",
    "            \n",
    "            Returns:\n",
    "                cam: Heatmap (H, W)\n",
    "            \"\"\"\n",
    "            # Forward pass\n",
    "            output = self.model(input_image)\n",
    "            \n",
    "            # Get target class\n",
    "            if target_class is None:\n",
    "                target_class = output.argmax(dim=1)\n",
    "            \n",
    "            # Zero gradients\n",
    "            self.model.zero_grad()\n",
    "            \n",
    "            # Backward pass\n",
    "            output[0, target_class].backward()\n",
    "            \n",
    "            # Get weights (global average pooling of gradients)\n",
    "            weights = self.gradients.mean(dim=(2, 3), keepdim=True)\n",
    "            \n",
    "            # Weighted combination of activation maps\n",
    "            cam = (weights * self.activations).sum(dim=1, keepdim=True)\n",
    "            \n",
    "            # ReLU (only positive influence)\n",
    "            cam = F.relu(cam)\n",
    "            \n",
    "            # Normalize\n",
    "            cam = cam.squeeze()\n",
    "            cam = cam - cam.min()\n",
    "            cam = cam / cam.max()\n",
    "            \n",
    "            return cam.cpu().numpy()\n",
    "    \n",
    "    def visualize_gradcam(model, image_tensor, original_image, target_layer):\n",
    "        \"\"\"Visualize Grad-CAM on an image.\"\"\"\n",
    "        # Create Grad-CAM\n",
    "        gradcam = GradCAM(model, target_layer)\n",
    "        \n",
    "        # Generate CAM\n",
    "        cam = gradcam.generate_cam(image_tensor)\n",
    "        \n",
    "        # Resize CAM to match image\n",
    "        cam_resized = cv2.resize(cam, (original_image.width, original_image.height))\n",
    "        \n",
    "        # Create heatmap\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * cam_resized), cv2.COLORMAP_JET)\n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Overlay on original\n",
    "        img_array = np.array(original_image)\n",
    "        overlaid = cv2.addWeighted(img_array, 0.6, heatmap, 0.4, 0)\n",
    "        \n",
    "        # Plot\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        axes[0].imshow(original_image)\n",
    "        axes[0].set_title('Original Image')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(heatmap)\n",
    "        axes[1].set_title('Grad-CAM Heatmap')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        axes[2].imshow(overlaid)\n",
    "        axes[2].set_title('Overlay')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"Grad-CAM visualization defined.\")\n",
    "    print(\"\\nUsage:\")\n",
    "    print(\"# For ResNet, use layer4 as target layer\")\n",
    "    print(\"visualize_gradcam(model, image_tensor, original_image, model.layer4)\")\n",
    "else:\n",
    "    print(\"PyTorch not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Convolutional Layers:**\n",
    "   - Extract spatial features from images\n",
    "   - Share weights across spatial locations\n",
    "   - Build hierarchical representations\n",
    "\n",
    "2. **CNN Architecture:**\n",
    "   - Conv layers: Feature extraction\n",
    "   - Pooling: Dimensionality reduction\n",
    "   - Batch norm: Stabilize training\n",
    "   - Fully connected: Classification\n",
    "\n",
    "3. **Modern Architectures:**\n",
    "   - VGG: Deep networks with small filters\n",
    "   - ResNet: Skip connections enable very deep networks\n",
    "   - Inception: Multi-scale feature extraction\n",
    "   - EfficientNet: Compound scaling\n",
    "\n",
    "4. **Transfer Learning:**\n",
    "   - Use pre-trained weights\n",
    "   - Freeze backbone, train classifier\n",
    "   - Fine-tune entire network\n",
    "   - Much faster than training from scratch\n",
    "\n",
    "5. **Data Augmentation:**\n",
    "   - Prevent overfitting\n",
    "   - Increase effective dataset size\n",
    "   - Improve generalization\n",
    "\n",
    "6. **Visualization:**\n",
    "   - Feature maps: What filters learn\n",
    "   - Grad-CAM: Where model looks\n",
    "   - Helps debug and interpret models\n",
    "\n",
    "### Interview Questions\n",
    "\n",
    "1. **Why use convolution instead of fully connected layers for images?**\n",
    "   - Parameter sharing (fewer parameters)\n",
    "   - Translation invariance\n",
    "   - Preserve spatial structure\n",
    "\n",
    "2. **What does pooling do and why is it useful?**\n",
    "   - Reduces spatial dimensions\n",
    "   - Provides translation invariance\n",
    "   - Reduces computation\n",
    "\n",
    "3. **Explain skip connections in ResNet.**\n",
    "   - Add input directly to output: y = F(x) + x\n",
    "   - Mitigates vanishing gradients\n",
    "   - Easier to optimize (identity easy to learn)\n",
    "   - Enables very deep networks (100+ layers)\n",
    "\n",
    "4. **What is transfer learning and when should you use it?**\n",
    "   - Use pre-trained model weights\n",
    "   - Useful when limited data\n",
    "   - Faster training, better performance\n",
    "   - Fine-tune on your specific task\n",
    "\n",
    "5. **How does Grad-CAM work?**\n",
    "   - Use gradients to weight activation maps\n",
    "   - Shows which regions influenced prediction\n",
    "   - Helps interpret and debug models\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Advanced Topics:** See [DEEP_LEARNING_ARCHITECTURES.md](../DEEP_LEARNING_ARCHITECTURES.md)\n",
    "- **Modern Techniques:** See [MODERN_ML_AI_TECHNIQUES_2024_2025.md](../MODERN_ML_AI_TECHNIQUES_2024_2025.md)\n",
    "- **Projects:** \n",
    "  - Image classification on custom dataset\n",
    "  - Object detection with YOLO\n",
    "  - Image segmentation\n",
    "  - Style transfer\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've mastered Computer Vision with CNNs. You now understand convolutions, modern architectures, transfer learning, and visualization techniques!\n",
    "\n",
    "**Next:** [11 - MLOps & Production Deployment](./11_mlops_production.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
