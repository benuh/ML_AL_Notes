{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Statistics for Machine Learning\n",
    "\n",
    "Welcome to statistical foundations of ML! Statistics helps us understand data, make inferences, and validate models.\n",
    "\n",
    "**Learning Goals:**\n",
    "- Master descriptive statistics\n",
    "- Understand hypothesis testing\n",
    "- Learn statistical inference\n",
    "- Apply confidence intervals\n",
    "- Validate ML models statistically\n",
    "\n",
    "**Sources:**\n",
    "- \"An Introduction to Statistical Learning\" - James et al. (2021)\n",
    "- \"Statistics for Machine Learning\" - Lantz (2019)\n",
    "- \"The Elements of Statistical Learning\" - Hastie, Tibshirani, Friedman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.datasets import load_boston, make_classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette('husl')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully!\")\n",
    "print(f\"NumPy: {np.__version__}, Pandas: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Part 1: Descriptive Statistics - Understanding Your Data\n",
    "\n",
    "Before building models, you MUST understand your data!\n",
    "\n",
    "**Key Concepts:**\n",
    "- Central tendency (mean, median, mode)\n",
    "- Spread (variance, standard deviation, IQR)\n",
    "- Distribution shape (skewness, kurtosis)\n",
    "- Relationships (correlation, covariance)\n",
    "\n",
    "**Source:** \"An Introduction to Statistical Learning\" Chapter 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data: Student exam scores\n",
    "np.random.seed(42)\n",
    "scores = np.concatenate([\n",
    "    np.random.normal(75, 10, 50),   # Class A: mean=75, std=10\n",
    "    np.random.normal(82, 8, 50),    # Class B: mean=82, std=8\n",
    "    np.random.exponential(15, 20) + 60  # Some outliers\n",
    "])\n",
    "scores = np.clip(scores, 0, 100)  # Ensure scores are in [0, 100]\n",
    "\n",
    "print(\"üìä Descriptive Statistics: Exam Scores\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Central Tendency\n",
    "mean_score = np.mean(scores)\n",
    "median_score = np.median(scores)\n",
    "mode_result = stats.mode(scores.round(), keepdims=True)\n",
    "mode_score = mode_result.mode[0]\n",
    "\n",
    "print(\"\\nüìç Central Tendency (typical values):\")\n",
    "print(f\"  Mean (average): {mean_score:.2f}\")\n",
    "print(f\"  Median (middle value): {median_score:.2f}\")\n",
    "print(f\"  Mode (most frequent): {mode_score:.2f}\")\n",
    "\n",
    "# Spread/Dispersion\n",
    "variance = np.var(scores)\n",
    "std_dev = np.std(scores)\n",
    "q1, q3 = np.percentile(scores, [25, 75])\n",
    "iqr = q3 - q1\n",
    "\n",
    "print(\"\\nüìè Spread (variability):\")\n",
    "print(f\"  Variance: {variance:.2f}\")\n",
    "print(f\"  Standard Deviation: {std_dev:.2f}\")\n",
    "print(f\"  Range: [{scores.min():.2f}, {scores.max():.2f}]\")\n",
    "print(f\"  IQR (Q3 - Q1): {iqr:.2f}\")\n",
    "\n",
    "# Shape\n",
    "skewness = stats.skew(scores)\n",
    "kurtosis = stats.kurtosis(scores)\n",
    "\n",
    "print(\"\\nüìê Shape:\")\n",
    "print(f\"  Skewness: {skewness:.2f}\", \"(negative = left skew, positive = right skew)\")\n",
    "print(f\"  Kurtosis: {kurtosis:.2f}\", \"(high = heavy tails, low = light tails)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize descriptive statistics\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "\n",
    "# 1. Histogram with statistics\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "ax1.hist(scores, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax1.axvline(mean_score, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_score:.1f}')\n",
    "ax1.axvline(median_score, color='green', linestyle='--', linewidth=2, label=f'Median: {median_score:.1f}')\n",
    "ax1.set_xlabel('Score')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Distribution with Central Tendency')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Box plot\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "box_parts = ax2.boxplot(scores, vert=True, patch_artist=True)\n",
    "box_parts['boxes'][0].set_facecolor('lightblue')\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.set_title('Box Plot\\n(Shows median, quartiles, outliers)')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add annotations\n",
    "ax2.text(1.2, median_score, f'Median: {median_score:.1f}', fontsize=10)\n",
    "ax2.text(1.2, q1, f'Q1: {q1:.1f}', fontsize=10)\n",
    "ax2.text(1.2, q3, f'Q3: {q3:.1f}', fontsize=10)\n",
    "\n",
    "# 3. Violin plot\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "parts = ax3.violinplot([scores], positions=[1], showmeans=True, showmedians=True)\n",
    "ax3.set_ylabel('Score')\n",
    "ax3.set_title('Violin Plot\\n(Combines box plot + density)')\n",
    "ax3.set_xticks([1])\n",
    "ax3.set_xticklabels(['Scores'])\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Cumulative distribution\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "sorted_scores = np.sort(scores)\n",
    "cumulative = np.arange(1, len(sorted_scores) + 1) / len(sorted_scores)\n",
    "ax4.plot(sorted_scores, cumulative, linewidth=2, color='purple')\n",
    "ax4.axhline(0.5, color='red', linestyle='--', alpha=0.5, label='Median')\n",
    "ax4.axvline(median_score, color='red', linestyle='--', alpha=0.5)\n",
    "ax4.set_xlabel('Score')\n",
    "ax4.set_ylabel('Cumulative Probability')\n",
    "ax4.set_title('Cumulative Distribution Function (CDF)')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Q-Q plot (check normality)\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "stats.probplot(scores, dist=\"norm\", plot=ax5)\n",
    "ax5.set_title('Q-Q Plot\\n(Check if data is normally distributed)')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Summary statistics table\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "ax6.axis('off')\n",
    "\n",
    "summary_data = [\n",
    "    ['Statistic', 'Value'],\n",
    "    ['Count', f'{len(scores):.0f}'],\n",
    "    ['Mean', f'{mean_score:.2f}'],\n",
    "    ['Median', f'{median_score:.2f}'],\n",
    "    ['Std Dev', f'{std_dev:.2f}'],\n",
    "    ['Min', f'{scores.min():.2f}'],\n",
    "    ['25%', f'{q1:.2f}'],\n",
    "    ['75%', f'{q3:.2f}'],\n",
    "    ['Max', f'{scores.max():.2f}'],\n",
    "    ['Skewness', f'{skewness:.2f}'],\n",
    "    ['Kurtosis', f'{kurtosis:.2f}']\n",
    "]\n",
    "\n",
    "table = ax6.table(cellText=summary_data, cellLoc='left', loc='center',\n",
    "                  colWidths=[0.5, 0.3])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1, 2)\n",
    "\n",
    "# Style header row\n",
    "for i in range(2):\n",
    "    table[(0, i)].set_facecolor('lightblue')\n",
    "    table[(0, i)].set_text_props(weight='bold')\n",
    "\n",
    "ax6.set_title('Summary Statistics', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"  ‚Ä¢ Box plots show outliers clearly\")\n",
    "print(\"  ‚Ä¢ Q-Q plot: points on line = normally distributed\")\n",
    "print(\"  ‚Ä¢ CDF shows cumulative probabilities\")\n",
    "print(\"  ‚Ä¢ Violin plot combines density + quartiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Correlation & Covariance: Relationships Between Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate correlated data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "# Create different types of relationships\n",
    "x = np.random.randn(n_samples)\n",
    "y_strong_pos = 2*x + np.random.randn(n_samples)*0.5  # Strong positive\n",
    "y_weak_pos = 0.5*x + np.random.randn(n_samples)*2    # Weak positive\n",
    "y_strong_neg = -2*x + np.random.randn(n_samples)*0.5 # Strong negative\n",
    "y_no_corr = np.random.randn(n_samples)                # No correlation\n",
    "y_nonlinear = x**2 + np.random.randn(n_samples)*0.5  # Nonlinear\n",
    "\n",
    "# Calculate correlations\n",
    "corr_strong_pos = np.corrcoef(x, y_strong_pos)[0, 1]\n",
    "corr_weak_pos = np.corrcoef(x, y_weak_pos)[0, 1]\n",
    "corr_strong_neg = np.corrcoef(x, y_strong_neg)[0, 1]\n",
    "corr_no = np.corrcoef(x, y_no_corr)[0, 1]\n",
    "corr_nonlinear = np.corrcoef(x, y_nonlinear)[0, 1]\n",
    "\n",
    "print(\"üîó Correlation Analysis\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nStrong Positive: r = {corr_strong_pos:.3f}\")\n",
    "print(f\"Weak Positive: r = {corr_weak_pos:.3f}\")\n",
    "print(f\"Strong Negative: r = {corr_strong_neg:.3f}\")\n",
    "print(f\"No Correlation: r = {corr_no:.3f}\")\n",
    "print(f\"Nonlinear: r = {corr_nonlinear:.3f}\")\n",
    "print(\"\\nüí° Note: Correlation only measures LINEAR relationships!\")\n",
    "\n",
    "# Visualize different correlations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "datasets = [\n",
    "    (x, y_strong_pos, f'Strong Positive\\nr = {corr_strong_pos:.3f}'),\n",
    "    (x, y_weak_pos, f'Weak Positive\\nr = {corr_weak_pos:.3f}'),\n",
    "    (x, y_strong_neg, f'Strong Negative\\nr = {corr_strong_neg:.3f}'),\n",
    "    (x, y_no_corr, f'No Correlation\\nr = {corr_no:.3f}'),\n",
    "    (x, y_nonlinear, f'Nonlinear\\nr = {corr_nonlinear:.3f}\\n(Correlation misleading!)'),\n",
    "]\n",
    "\n",
    "for idx, (x_data, y_data, title) in enumerate(datasets):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    ax.scatter(x_data, y_data, alpha=0.6, s=30)\n",
    "    \n",
    "    # Fit line\n",
    "    z = np.polyfit(x_data, y_data, 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax.plot(x_data, p(x_data), \"r--\", linewidth=2, alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Last subplot: correlation interpretation guide\n",
    "ax = axes[1, 2]\n",
    "ax.axis('off')\n",
    "\n",
    "guide_text = \"\"\"\n",
    "üìä Correlation Coefficient (r)\n",
    "\n",
    "Range: -1 to +1\n",
    "\n",
    "+1.0 = Perfect positive\n",
    "+0.7 to +1.0 = Strong positive\n",
    "+0.3 to +0.7 = Moderate positive\n",
    "-0.3 to +0.3 = Weak/None\n",
    "-0.7 to -0.3 = Moderate negative\n",
    "-1.0 to -0.7 = Strong negative\n",
    "-1.0 = Perfect negative\n",
    "\n",
    "‚ö†Ô∏è Warnings:\n",
    "‚Ä¢ Correlation ‚â† Causation!\n",
    "‚Ä¢ Only measures linear relationships\n",
    "‚Ä¢ Sensitive to outliers\n",
    "\"\"\"\n",
    "\n",
    "ax.text(0.1, 0.5, guide_text, fontsize=11, family='monospace',\n",
    "        verticalalignment='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-world example: Correlation matrix with heatmap\n",
    "# Create synthetic housing data\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "\n",
    "# Generate correlated features\n",
    "size = np.random.uniform(1000, 3000, n)\n",
    "bedrooms = np.round(size/500 + np.random.randn(n)*0.5).astype(int)\n",
    "age = np.random.uniform(0, 50, n)\n",
    "distance_to_city = np.random.uniform(1, 30, n)\n",
    "price = (size * 200 + bedrooms * 20000 - age * 1000 - \n",
    "         distance_to_city * 3000 + np.random.randn(n) * 50000)\n",
    "\n",
    "# Create DataFrame\n",
    "housing_data = pd.DataFrame({\n",
    "    'Price ($1000s)': price / 1000,\n",
    "    'Size (sqft)': size,\n",
    "    'Bedrooms': bedrooms,\n",
    "    'Age (years)': age,\n",
    "    'Distance to City (miles)': distance_to_city\n",
    "})\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = housing_data.corr()\n",
    "\n",
    "print(\"üè† Housing Data Correlation Analysis\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(corr_matrix.round(3))\n",
    "\n",
    "# Visualize correlation matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8},\n",
    "            fmt='.2f', ax=axes[0])\n",
    "axes[0].set_title('Correlation Heatmap\\n(Red = positive, Blue = negative)', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "\n",
    "# Pairplot for key relationships\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Create mini scatter plots\n",
    "fig2, mini_axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Price vs Size\n",
    "mini_axes[0, 0].scatter(housing_data['Size (sqft)'], housing_data['Price ($1000s)'], alpha=0.5)\n",
    "mini_axes[0, 0].set_xlabel('Size (sqft)')\n",
    "mini_axes[0, 0].set_ylabel('Price ($1000s)')\n",
    "mini_axes[0, 0].set_title(f'Price vs Size (r={corr_matrix.loc[\"Price ($1000s)\", \"Size (sqft)\"]:.3f})')\n",
    "mini_axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Price vs Age\n",
    "mini_axes[0, 1].scatter(housing_data['Age (years)'], housing_data['Price ($1000s)'], alpha=0.5, color='orange')\n",
    "mini_axes[0, 1].set_xlabel('Age (years)')\n",
    "mini_axes[0, 1].set_ylabel('Price ($1000s)')\n",
    "mini_axes[0, 1].set_title(f'Price vs Age (r={corr_matrix.loc[\"Price ($1000s)\", \"Age (years)\"]:.3f})')\n",
    "mini_axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Price vs Distance\n",
    "mini_axes[1, 0].scatter(housing_data['Distance to City (miles)'], housing_data['Price ($1000s)'], alpha=0.5, color='green')\n",
    "mini_axes[1, 0].set_xlabel('Distance to City (miles)')\n",
    "mini_axes[1, 0].set_ylabel('Price ($1000s)')\n",
    "mini_axes[1, 0].set_title(f'Price vs Distance (r={corr_matrix.loc[\"Price ($1000s)\", \"Distance to City (miles)\"]:.3f})')\n",
    "mini_axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Size vs Bedrooms\n",
    "mini_axes[1, 1].scatter(housing_data['Size (sqft)'], housing_data['Bedrooms'], alpha=0.5, color='red')\n",
    "mini_axes[1, 1].set_xlabel('Size (sqft)')\n",
    "mini_axes[1, 1].set_ylabel('Bedrooms')\n",
    "mini_axes[1, 1].set_title(f'Size vs Bedrooms (r={corr_matrix.loc[\"Size (sqft)\", \"Bedrooms\"]:.3f})')\n",
    "mini_axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° ML Insights:\")\n",
    "print(\"  ‚Ä¢ Strong correlations suggest predictive features\")\n",
    "print(\"  ‚Ä¢ Multicollinearity: highly correlated features can cause issues\")\n",
    "print(\"  ‚Ä¢ Use correlation for feature selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Part 2: Hypothesis Testing - Making Decisions from Data\n",
    "\n",
    "**The Scientific Method for ML:**\n",
    "1. State a hypothesis (claim about data)\n",
    "2. Collect data and calculate test statistic\n",
    "3. Compute p-value\n",
    "4. Make decision (reject or fail to reject hypothesis)\n",
    "\n",
    "**Common Tests:**\n",
    "- t-test (compare means)\n",
    "- Chi-square (categorical data)\n",
    "- ANOVA (compare multiple groups)\n",
    "- A/B testing\n",
    "\n",
    "**Source:** \"Statistics for Machine Learning\" - Lantz, Chapter 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: A/B Testing for ML Models\n",
    "print(\"üî¨ Hypothesis Testing: A/B Test for Model Performance\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Scenario: Two ML models, which is better?\n",
    "np.random.seed(42)\n",
    "\n",
    "# Model A accuracies over 30 runs\n",
    "model_a_scores = np.random.normal(0.85, 0.03, 30)\n",
    "\n",
    "# Model B accuracies over 30 runs (slightly better)\n",
    "model_b_scores = np.random.normal(0.87, 0.03, 30)\n",
    "\n",
    "print(\"\\nüìä Data:\")\n",
    "print(f\"  Model A: mean = {model_a_scores.mean():.4f}, std = {model_a_scores.std():.4f}\")\n",
    "print(f\"  Model B: mean = {model_b_scores.mean():.4f}, std = {model_b_scores.std():.4f}\")\n",
    "print(f\"  Difference: {model_b_scores.mean() - model_a_scores.mean():.4f}\")\n",
    "\n",
    "# Hypothesis Testing\n",
    "print(\"\\nüß™ Hypothesis Test:\")\n",
    "print(\"  H‚ÇÄ (Null): Model A and B have same performance\")\n",
    "print(\"  H‚ÇÅ (Alternative): Model B is better than Model A\")\n",
    "print(\"  Significance level (Œ±): 0.05\")\n",
    "\n",
    "# Perform independent t-test\n",
    "t_statistic, p_value = stats.ttest_ind(model_b_scores, model_a_scores)\n",
    "\n",
    "print(f\"\\nüìà Test Results:\")\n",
    "print(f\"  t-statistic: {t_statistic:.4f}\")\n",
    "print(f\"  p-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"\\n‚úÖ REJECT null hypothesis (p < 0.05)\")\n",
    "    print(f\"  ‚Üí Model B is SIGNIFICANTLY better than Model A\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå FAIL TO REJECT null hypothesis (p ‚â• 0.05)\")\n",
    "    print(f\"  ‚Üí No significant difference between models\")\n",
    "\n",
    "# Effect size (Cohen's d)\n",
    "pooled_std = np.sqrt((model_a_scores.std()**2 + model_b_scores.std()**2) / 2)\n",
    "cohens_d = (model_b_scores.mean() - model_a_scores.mean()) / pooled_std\n",
    "\n",
    "print(f\"\\nüìè Effect Size (Cohen's d): {cohens_d:.4f}\")\n",
    "if abs(cohens_d) < 0.2:\n",
    "    effect = \"small\"\n",
    "elif abs(cohens_d) < 0.5:\n",
    "    effect = \"medium\"\n",
    "else:\n",
    "    effect = \"large\"\n",
    "print(f\"  ‚Üí {effect.capitalize()} practical difference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hypothesis test\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Distributions\n",
    "axes[0].hist(model_a_scores, bins=15, alpha=0.7, label='Model A', color='blue', edgecolor='black')\n",
    "axes[0].hist(model_b_scores, bins=15, alpha=0.7, label='Model B', color='red', edgecolor='black')\n",
    "axes[0].axvline(model_a_scores.mean(), color='blue', linestyle='--', linewidth=2, label=f'Mean A: {model_a_scores.mean():.3f}')\n",
    "axes[0].axvline(model_b_scores.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean B: {model_b_scores.mean():.3f}')\n",
    "axes[0].set_xlabel('Accuracy')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Model Performance Distributions')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Box plots for comparison\n",
    "axes[1].boxplot([model_a_scores, model_b_scores], labels=['Model A', 'Model B'],\n",
    "                patch_artist=True,\n",
    "                boxprops=dict(facecolor='lightblue'),\n",
    "                medianprops=dict(color='red', linewidth=2))\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Box Plot Comparison')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 3: t-distribution with test statistic\n",
    "df = len(model_a_scores) + len(model_b_scores) - 2  # degrees of freedom\n",
    "x = np.linspace(-4, 4, 100)\n",
    "y = stats.t.pdf(x, df)\n",
    "\n",
    "axes[2].plot(x, y, 'b-', linewidth=2, label='t-distribution')\n",
    "axes[2].fill_between(x[x >= stats.t.ppf(0.95, df)], 0, \n",
    "                      stats.t.pdf(x[x >= stats.t.ppf(0.95, df)], df),\n",
    "                      alpha=0.3, color='red', label='Rejection region (Œ±=0.05)')\n",
    "axes[2].axvline(t_statistic, color='green', linestyle='--', linewidth=2, \n",
    "                label=f't-stat: {t_statistic:.2f}')\n",
    "axes[2].set_xlabel('t-value')\n",
    "axes[2].set_ylabel('Probability Density')\n",
    "axes[2].set_title(f't-Test Visualization\\np-value = {p_value:.4f}')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Concepts:\")\n",
    "print(\"  ‚Ä¢ p-value: Probability of observing this difference by chance\")\n",
    "print(\"  ‚Ä¢ Œ± = 0.05: We accept 5% chance of false positive (Type I error)\")\n",
    "print(\"  ‚Ä¢ Statistical significance ‚â† practical significance\")\n",
    "print(\"  ‚Ä¢ Always report effect size along with p-value!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Part 3: Confidence Intervals - Quantifying Uncertainty\n",
    "\n",
    "**Intuition:** A confidence interval gives a range of plausible values for a parameter.\n",
    "\n",
    "**95% CI Interpretation:**\n",
    "If we repeated this experiment 100 times, about 95 of the intervals would contain the true parameter.\n",
    "\n",
    "**ML Applications:**\n",
    "- Model performance estimates\n",
    "- Prediction intervals\n",
    "- Feature importance ranges\n",
    "\n",
    "**Source:** \"An Introduction to Statistical Learning\" Chapter 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Confidence interval for model accuracy\n",
    "print(\"üìä Confidence Intervals for ML Model Performance\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Simulate model evaluation\n",
    "np.random.seed(42)\n",
    "n_trials = 50\n",
    "model_accuracies = np.random.normal(0.88, 0.04, n_trials)\n",
    "\n",
    "# Calculate statistics\n",
    "mean_acc = model_accuracies.mean()\n",
    "std_acc = model_accuracies.std(ddof=1)  # sample std\n",
    "se_acc = std_acc / np.sqrt(n_trials)     # standard error\n",
    "\n",
    "# 95% confidence interval (t-distribution)\n",
    "confidence_level = 0.95\n",
    "alpha = 1 - confidence_level\n",
    "t_critical = stats.t.ppf(1 - alpha/2, n_trials - 1)\n",
    "\n",
    "margin_of_error = t_critical * se_acc\n",
    "ci_lower = mean_acc - margin_of_error\n",
    "ci_upper = mean_acc + margin_of_error\n",
    "\n",
    "print(f\"\\nüìà Model Performance Statistics:\")\n",
    "print(f\"  Number of trials: {n_trials}\")\n",
    "print(f\"  Mean accuracy: {mean_acc:.4f}\")\n",
    "print(f\"  Standard deviation: {std_acc:.4f}\")\n",
    "print(f\"  Standard error: {se_acc:.4f}\")\n",
    "\n",
    "print(f\"\\nüéØ 95% Confidence Interval:\")\n",
    "print(f\"  [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "print(f\"  Margin of error: ¬±{margin_of_error:.4f}\")\n",
    "\n",
    "print(f\"\\nüí° Interpretation:\")\n",
    "print(f\"  We are 95% confident that the true model accuracy\")\n",
    "print(f\"  is between {ci_lower:.2%} and {ci_upper:.2%}\")\n",
    "\n",
    "# Calculate different confidence levels\n",
    "confidence_levels = [0.90, 0.95, 0.99]\n",
    "intervals = []\n",
    "\n",
    "print(f\"\\nüìä Different Confidence Levels:\")\n",
    "for conf in confidence_levels:\n",
    "    t_crit = stats.t.ppf(1 - (1-conf)/2, n_trials - 1)\n",
    "    margin = t_crit * se_acc\n",
    "    lower = mean_acc - margin\n",
    "    upper = mean_acc + margin\n",
    "    intervals.append((lower, upper, margin))\n",
    "    print(f\"  {conf*100:.0f}% CI: [{lower:.4f}, {upper:.4f}] (¬±{margin:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confidence intervals\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Distribution with CI\n",
    "axes[0].hist(model_accuracies, bins=20, alpha=0.7, color='skyblue', \n",
    "             edgecolor='black', density=True)\n",
    "\n",
    "# Overlay normal distribution\n",
    "x = np.linspace(model_accuracies.min(), model_accuracies.max(), 100)\n",
    "axes[0].plot(x, stats.norm.pdf(x, mean_acc, std_acc), 'r-', \n",
    "             linewidth=2, label='Normal fit')\n",
    "\n",
    "# Mark CI\n",
    "axes[0].axvline(mean_acc, color='green', linestyle='-', linewidth=2, \n",
    "                label=f'Mean: {mean_acc:.3f}')\n",
    "axes[0].axvline(ci_lower, color='orange', linestyle='--', linewidth=2, \n",
    "                label=f'95% CI: [{ci_lower:.3f}, {ci_upper:.3f}]')\n",
    "axes[0].axvline(ci_upper, color='orange', linestyle='--', linewidth=2)\n",
    "axes[0].fill_betweenx([0, axes[0].get_ylim()[1]], ci_lower, ci_upper, \n",
    "                       alpha=0.2, color='orange')\n",
    "\n",
    "axes[0].set_xlabel('Accuracy')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].set_title('Model Accuracy Distribution with 95% CI')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Multiple confidence levels\n",
    "y_pos = np.arange(len(confidence_levels))\n",
    "colors = ['lightgreen', 'lightblue', 'lightcoral']\n",
    "\n",
    "for i, (conf, (lower, upper, margin)) in enumerate(zip(confidence_levels, intervals)):\n",
    "    axes[1].barh(i, upper - lower, left=lower, height=0.5, \n",
    "                 color=colors[i], alpha=0.7, edgecolor='black',\n",
    "                 label=f'{conf*100:.0f}% CI')\n",
    "    axes[1].plot(mean_acc, i, 'ko', markersize=10)\n",
    "\n",
    "axes[1].axvline(mean_acc, color='red', linestyle='--', linewidth=2, \n",
    "                alpha=0.7, label='Mean')\n",
    "axes[1].set_yticks(y_pos)\n",
    "axes[1].set_yticklabels([f'{int(c*100)}%' for c in confidence_levels])\n",
    "axes[1].set_xlabel('Accuracy')\n",
    "axes[1].set_ylabel('Confidence Level')\n",
    "axes[1].set_title('Confidence Intervals at Different Levels\\n(Higher confidence = wider interval)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"  ‚Ä¢ Wider CI = more uncertainty\")\n",
    "print(\"  ‚Ä¢ Higher confidence level = wider interval\")\n",
    "print(\"  ‚Ä¢ Larger sample size = narrower interval\")\n",
    "print(\"  ‚Ä¢ CI gives range, not just point estimate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Part 4: Statistical Validation of ML Models\n",
    "\n",
    "**Why Statistics Matters in ML:**\n",
    "- Evaluate if improvements are real or due to chance\n",
    "- Compare multiple models rigorously\n",
    "- Report results with uncertainty\n",
    "- Avoid overfitting\n",
    "\n",
    "**Key Techniques:**\n",
    "- Cross-validation\n",
    "- Bootstrap\n",
    "- Permutation tests\n",
    "- Multiple testing correction\n",
    "\n",
    "**Source:** \"The Elements of Statistical Learning\" Chapter 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation with statistical analysis\n",
    "print(\"üî¨ Statistical Validation of ML Models\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Generate synthetic classification data\n",
    "X, y = make_classification(n_samples=500, n_features=20, n_informative=15,\n",
    "                          n_redundant=5, random_state=42)\n",
    "\n",
    "# Train multiple models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(kernel='rbf', random_state=42)\n",
    "}\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv_results = {}\n",
    "n_folds = 10\n",
    "\n",
    "print(\"\\nüîÑ Performing 10-fold Cross-Validation...\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X, y, cv=n_folds, scoring='accuracy')\n",
    "    cv_results[name] = scores\n",
    "    \n",
    "    mean_score = scores.mean()\n",
    "    std_score = scores.std()\n",
    "    se_score = std_score / np.sqrt(n_folds)\n",
    "    \n",
    "    # 95% CI\n",
    "    t_crit = stats.t.ppf(0.975, n_folds - 1)\n",
    "    ci_lower = mean_score - t_crit * se_score\n",
    "    ci_upper = mean_score + t_crit * se_score\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Mean: {mean_score:.4f} ¬± {std_score:.4f}\")\n",
    "    print(f\"  95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "    print()\n",
    "\n",
    "# Statistical comparison between best two models\n",
    "model_names = list(cv_results.keys())\n",
    "best_model = max(cv_results, key=lambda k: cv_results[k].mean())\n",
    "best_scores = cv_results[best_model]\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model}\")\n",
    "print(f\"   Mean Accuracy: {best_scores.mean():.4f}\")\n",
    "\n",
    "# Compare best model with others\n",
    "print(f\"\\nüî¨ Statistical Comparisons (Paired t-test):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, scores in cv_results.items():\n",
    "    if name != best_model:\n",
    "        t_stat, p_val = stats.ttest_rel(best_scores, scores)\n",
    "        \n",
    "        print(f\"\\n{best_model} vs {name}:\")\n",
    "        print(f\"  Mean difference: {best_scores.mean() - scores.mean():.4f}\")\n",
    "        print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "        print(f\"  p-value: {p_val:.4f}\")\n",
    "        \n",
    "        if p_val < 0.05:\n",
    "            print(f\"  ‚úÖ {best_model} is SIGNIFICANTLY better\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå No significant difference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-validation results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Box plots\n",
    "positions = np.arange(len(cv_results))\n",
    "data_to_plot = [scores for scores in cv_results.values()]\n",
    "\n",
    "bp = axes[0].boxplot(data_to_plot, labels=cv_results.keys(), \n",
    "                     patch_artist=True, showmeans=True)\n",
    "\n",
    "# Color boxes\n",
    "colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('Cross-Validation Results\\n(Box = IQR, Line = Median, Triangle = Mean)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: Means with 95% CI error bars\n",
    "means = [scores.mean() for scores in cv_results.values()]\n",
    "stds = [scores.std() for scores in cv_results.values()]\n",
    "ses = [std / np.sqrt(n_folds) for std in stds]\n",
    "t_crit = stats.t.ppf(0.975, n_folds - 1)\n",
    "ci_margins = [t_crit * se for se in ses]\n",
    "\n",
    "x_pos = np.arange(len(cv_results))\n",
    "axes[1].bar(x_pos, means, yerr=ci_margins, capsize=10, alpha=0.7,\n",
    "           color=colors, edgecolor='black', linewidth=1.5,\n",
    "           error_kw={'linewidth': 2, 'ecolor': 'darkred'})\n",
    "\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(cv_results.keys(), rotation=45, ha='right')\n",
    "axes[1].set_ylabel('Mean Accuracy', fontsize=12)\n",
    "axes[1].set_title('Mean Accuracy with 95% Confidence Intervals\\n(Error bars show uncertainty)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (mean, margin) in enumerate(zip(means, ci_margins)):\n",
    "    axes[1].text(i, mean + margin + 0.01, f'{mean:.3f}', \n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Best Practices:\")\n",
    "print(\"  ‚Ä¢ Always use cross-validation, never single train/test split\")\n",
    "print(\"  ‚Ä¢ Report mean ¬± std or 95% CI\")\n",
    "print(\"  ‚Ä¢ Use paired t-test for cross-validation folds (same data splits)\")\n",
    "print(\"  ‚Ä¢ Consider multiple testing correction (Bonferroni)\")\n",
    "print(\"  ‚Ä¢ Statistical significance + practical significance both matter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéÆ Interactive Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this exercise\n",
    "\n",
    "# Given dataset: House prices\n",
    "house_prices = np.array([250000, 320000, 180000, 450000, 290000, \n",
    "                         310000, 275000, 520000, 195000, 340000,\n",
    "                         1500000])  # Note: one outlier!\n",
    "\n",
    "# Task 1: Calculate mean, median, and standard deviation\n",
    "mean_price = None  # YOUR CODE HERE\n",
    "median_price = None  # YOUR CODE HERE\n",
    "std_price = None  # YOUR CODE HERE\n",
    "\n",
    "# Task 2: Identify outliers using IQR method\n",
    "# Rule: Outlier if value < Q1 - 1.5*IQR or value > Q3 + 1.5*IQR\n",
    "q1 = None  # YOUR CODE HERE\n",
    "q3 = None  # YOUR CODE HERE\n",
    "iqr = None  # YOUR CODE HERE\n",
    "outliers = None  # YOUR CODE HERE (return array of outlier values)\n",
    "\n",
    "# Task 3: Calculate z-scores and identify outliers (|z| > 3)\n",
    "z_scores = None  # YOUR CODE HERE\n",
    "z_outliers = None  # YOUR CODE HERE\n",
    "\n",
    "print(\"‚úÖ Solutions:\")\n",
    "print(f\"Mean: ${mean_price:,.0f}\" if mean_price else \"Not computed\")\n",
    "print(f\"Median: ${median_price:,.0f}\" if median_price else \"Not computed\")\n",
    "print(f\"Std Dev: ${std_price:,.0f}\" if std_price else \"Not computed\")\n",
    "print(f\"IQR outliers: {outliers}\")\n",
    "print(f\"Z-score outliers: {z_outliers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this exercise\n",
    "\n",
    "# Scenario: Testing if a new feature improves model performance\n",
    "baseline_scores = np.array([0.82, 0.85, 0.83, 0.86, 0.84, 0.81, 0.87, 0.83, 0.85, 0.84])\n",
    "new_feature_scores = np.array([0.86, 0.88, 0.85, 0.89, 0.87, 0.86, 0.90, 0.85, 0.88, 0.87])\n",
    "\n",
    "# Task 1: Perform paired t-test\n",
    "# H0: New feature doesn't improve performance\n",
    "# H1: New feature improves performance\n",
    "t_statistic = None  # YOUR CODE HERE\n",
    "p_value = None  # YOUR CODE HERE\n",
    "\n",
    "# Task 2: Calculate effect size (Cohen's d for paired samples)\n",
    "# d = mean(differences) / std(differences)\n",
    "differences = None  # YOUR CODE HERE\n",
    "cohens_d = None  # YOUR CODE HERE\n",
    "\n",
    "# Task 3: Calculate 95% CI for the difference\n",
    "mean_diff = None  # YOUR CODE HERE\n",
    "se_diff = None  # YOUR CODE HERE (standard error of differences)\n",
    "ci_lower = None  # YOUR CODE HERE\n",
    "ci_upper = None  # YOUR CODE HERE\n",
    "\n",
    "print(\"‚úÖ Solutions:\")\n",
    "print(f\"t-statistic: {t_statistic}\" if t_statistic else \"Not computed\")\n",
    "print(f\"p-value: {p_value}\" if p_value else \"Not computed\")\n",
    "print(f\"Cohen's d: {cohens_d}\" if cohens_d else \"Not computed\")\n",
    "print(f\"95% CI for difference: [{ci_lower}, {ci_upper}]\" if ci_lower else \"Not computed\")\n",
    "\n",
    "if p_value and p_value < 0.05:\n",
    "    print(\"‚úÖ New feature significantly improves performance!\")\n",
    "else:\n",
    "    print(\"‚ùå No significant improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Bootstrap Confidence Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement bootstrap confidence interval\n",
    "\n",
    "def bootstrap_ci(data, statistic_func, n_bootstrap=10000, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Calculate bootstrap confidence interval\n",
    "    \n",
    "    Parameters:\n",
    "    - data: original sample\n",
    "    - statistic_func: function to compute statistic (e.g., np.mean)\n",
    "    - n_bootstrap: number of bootstrap samples\n",
    "    - confidence: confidence level\n",
    "    \n",
    "    Returns:\n",
    "    - (lower_bound, upper_bound)\n",
    "    \"\"\"\n",
    "    bootstrap_statistics = []\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        # YOUR CODE HERE\n",
    "        # 1. Resample with replacement\n",
    "        # 2. Calculate statistic on resample\n",
    "        # 3. Append to bootstrap_statistics\n",
    "        pass\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # Calculate percentiles for CI\n",
    "    alpha = 1 - confidence\n",
    "    lower = None  # (alpha/2) percentile\n",
    "    upper = None  # (1 - alpha/2) percentile\n",
    "    \n",
    "    return lower, upper\n",
    "\n",
    "# Test your implementation\n",
    "sample_data = np.array([85, 90, 78, 92, 88, 76, 95, 89, 84, 91])\n",
    "\n",
    "mean_ci = bootstrap_ci(sample_data, np.mean, n_bootstrap=10000)\n",
    "median_ci = bootstrap_ci(sample_data, np.median, n_bootstrap=10000)\n",
    "\n",
    "print(\"‚úÖ Bootstrap Confidence Intervals:\")\n",
    "print(f\"Mean: {np.mean(sample_data):.2f}, 95% CI: {mean_ci}\" if mean_ci[0] else \"Not computed\")\n",
    "print(f\"Median: {np.median(sample_data):.2f}, 95% CI: {median_ci}\" if median_ci[0] else \"Not computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Summary & Next Steps\n",
    "\n",
    "### ‚úÖ What You've Learned:\n",
    "\n",
    "**Descriptive Statistics:**\n",
    "- Central tendency (mean, median, mode)\n",
    "- Spread (variance, std, IQR)\n",
    "- Correlation and relationships\n",
    "- Visualizing distributions\n",
    "\n",
    "**Inferential Statistics:**\n",
    "- Hypothesis testing framework\n",
    "- p-values and significance\n",
    "- Confidence intervals\n",
    "- Effect sizes\n",
    "\n",
    "**ML Validation:**\n",
    "- Cross-validation\n",
    "- Model comparison\n",
    "- Reporting uncertainty\n",
    "- Statistical significance\n",
    "\n",
    "### üîë Key Formulas:\n",
    "\n",
    "1. **Standard Error**: $SE = \\frac{\\sigma}{\\sqrt{n}}$\n",
    "\n",
    "2. **Confidence Interval**: $\\bar{x} \\pm t_{\\alpha/2} \\cdot SE$\n",
    "\n",
    "3. **t-statistic**: $t = \\frac{\\bar{x}_1 - \\bar{x}_2}{SE_{\\text{diff}}}$\n",
    "\n",
    "4. **Cohen's d**: $d = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sigma_{\\text{pooled}}}$\n",
    "\n",
    "5. **Correlation**: $r = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum(x_i - \\bar{x})^2 \\sum(y_i - \\bar{y})^2}}$\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "1. **[Data Processing](04_data_processing.ipynb)** - Clean and prepare data\n",
    "2. **[Classical ML](05_classical_ml.ipynb)** - Apply statistical ML algorithms\n",
    "3. **[Deep Learning](06_deep_learning.ipynb)** - Neural networks and beyond\n",
    "\n",
    "### üìñ Recommended Reading:\n",
    "\n",
    "- \"An Introduction to Statistical Learning\" - James et al.\n",
    "- \"Statistics for Machine Learning\" - Lantz\n",
    "- \"The Elements of Statistical Learning\" - Hastie et al.\n",
    "- \"Statistical Rethinking\" - McElreath\n",
    "\n",
    "### üí™ Challenge Problems:\n",
    "\n",
    "1. Implement A/B testing framework for ML models\n",
    "2. Create bootstrap resampling for any statistic\n",
    "3. Build statistical power calculator\n",
    "4. Implement multiple testing correction (Bonferroni, FDR)\n",
    "\n",
    "**Remember**: Statistics is essential for rigorous ML. Never report results without uncertainty measures! üìä"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
