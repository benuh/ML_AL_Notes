{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 MLOps & Production Deployment\n",
    "\n",
    "## From Experiment to Production-Ready ML Systems\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Experiment tracking and model versioning\n",
    "- Model deployment (Flask, FastAPI, Docker)\n",
    "- CI/CD pipelines for ML\n",
    "- Monitoring and logging\n",
    "- A/B testing strategies\n",
    "- Data drift detection\n",
    "- Production best practices\n",
    "\n",
    "**Prerequisites:** Basic ML knowledge, Python programming\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import json\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.datasets import load_iris, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Plotting\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Experiment Tracking with MLflow\n",
    "\n",
    "### 1.1 Setting up MLflow\n",
    "\n",
    "**MLflow:** Open-source platform for ML lifecycle management\n",
    "\n",
    "**Key Features:**\n",
    "- Track experiments (parameters, metrics, artifacts)\n",
    "- Package code in reproducible format\n",
    "- Deploy models to various platforms\n",
    "- Model registry for versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install MLflow if needed\n",
    "# !pip install mlflow\n",
    "\n",
    "try:\n",
    "    import mlflow\n",
    "    import mlflow.sklearn\n",
    "    print(f\"MLflow version: {mlflow.__version__}\")\n",
    "    MLFLOW_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"MLflow not installed. Install with: pip install mlflow\")\n",
    "    MLFLOW_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Tracking Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MLFLOW_AVAILABLE:\n",
    "    # Load data\n",
    "    X, y = make_classification(\n",
    "        n_samples=1000, n_features=20, n_informative=15,\n",
    "        n_redundant=5, random_state=42\n",
    "    )\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Set experiment\n",
    "    mlflow.set_experiment(\"iris_classification\")\n",
    "    \n",
    "    # Function to train and track\n",
    "    def train_with_mlflow(n_estimators, max_depth, min_samples_split):\n",
    "        \"\"\"Train model and track with MLflow.\"\"\"\n",
    "        \n",
    "        with mlflow.start_run():\n",
    "            # Log parameters\n",
    "            mlflow.log_param(\"n_estimators\", n_estimators)\n",
    "            mlflow.log_param(\"max_depth\", max_depth)\n",
    "            mlflow.log_param(\"min_samples_split\", min_samples_split)\n",
    "            \n",
    "            # Train model\n",
    "            model = RandomForestClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                min_samples_split=min_samples_split,\n",
    "                random_state=42\n",
    "            )\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Predictions\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, average='weighted')\n",
    "            recall = recall_score(y_test, y_pred, average='weighted')\n",
    "            f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "            \n",
    "            # Log metrics\n",
    "            mlflow.log_metric(\"accuracy\", accuracy)\n",
    "            mlflow.log_metric(\"precision\", precision)\n",
    "            mlflow.log_metric(\"recall\", recall)\n",
    "            mlflow.log_metric(\"f1_score\", f1)\n",
    "            \n",
    "            # Log model\n",
    "            mlflow.sklearn.log_model(model, \"model\")\n",
    "            \n",
    "            # Log artifacts (e.g., plots)\n",
    "            from sklearn.metrics import confusion_matrix\n",
    "            import matplotlib.pyplot as plt\n",
    "            \n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "            plt.title('Confusion Matrix')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.xlabel('Predicted Label')\n",
    "            plt.savefig('confusion_matrix.png')\n",
    "            mlflow.log_artifact('confusion_matrix.png')\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"Run completed: Accuracy = {accuracy:.4f}\")\n",
    "            \n",
    "            return model, accuracy\n",
    "    \n",
    "    # Run multiple experiments\n",
    "    print(\"Running experiments with different hyperparameters...\\n\")\n",
    "    \n",
    "    experiments = [\n",
    "        (50, 5, 2),\n",
    "        (100, 10, 2),\n",
    "        (100, 15, 5),\n",
    "        (200, 20, 2),\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for n_est, max_d, min_split in experiments:\n",
    "        print(f\"Training: n_estimators={n_est}, max_depth={max_d}, min_samples_split={min_split}\")\n",
    "        model, acc = train_with_mlflow(n_est, max_d, min_split)\n",
    "        results.append((n_est, max_d, min_split, acc))\n",
    "        print()\n",
    "    \n",
    "    # Display results\n",
    "    results_df = pd.DataFrame(\n",
    "        results, \n",
    "        columns=['n_estimators', 'max_depth', 'min_samples_split', 'accuracy']\n",
    "    )\n",
    "    print(\"\\nExperiment Results:\")\n",
    "    print(results_df.to_string(index=False))\n",
    "    print(\"\\nTo view experiments: mlflow ui\")\n",
    "else:\n",
    "    print(\"MLflow not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MLFLOW_AVAILABLE:\n",
    "    # Register best model\n",
    "    model_name = \"iris_classifier\"\n",
    "    \n",
    "    # In practice, you would:\n",
    "    # 1. Get the best run from experiments\n",
    "    # 2. Register the model\n",
    "    # 3. Transition to \"Production\" stage\n",
    "    \n",
    "    print(\"Model Registry Workflow:\")\n",
    "    print(\"\"\"\n",
    "    1. Register model:\n",
    "       mlflow.register_model(\n",
    "           model_uri=f\"runs:/{run_id}/model\",\n",
    "           name=\"iris_classifier\"\n",
    "       )\n",
    "    \n",
    "    2. Transition to production:\n",
    "       client = mlflow.tracking.MlflowClient()\n",
    "       client.transition_model_version_stage(\n",
    "           name=\"iris_classifier\",\n",
    "           version=1,\n",
    "           stage=\"Production\"\n",
    "       )\n",
    "    \n",
    "    3. Load production model:\n",
    "       model = mlflow.pyfunc.load_model(\n",
    "           model_uri=f\"models:/{model_name}/Production\"\n",
    "       )\n",
    "    \"\"\")\n",
    "else:\n",
    "    print(\"MLflow not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Model Deployment\n",
    "\n",
    "### 2.1 Flask API for Model Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a simple model for deployment examples\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "joblib.dump(model, 'model.pkl')\n",
    "print(\"Model saved to model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app_flask.py\n",
    "\"\"\"\n",
    "Flask API for model serving.\n",
    "\n",
    "Run with: python app_flask.py\n",
    "Test with: curl -X POST http://localhost:5000/predict -H \"Content-Type: application/json\" -d '{\"features\": [1.0, 2.0, ...]}'\n",
    "\"\"\"\n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load model at startup\n",
    "model = joblib.load('model.pkl')\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    \"\"\"Health check endpoint.\"\"\"\n",
    "    return jsonify({'status': 'healthy'})\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    \"\"\"Prediction endpoint.\"\"\"\n",
    "    try:\n",
    "        # Get data from request\n",
    "        data = request.get_json()\n",
    "        features = np.array(data['features']).reshape(1, -1)\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model.predict(features)\n",
    "        probability = model.predict_proba(features)\n",
    "        \n",
    "        # Return response\n",
    "        response = {\n",
    "            'prediction': int(prediction[0]),\n",
    "            'probability': probability[0].tolist()\n",
    "        }\n",
    "        \n",
    "        return jsonify(response)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 400\n",
    "\n",
    "@app.route('/batch_predict', methods=['POST'])\n",
    "def batch_predict():\n",
    "    \"\"\"Batch prediction endpoint.\"\"\"\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        features = np.array(data['features'])\n",
    "        \n",
    "        predictions = model.predict(features)\n",
    "        probabilities = model.predict_proba(features)\n",
    "        \n",
    "        response = {\n",
    "            'predictions': predictions.tolist(),\n",
    "            'probabilities': probabilities.tolist()\n",
    "        }\n",
    "        \n",
    "        return jsonify(response)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 400\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 FastAPI for High-Performance Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app_fastapi.py\n",
    "\"\"\"\n",
    "FastAPI for high-performance model serving.\n",
    "\n",
    "Run with: uvicorn app_fastapi:app --reload\n",
    "Docs: http://localhost:8000/docs\n",
    "\"\"\"\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "import joblib\n",
    "import numpy as np\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"ML Model API\",\n",
    "    description=\"API for serving ML model predictions\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = joblib.load('model.pkl')\n",
    "\n",
    "# Request/Response schemas\n",
    "class PredictionInput(BaseModel):\n",
    "    features: List[float] = Field(..., description=\"Input features for prediction\")\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"features\": [1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "            }\n",
    "        }\n",
    "\n",
    "class PredictionOutput(BaseModel):\n",
    "    prediction: int\n",
    "    probability: List[float]\n",
    "    confidence: float\n",
    "\n",
    "class BatchPredictionInput(BaseModel):\n",
    "    features: List[List[float]]\n",
    "\n",
    "class BatchPredictionOutput(BaseModel):\n",
    "    predictions: List[int]\n",
    "    probabilities: List[List[float]]\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"ML Model API\", \"version\": \"1.0.0\"}\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictionOutput)\n",
    "async def predict(input_data: PredictionInput):\n",
    "    \"\"\"Make a single prediction.\"\"\"\n",
    "    try:\n",
    "        features = np.array(input_data.features).reshape(1, -1)\n",
    "        \n",
    "        prediction = model.predict(features)\n",
    "        probability = model.predict_proba(features)\n",
    "        \n",
    "        return PredictionOutput(\n",
    "            prediction=int(prediction[0]),\n",
    "            probability=probability[0].tolist(),\n",
    "            confidence=float(max(probability[0]))\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))\n",
    "\n",
    "@app.post(\"/batch_predict\", response_model=BatchPredictionOutput)\n",
    "async def batch_predict(input_data: BatchPredictionInput):\n",
    "    \"\"\"Make batch predictions.\"\"\"\n",
    "    try:\n",
    "        features = np.array(input_data.features)\n",
    "        \n",
    "        predictions = model.predict(features)\n",
    "        probabilities = model.predict_proba(features)\n",
    "        \n",
    "        return BatchPredictionOutput(\n",
    "            predictions=predictions.tolist(),\n",
    "            probabilities=probabilities.tolist()\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Dockerfile for Containerization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "# Multi-stage build for smaller image\n",
    "FROM python:3.9-slim as builder\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir --user -r requirements.txt\n",
    "\n",
    "# Final stage\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy dependencies from builder\n",
    "COPY --from=builder /root/.local /root/.local\n",
    "\n",
    "# Copy application files\n",
    "COPY app_fastapi.py .\n",
    "COPY model.pkl .\n",
    "\n",
    "# Make sure scripts in .local are usable\n",
    "ENV PATH=/root/.local/bin:$PATH\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\\n",
    "  CMD curl -f http://localhost:8000/health || exit 1\n",
    "\n",
    "# Run application\n",
    "CMD [\"uvicorn\", \"app_fastapi:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "fastapi==0.104.1\n",
    "uvicorn[standard]==0.24.0\n",
    "pydantic==2.5.0\n",
    "numpy==1.24.3\n",
    "scikit-learn==1.3.2\n",
    "joblib==1.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker-compose.yml\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  ml-api:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    environment:\n",
    "      - MODEL_PATH=/app/model.pkl\n",
    "    volumes:\n",
    "      - ./logs:/app/logs\n",
    "    restart: unless-stopped\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n",
    "      interval: 30s\n",
    "      timeout: 3s\n",
    "      retries: 3\n",
    "      start_period: 5s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Monitoring and Logging\n",
    "\n",
    "### 3.1 Logging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "def setup_logging():\n",
    "    \"\"\"Configure logging for production.\"\"\"\n",
    "    \n",
    "    # Create logs directory\n",
    "    Path('logs').mkdir(exist_ok=True)\n",
    "    \n",
    "    # Configure logger\n",
    "    logger = logging.getLogger('ml_api')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # File handler\n",
    "    file_handler = logging.FileHandler(\n",
    "        f'logs/ml_api_{datetime.now().strftime(\"%Y%m%d\")}.log'\n",
    "    )\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    \n",
    "    # Console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    \n",
    "    # Formatter\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    file_handler.setFormatter(formatter)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    \n",
    "    # Add handlers\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "logger = setup_logging()\n",
    "logger.info(\"Logging configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Prediction Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionMonitor:\n",
    "    \"\"\"Monitor predictions in production.\"\"\"\n",
    "    \n",
    "    def __init__(self, log_path='logs/predictions.jsonl'):\n",
    "        self.log_path = log_path\n",
    "        Path(log_path).parent.mkdir(exist_ok=True)\n",
    "    \n",
    "    def log_prediction(self, features, prediction, probability, latency_ms):\n",
    "        \"\"\"Log a prediction.\"\"\"\n",
    "        log_entry = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'features': features.tolist() if hasattr(features, 'tolist') else features,\n",
    "            'prediction': int(prediction),\n",
    "            'probability': probability.tolist() if hasattr(probability, 'tolist') else probability,\n",
    "            'confidence': float(max(probability)),\n",
    "            'latency_ms': latency_ms\n",
    "        }\n",
    "        \n",
    "        # Append to log file\n",
    "        with open(self.log_path, 'a') as f:\n",
    "            f.write(json.dumps(log_entry) + '\\n')\n",
    "    \n",
    "    def get_statistics(self, n=1000):\n",
    "        \"\"\"Get statistics from recent predictions.\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        # Read last n predictions\n",
    "        try:\n",
    "            with open(self.log_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines[-n:]:\n",
    "                    predictions.append(json.loads(line))\n",
    "        except FileNotFoundError:\n",
    "            return {}\n",
    "        \n",
    "        if not predictions:\n",
    "            return {}\n",
    "        \n",
    "        # Calculate statistics\n",
    "        confidences = [p['confidence'] for p in predictions]\n",
    "        latencies = [p['latency_ms'] for p in predictions]\n",
    "        \n",
    "        stats = {\n",
    "            'total_predictions': len(predictions),\n",
    "            'avg_confidence': np.mean(confidences),\n",
    "            'min_confidence': np.min(confidences),\n",
    "            'avg_latency_ms': np.mean(latencies),\n",
    "            'p95_latency_ms': np.percentile(latencies, 95),\n",
    "            'p99_latency_ms': np.percentile(latencies, 99),\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# Example usage\n",
    "monitor = PredictionMonitor()\n",
    "\n",
    "# Simulate some predictions\n",
    "for _ in range(10):\n",
    "    features = np.random.randn(20)\n",
    "    \n",
    "    start = time.time()\n",
    "    prediction = model.predict(features.reshape(1, -1))[0]\n",
    "    probability = model.predict_proba(features.reshape(1, -1))[0]\n",
    "    latency = (time.time() - start) * 1000  # ms\n",
    "    \n",
    "    monitor.log_prediction(features, prediction, probability, latency)\n",
    "\n",
    "# Get statistics\n",
    "stats = monitor.get_statistics()\n",
    "print(\"\\nPrediction Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Data Drift Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats as scipy_stats\n",
    "\n",
    "class DataDriftDetector:\n",
    "    \"\"\"Detect data drift in production.\"\"\"\n",
    "    \n",
    "    def __init__(self, reference_data):\n",
    "        \"\"\"Initialize with reference (training) data.\"\"\"\n",
    "        self.reference_data = reference_data\n",
    "        self.reference_mean = np.mean(reference_data, axis=0)\n",
    "        self.reference_std = np.std(reference_data, axis=0)\n",
    "    \n",
    "    def kolmogorov_smirnov_test(self, current_data, threshold=0.05):\n",
    "        \"\"\"\n",
    "        Perform KS test to detect drift.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Drift detection results per feature\n",
    "        \"\"\"\n",
    "        n_features = self.reference_data.shape[1]\n",
    "        drift_detected = {}\n",
    "        \n",
    "        for i in range(n_features):\n",
    "            # KS test\n",
    "            statistic, p_value = scipy_stats.ks_2samp(\n",
    "                self.reference_data[:, i],\n",
    "                current_data[:, i]\n",
    "            )\n",
    "            \n",
    "            drift_detected[f'feature_{i}'] = {\n",
    "                'statistic': statistic,\n",
    "                'p_value': p_value,\n",
    "                'drift': p_value < threshold\n",
    "            }\n",
    "        \n",
    "        return drift_detected\n",
    "    \n",
    "    def psi(self, current_data, bins=10):\n",
    "        \"\"\"\n",
    "        Calculate Population Stability Index (PSI).\n",
    "        \n",
    "        PSI < 0.1: No significant change\n",
    "        0.1 < PSI < 0.2: Moderate change\n",
    "        PSI > 0.2: Significant change\n",
    "        \"\"\"\n",
    "        n_features = self.reference_data.shape[1]\n",
    "        psi_values = {}\n",
    "        \n",
    "        for i in range(n_features):\n",
    "            # Calculate bins based on reference data\n",
    "            _, bin_edges = np.histogram(self.reference_data[:, i], bins=bins)\n",
    "            \n",
    "            # Calculate distributions\n",
    "            ref_dist, _ = np.histogram(self.reference_data[:, i], bins=bin_edges)\n",
    "            cur_dist, _ = np.histogram(current_data[:, i], bins=bin_edges)\n",
    "            \n",
    "            # Normalize\n",
    "            ref_dist = ref_dist / ref_dist.sum()\n",
    "            cur_dist = cur_dist / cur_dist.sum()\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            ref_dist = np.where(ref_dist == 0, 0.0001, ref_dist)\n",
    "            cur_dist = np.where(cur_dist == 0, 0.0001, cur_dist)\n",
    "            \n",
    "            # Calculate PSI\n",
    "            psi = np.sum((cur_dist - ref_dist) * np.log(cur_dist / ref_dist))\n",
    "            \n",
    "            psi_values[f'feature_{i}'] = {\n",
    "                'psi': psi,\n",
    "                'status': 'stable' if psi < 0.1 else ('moderate' if psi < 0.2 else 'significant')\n",
    "            }\n",
    "        \n",
    "        return psi_values\n",
    "    \n",
    "    def visualize_drift(self, current_data, feature_idx=0):\n",
    "        \"\"\"Visualize distribution drift for a feature.\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Distributions\n",
    "        axes[0].hist(self.reference_data[:, feature_idx], bins=30, alpha=0.5, label='Reference', density=True)\n",
    "        axes[0].hist(current_data[:, feature_idx], bins=30, alpha=0.5, label='Current', density=True)\n",
    "        axes[0].set_xlabel('Value')\n",
    "        axes[0].set_ylabel('Density')\n",
    "        axes[0].set_title(f'Distribution Comparison - Feature {feature_idx}')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Q-Q plot\n",
    "        scipy_stats.probplot(current_data[:, feature_idx], dist=scipy_stats.norm, plot=axes[1])\n",
    "        axes[1].set_title(f'Q-Q Plot - Feature {feature_idx}')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example\n",
    "detector = DataDriftDetector(X_train)\n",
    "\n",
    "# Simulate drifted data\n",
    "drifted_data = X_test + np.random.randn(*X_test.shape) * 0.5\n",
    "\n",
    "# Detect drift\n",
    "drift_results = detector.kolmogorov_smirnov_test(drifted_data)\n",
    "psi_results = detector.psi(drifted_data)\n",
    "\n",
    "print(\"\\nDrift Detection (KS Test):\")\n",
    "drifted_features = [k for k, v in drift_results.items() if v['drift']]\n",
    "print(f\"Features with drift detected: {len(drifted_features)}/{len(drift_results)}\")\n",
    "\n",
    "print(\"\\nPSI Analysis:\")\n",
    "for feature, result in list(psi_results.items())[:5]:  # Show first 5\n",
    "    print(f\"{feature}: PSI={result['psi']:.4f} ({result['status']})\")\n",
    "\n",
    "# Visualize\n",
    "detector.visualize_drift(drifted_data, feature_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: A/B Testing\n",
    "\n",
    "### 4.1 A/B Test Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABTestFramework:\n",
    "    \"\"\"A/B testing framework for model comparison.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_a, model_b, traffic_split=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_a: Control model\n",
    "            model_b: Treatment model\n",
    "            traffic_split: Fraction of traffic to model_b (0-1)\n",
    "        \"\"\"\n",
    "        self.model_a = model_a\n",
    "        self.model_b = model_b\n",
    "        self.traffic_split = traffic_split\n",
    "        \n",
    "        self.results_a = []\n",
    "        self.results_b = []\n",
    "    \n",
    "    def predict(self, features):\n",
    "        \"\"\"Route request to model A or B.\"\"\"\n",
    "        # Random assignment\n",
    "        use_b = np.random.random() < self.traffic_split\n",
    "        \n",
    "        if use_b:\n",
    "            prediction = self.model_b.predict(features)\n",
    "            probability = self.model_b.predict_proba(features)\n",
    "            model_used = 'B'\n",
    "        else:\n",
    "            prediction = self.model_a.predict(features)\n",
    "            probability = self.model_a.predict_proba(features)\n",
    "            model_used = 'A'\n",
    "        \n",
    "        return prediction, probability, model_used\n",
    "    \n",
    "    def record_outcome(self, model_used, predicted, actual, latency_ms):\n",
    "        \"\"\"Record prediction outcome.\"\"\"\n",
    "        result = {\n",
    "            'predicted': predicted,\n",
    "            'actual': actual,\n",
    "            'correct': predicted == actual,\n",
    "            'latency_ms': latency_ms\n",
    "        }\n",
    "        \n",
    "        if model_used == 'A':\n",
    "            self.results_a.append(result)\n",
    "        else:\n",
    "            self.results_b.append(result)\n",
    "    \n",
    "    def analyze_results(self):\n",
    "        \"\"\"Analyze A/B test results.\"\"\"\n",
    "        if not self.results_a or not self.results_b:\n",
    "            return None\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy_a = np.mean([r['correct'] for r in self.results_a])\n",
    "        accuracy_b = np.mean([r['correct'] for r in self.results_b])\n",
    "        \n",
    "        latency_a = np.mean([r['latency_ms'] for r in self.results_a])\n",
    "        latency_b = np.mean([r['latency_ms'] for r in self.results_b])\n",
    "        \n",
    "        # Statistical test\n",
    "        correct_a = [r['correct'] for r in self.results_a]\n",
    "        correct_b = [r['correct'] for r in self.results_b]\n",
    "        \n",
    "        # Two-proportion z-test\n",
    "        n_a, n_b = len(correct_a), len(correct_b)\n",
    "        p_a, p_b = accuracy_a, accuracy_b\n",
    "        p_pooled = (sum(correct_a) + sum(correct_b)) / (n_a + n_b)\n",
    "        \n",
    "        se = np.sqrt(p_pooled * (1 - p_pooled) * (1/n_a + 1/n_b))\n",
    "        z_score = (p_b - p_a) / se if se > 0 else 0\n",
    "        p_value = 2 * (1 - scipy_stats.norm.cdf(abs(z_score)))\n",
    "        \n",
    "        results = {\n",
    "            'model_a': {\n",
    "                'samples': n_a,\n",
    "                'accuracy': accuracy_a,\n",
    "                'latency_ms': latency_a\n",
    "            },\n",
    "            'model_b': {\n",
    "                'samples': n_b,\n",
    "                'accuracy': accuracy_b,\n",
    "                'latency_ms': latency_b\n",
    "            },\n",
    "            'difference': {\n",
    "                'accuracy_diff': accuracy_b - accuracy_a,\n",
    "                'latency_diff': latency_b - latency_a,\n",
    "                'z_score': z_score,\n",
    "                'p_value': p_value,\n",
    "                'significant': p_value < 0.05\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def visualize_results(self):\n",
    "        \"\"\"Visualize A/B test results.\"\"\"\n",
    "        results = self.analyze_results()\n",
    "        if results is None:\n",
    "            print(\"No results to visualize\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Accuracy comparison\n",
    "        models = ['Model A', 'Model B']\n",
    "        accuracies = [\n",
    "            results['model_a']['accuracy'],\n",
    "            results['model_b']['accuracy']\n",
    "        ]\n",
    "        \n",
    "        axes[0].bar(models, accuracies, color=['blue', 'orange'])\n",
    "        axes[0].set_ylabel('Accuracy')\n",
    "        axes[0].set_title('Model Accuracy Comparison')\n",
    "        axes[0].set_ylim([min(accuracies) * 0.95, max(accuracies) * 1.05])\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Latency comparison\n",
    "        latencies = [\n",
    "            results['model_a']['latency_ms'],\n",
    "            results['model_b']['latency_ms']\n",
    "        ]\n",
    "        \n",
    "        axes[1].bar(models, latencies, color=['blue', 'orange'])\n",
    "        axes[1].set_ylabel('Latency (ms)')\n",
    "        axes[1].set_title('Model Latency Comparison')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\nA/B Test Results:\")\n",
    "        print(f\"Model A: {results['model_a']['samples']} samples, \"\n",
    "              f\"Accuracy={results['model_a']['accuracy']:.4f}\")\n",
    "        print(f\"Model B: {results['model_b']['samples']} samples, \"\n",
    "              f\"Accuracy={results['model_b']['accuracy']:.4f}\")\n",
    "        print(f\"\\nDifference: {results['difference']['accuracy_diff']:.4f}\")\n",
    "        print(f\"P-value: {results['difference']['p_value']:.4f}\")\n",
    "        print(f\"Statistically significant: {results['difference']['significant']}\")\n",
    "\n",
    "# Example\n",
    "# Train two different models\n",
    "model_a = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "model_b = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "model_a.fit(X_train, y_train)\n",
    "model_b.fit(X_train, y_train)\n",
    "\n",
    "# Run A/B test\n",
    "ab_test = ABTestFramework(model_a, model_b, traffic_split=0.5)\n",
    "\n",
    "# Simulate requests\n",
    "for i in range(200):\n",
    "    features = X_test[i:i+1]\n",
    "    actual = y_test[i]\n",
    "    \n",
    "    start = time.time()\n",
    "    prediction, probability, model_used = ab_test.predict(features)\n",
    "    latency = (time.time() - start) * 1000\n",
    "    \n",
    "    ab_test.record_outcome(model_used, prediction[0], actual, latency)\n",
    "\n",
    "# Analyze and visualize\n",
    "ab_test.visualize_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: CI/CD for ML\n",
    "\n",
    "### 5.1 GitHub Actions Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .github/workflows/ml-pipeline.yml\n",
    "name: ML Pipeline\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ main, develop ]\n",
    "  pull_request:\n",
    "    branches: [ main ]\n",
    "\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    \n",
    "    steps:\n",
    "    - uses: actions/checkout@v3\n",
    "    \n",
    "    - name: Set up Python\n",
    "      uses: actions/setup-python@v4\n",
    "      with:\n",
    "        python-version: '3.9'\n",
    "    \n",
    "    - name: Install dependencies\n",
    "      run: |\n",
    "        python -m pip install --upgrade pip\n",
    "        pip install -r requirements.txt\n",
    "        pip install pytest pytest-cov\n",
    "    \n",
    "    - name: Run tests\n",
    "      run: |\n",
    "        pytest tests/ --cov=. --cov-report=xml\n",
    "    \n",
    "    - name: Upload coverage\n",
    "      uses: codecov/codecov-action@v3\n",
    "      with:\n",
    "        file: ./coverage.xml\n",
    "  \n",
    "  train:\n",
    "    needs: test\n",
    "    runs-on: ubuntu-latest\n",
    "    \n",
    "    steps:\n",
    "    - uses: actions/checkout@v3\n",
    "    \n",
    "    - name: Set up Python\n",
    "      uses: actions/setup-python@v4\n",
    "      with:\n",
    "        python-version: '3.9'\n",
    "    \n",
    "    - name: Install dependencies\n",
    "      run: |\n",
    "        pip install -r requirements.txt\n",
    "    \n",
    "    - name: Train model\n",
    "      run: |\n",
    "        python train.py\n",
    "    \n",
    "    - name: Validate model\n",
    "      run: |\n",
    "        python validate.py\n",
    "    \n",
    "    - name: Upload model artifact\n",
    "      uses: actions/upload-artifact@v3\n",
    "      with:\n",
    "        name: model\n",
    "        path: model.pkl\n",
    "  \n",
    "  deploy:\n",
    "    needs: train\n",
    "    runs-on: ubuntu-latest\n",
    "    if: github.ref == 'refs/heads/main'\n",
    "    \n",
    "    steps:\n",
    "    - uses: actions/checkout@v3\n",
    "    \n",
    "    - name: Download model\n",
    "      uses: actions/download-artifact@v3\n",
    "      with:\n",
    "        name: model\n",
    "    \n",
    "    - name: Build Docker image\n",
    "      run: |\n",
    "        docker build -t ml-api:latest .\n",
    "    \n",
    "    - name: Push to registry\n",
    "      run: |\n",
    "        echo ${{ secrets.DOCKER_PASSWORD }} | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin\n",
    "        docker tag ml-api:latest ${{ secrets.DOCKER_USERNAME }}/ml-api:latest\n",
    "        docker push ${{ secrets.DOCKER_USERNAME }}/ml-api:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📝 Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Experiment Tracking:**\n",
    "   - Use MLflow to track parameters, metrics, artifacts\n",
    "   - Model registry for versioning\n",
    "   - Reproducible experiments\n",
    "\n",
    "2. **Model Deployment:**\n",
    "   - Flask for simple APIs\n",
    "   - FastAPI for high performance\n",
    "   - Docker for containerization\n",
    "   - Health checks and monitoring\n",
    "\n",
    "3. **Monitoring:**\n",
    "   - Log all predictions\n",
    "   - Track latency and confidence\n",
    "   - Detect data drift (KS test, PSI)\n",
    "   - Alert on anomalies\n",
    "\n",
    "4. **A/B Testing:**\n",
    "   - Compare models in production\n",
    "   - Statistical significance testing\n",
    "   - Traffic splitting\n",
    "\n",
    "5. **CI/CD:**\n",
    "   - Automated testing\n",
    "   - Model training pipelines\n",
    "   - Automated deployment\n",
    "   - Version control\n",
    "\n",
    "### Production Checklist\n",
    "\n",
    "**Before Deployment:**\n",
    "- ✅ Model performance meets requirements\n",
    "- ✅ Comprehensive tests written\n",
    "- ✅ API documentation complete\n",
    "- ✅ Logging configured\n",
    "- ✅ Monitoring setup\n",
    "- ✅ Docker image tested\n",
    "\n",
    "**After Deployment:**\n",
    "- ✅ Health checks passing\n",
    "- ✅ Predictions logged\n",
    "- ✅ Latency within limits\n",
    "- ✅ Error rate low\n",
    "- ✅ Drift detection active\n",
    "- ✅ Alerts configured\n",
    "\n",
    "### Interview Questions\n",
    "\n",
    "1. **How do you deploy an ML model to production?**\n",
    "   - Containerize with Docker\n",
    "   - Expose via REST API (Flask/FastAPI)\n",
    "   - Deploy to cloud (AWS/GCP/Azure)\n",
    "   - Add monitoring and logging\n",
    "\n",
    "2. **How do you detect if a model is degrading in production?**\n",
    "   - Monitor prediction confidence\n",
    "   - Track labeled feedback accuracy\n",
    "   - Detect data drift (KS test, PSI)\n",
    "   - Compare to baseline metrics\n",
    "\n",
    "3. **What is data drift and how do you handle it?**\n",
    "   - Input distribution changes over time\n",
    "   - Detect with statistical tests (KS, PSI)\n",
    "   - Retrain model on recent data\n",
    "   - Update model in production\n",
    "\n",
    "4. **How do you do A/B testing for models?**\n",
    "   - Split traffic between models\n",
    "   - Track performance metrics\n",
    "   - Statistical significance test\n",
    "   - Gradual rollout of winner\n",
    "\n",
    "5. **What should you log in production?**\n",
    "   - Input features\n",
    "   - Predictions and probabilities\n",
    "   - Latency\n",
    "   - Errors and exceptions\n",
    "   - Model version\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Deploy:** Build and deploy your own model API\n",
    "- **Monitor:** Set up comprehensive monitoring\n",
    "- **Scale:** Learn Kubernetes for scaling\n",
    "- **Advanced:** Feature stores, model governance\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've learned MLOps and production deployment. You can now take models from experiment to production!\n",
    "\n",
    "**Next:** [12 - Reinforcement Learning](./12_reinforcement_learning.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
