# üìê Mathematics for Machine Learning

## Complete Mathematical Foundation for ML/AI

This comprehensive guide covers all essential mathematics needed to deeply understand and implement machine learning algorithms.

---

## üìã Table of Contents

1. [Linear Algebra](#linear-algebra)
2. [Calculus](#calculus)
3. [Probability Theory](#probability-theory)
4. [Statistics](#statistics)
5. [Optimization](#optimization)
6. [Information Theory](#information-theory)

---

## üî¢ Linear Algebra

Linear algebra is the foundation of machine learning. Neural networks, dimensionality reduction, and most ML algorithms rely heavily on linear algebra operations.

### Vectors and Matrices

**Vectors** (1D arrays):
```
v = [1, 2, 3]

Geometric interpretation:
- Direction and magnitude in space
- Point in n-dimensional space

ML applications:
- Feature vector: [age, income, credit_score]
- Word embedding: 300-dimensional vector representing a word
- Model parameters: weights and biases
```

**Matrices** (2D arrays):
```
A = [[1, 2, 3],
     [4, 5, 6],
     [7, 8, 9]]

Shape: 3√ó3 (rows √ó columns)

ML applications:
- Dataset: Each row is a sample, each column is a feature
- Weight matrix in neural network layer
- Covariance matrix
- Transformation matrix
```

### Key Operations

**Dot Product** (scalar result):
```python
import numpy as np

a = np.array([1, 2, 3])
b = np.array([4, 5, 6])

dot_product = np.dot(a, b)
# = 1√ó4 + 2√ó5 + 3√ó6 = 4 + 10 + 18 = 32

# Alternative notation
dot_product = a @ b
dot_product = np.sum(a * b)

# Geometric meaning:
# dot(a, b) = ||a|| √ó ||b|| √ó cos(Œ∏)
# where Œ∏ is the angle between vectors

# Mathematical properties:
# 1. Commutativity: a¬∑b = b¬∑a
# 2. Distributivity: a¬∑(b+c) = a¬∑b + a¬∑c
# 3. Cauchy-Schwarz inequality: |a¬∑b| ‚â§ ||a|| ||b||
# 4. When a¬∑b = 0: vectors are orthogonal (perpendicular)
# 5. When a¬∑b > 0: angle Œ∏ < 90¬∞ (similar direction)
# 6. When a¬∑b < 0: angle Œ∏ > 90¬∞ (opposite direction)

# Normalized dot product (cosine similarity):
# cos(Œ∏) = (a¬∑b) / (||a|| ||b||) ‚àà [-1, 1]

# ML application: Cosine similarity between embeddings
cosine_sim = (word_embedding_1 @ word_embedding_2) / (
    np.linalg.norm(word_embedding_1) * np.linalg.norm(word_embedding_2)
)
```

**Matrix Multiplication**:
```python
# A (m√ón) √ó B (n√óp) = C (m√óp)

A = np.array([[1, 2],
              [3, 4],
              [5, 6]])  # 3√ó2

B = np.array([[7, 8, 9],
              [10, 11, 12]])  # 2√ó3

C = A @ B  # 3√ó3

# C[i,j] = sum of (row i of A) √ó (column j of B)
# C[0,0] = 1√ó7 + 2√ó10 = 27
# C[0,1] = 1√ó8 + 2√ó11 = 30
# C[0,2] = 1√ó9 + 2√ó12 = 33

# ML application: Forward pass in neural network
# X (batch_size, input_dim) @ W (input_dim, hidden_dim) = H (batch_size, hidden_dim)
```

**Matrix Transpose**:
```python
A = np.array([[1, 2, 3],
              [4, 5, 6]])  # 2√ó3

A_T = A.T  # 3√ó2
# [[1, 4],
#  [2, 5],
#  [3, 6]]

# Properties:
# 1. (A^T)^T = A (involution)
# 2. (AB)^T = B^T A^T (reversal rule)
# 3. (A + B)^T = A^T + B^T (linearity)
# 4. (cA)^T = cA^T for scalar c
# 5. For symmetric matrix: A^T = A

# Special matrices:
# - Symmetric: A = A^T (covariance matrices)
# - Skew-symmetric: A = -A^T (cross-product matrices)
# - Orthogonal: A^T A = I (rotation matrices)

# ML application: Backpropagation gradient computation
# For Y = XW, gradient of loss L w.r.t. W:
# ‚àÇL/‚àÇW = X^T @ (‚àÇL/‚àÇY)
# Shape: (d_in, d_out) = (n, d_in)^T @ (n, d_out)
```

### Advanced Concepts

**Eigenvalues and Eigenvectors**:
```python
# Definition: Av = Œªv
# where v ‚â† 0 is eigenvector, Œª ‚àà ‚ÑÇ is eigenvalue

# Characteristic equation: det(A - ŒªI) = 0
# For n√ón matrix: n eigenvalues (counting multiplicity)

A = np.array([[4, 2],
              [1, 3]])

eigenvalues, eigenvectors = np.linalg.eig(A)

# eigenvalues: [5, 2]
# eigenvectors: [[0.89, -0.71],
#                [0.45,  0.71]]

# Geometric meaning:
# v is a direction that only gets scaled (not rotated) by transformation A
# Œª is the scaling factor

# Properties:
# 1. tr(A) = Œ£Œª_i (trace equals sum of eigenvalues)
# 2. det(A) = Œ†Œª_i (determinant equals product of eigenvalues)
# 3. For symmetric A: all eigenvalues are real
# 4. For positive definite A: all eigenvalues > 0
# 5. Eigenvectors corresponding to distinct eigenvalues are orthogonal (for symmetric A)

# Spectral theorem (symmetric matrices):
# A = QŒõQ^T where Q is orthogonal, Œõ is diagonal with eigenvalues

# ML applications:
# 1. Principal Component Analysis (PCA)
#    - Find directions of maximum variance
#    - Eigenvectors of covariance matrix

# 2. PageRank algorithm
#    - Dominant eigenvector of web graph

# 3. Spectral clustering
#    - Eigenvectors of graph Laplacian
```

### Spectral Theorem: Complete Theory

**Theorem 1 (Spectral Theorem for Real Symmetric Matrices):**

Let A ‚àà ‚Ñù‚ÅøÀ£‚Åø be a symmetric matrix (A = A·µÄ). Then:

1. **All eigenvalues are real:** Œª·µ¢ ‚àà ‚Ñù for all i
2. **Eigenvectors can be chosen orthonormal:** There exists orthonormal basis {v‚ÇÅ, ..., v‚Çô}
3. **Spectral decomposition exists:** A = QŒõQ·µÄ where:
   - Q = [v‚ÇÅ | v‚ÇÇ | ... | v‚Çô] is orthogonal (Q·µÄQ = QQ·µÄ = I)
   - Œõ = diag(Œª‚ÇÅ, Œª‚ÇÇ, ..., Œª‚Çô) contains eigenvalues

**Proof Sketch:**

**Part 1: Eigenvalues are real**

Let Œª be an eigenvalue with eigenvector v (possibly complex): Av = Œªv

Take conjugate transpose: (Av)* = (Œªv)* ‚Üí v·µÄA·µÄ = ŒªÃÑv·µÄ

Since A = A·µÄ (symmetric): v·µÄA = ŒªÃÑv·µÄ

Multiply both sides by v:
```
v·µÄAv = ŒªÃÑv·µÄv
Av = Œªv  (multiply original equation by v·µÄ)
v·µÄAv = Œªv·µÄv
```

Therefore: ŒªÃÑv·µÄv = Œªv·µÄv

Since v·µÄv = ||v||¬≤ > 0: ŒªÃÑ = Œª ‚Üí Œª ‚àà ‚Ñù  ‚úì

**Part 2: Eigenvectors for distinct eigenvalues are orthogonal**

Let Av‚ÇÅ = Œª‚ÇÅv‚ÇÅ and Av‚ÇÇ = Œª‚ÇÇv‚ÇÇ with Œª‚ÇÅ ‚â† Œª‚ÇÇ

Compute v‚ÇÅ·µÄAv‚ÇÇ in two ways:
```
Method 1: v‚ÇÅ·µÄAv‚ÇÇ = v‚ÇÅ·µÄ(Œª‚ÇÇv‚ÇÇ) = Œª‚ÇÇ(v‚ÇÅ·µÄv‚ÇÇ)
Method 2: v‚ÇÅ·µÄAv‚ÇÇ = (Av‚ÇÅ)·µÄv‚ÇÇ = (Œª‚ÇÅv‚ÇÅ)·µÄv‚ÇÇ = Œª‚ÇÅ(v‚ÇÅ·µÄv‚ÇÇ)
```

Therefore: Œª‚ÇÇ(v‚ÇÅ·µÄv‚ÇÇ) = Œª‚ÇÅ(v‚ÇÅ·µÄv‚ÇÇ)

Since Œª‚ÇÅ ‚â† Œª‚ÇÇ: v‚ÇÅ·µÄv‚ÇÇ = 0 (orthogonal)  ‚úì

**Part 3: Complete orthonormal basis exists**

By induction on dimension n:
- Base case (n=1): Trivial
- Inductive step: Let v‚ÇÅ be unit eigenvector for Œª‚ÇÅ
  - Consider subspace V‚ä• orthogonal to v‚ÇÅ
  - A maps V‚ä• to itself (since symmetric)
  - Apply induction to A|_{V‚ä•} to get {v‚ÇÇ, ..., v‚Çô}

Result: Complete orthonormal eigenbasis {v‚ÇÅ, ..., v‚Çô}  ‚úì

**Corollary 1 (Diagonalization):**
For symmetric A:
```
A = QŒõQ·µÄ
  = [v‚ÇÅ | ... | v‚Çô] ¬∑ diag(Œª‚ÇÅ, ..., Œª‚Çô) ¬∑ [v‚ÇÅ | ... | v‚Çô]·µÄ
  = Œ£·µ¢ Œª·µ¢ v·µ¢v·µ¢·µÄ  (spectral decomposition)
```

**Corollary 2 (Positive Definiteness Characterization):**
Symmetric A is positive definite ‚ü∫ all eigenvalues Œª·µ¢ > 0

**Proof:** For any x ‚â† 0:
```
x·µÄAx = x·µÄQŒõQ·µÄx = (Q·µÄx)·µÄŒõ(Q·µÄx) = Œ£·µ¢ Œª·µ¢(q·µ¢·µÄx)¬≤
```
where q·µ¢ are columns of Q.

Since Q is orthogonal, Q·µÄx ‚â† 0. Thus x·µÄAx > 0 ‚ü∫ all Œª·µ¢ > 0  ‚úì

**Corollary 3 (Matrix Functions):**
For symmetric A = QŒõQ·µÄ, we can define:
```
f(A) = Qf(Œõ)Q·µÄ = Q ¬∑ diag(f(Œª‚ÇÅ), ..., f(Œª‚Çô)) ¬∑ Q·µÄ
```

Examples:
- **Matrix square root:** A^(1/2) = Q ¬∑ diag(‚àöŒª‚ÇÅ, ..., ‚àöŒª‚Çô) ¬∑ Q·µÄ (requires Œª·µ¢ ‚â• 0)
- **Matrix exponential:** e^A = Q ¬∑ diag(e^Œª‚ÇÅ, ..., e^Œª‚Çô) ¬∑ Q·µÄ
- **Matrix inverse:** A‚Åª¬π = Q ¬∑ diag(1/Œª‚ÇÅ, ..., 1/Œª‚Çô) ¬∑ Q·µÄ (requires Œª·µ¢ ‚â† 0)

**Application: PCA Theoretical Foundation**

Given data matrix X ‚àà ‚Ñù‚ÅøÀ£·µà (centered), covariance matrix:
```
C = (1/n)X·µÄX ‚àà ‚Ñù·µàÀ£·µà
```

C is symmetric and positive semi-definite. By spectral theorem:
```
C = QŒõQ·µÄ where Œª‚ÇÅ ‚â• Œª‚ÇÇ ‚â• ... ‚â• Œª_d ‚â• 0
```

**Theorem 2 (PCA Optimality):**
The first k principal components {v‚ÇÅ, ..., v_k} maximize the variance captured:
```
max_{V‚ä•, dim(V)=k} tr(V·µÄC·µÄV) = Œ£·µ¢‚Çå‚ÇÅ·µè Œª·µ¢
```

**Proof:**
Let V = [v‚ÇÅ | ... | v_k] with V·µÄV = I_k.

Variance captured:
```
tr(V·µÄCV) = tr(V·µÄQŒõQ·µÄV)
         = tr((Q·µÄV)·µÄŒõ(Q·µÄV))
         = Œ£‚±º‚Çå‚ÇÅ·µà Œª‚±º ¬∑ ||Q·µÄVe‚±º||¬≤
```

Since V has orthonormal columns: Œ£‚±º ||Q·µÄVe‚±º||¬≤ = k

By rearrangement inequality, sum is maximized when largest Œª‚±º get weight 1:
```
tr(V·µÄCV) ‚â§ Œª‚ÇÅ + Œª‚ÇÇ + ... + Œª_k
```

Equality when V spans first k eigenvectors. ‚úì

**Reconstruction Error:**
```
||X - X_k||¬≤_F = Œ£·µ¢‚Çå‚Çñ‚Çä‚ÇÅ·µà Œª·µ¢
```

where X_k is projection onto first k principal components.

**Singular Value Decomposition (SVD)**:
```python
# Singular Value Decomposition (SVD)
# Theorem: Any matrix A ‚àà ‚Ñù^(m√ón) can be decomposed as:
# A = UŒ£V^T

# where:
# - U ‚àà ‚Ñù^(m√óm): Left singular vectors (U^T U = I)
# - Œ£ ‚àà ‚Ñù^(m√ón): Diagonal matrix with singular values œÉ_i ‚â• 0
# - V ‚àà ‚Ñù^(n√ón): Right singular vectors (V^T V = I)

# Relationship to eigendecomposition:
# - A^T A = V Œ£^T Œ£ V^T  (eigendecomposition of A^T A)
# - A A^T = U Œ£ Œ£^T U^T  (eigendecomposition of A A^T)
# - œÉ_i¬≤ are eigenvalues of both A^T A and A A^T

A = np.random.rand(4, 3)
U, S, VT = np.linalg.svd(A, full_matrices=False)

# U (4√ó3): Left singular vectors (orthonormal columns)
# S (3,): Singular values œÉ‚ÇÅ ‚â• œÉ‚ÇÇ ‚â• ... ‚â• œÉ_r > 0, r = rank(A)
# VT (3√ó3): Right singular vectors (orthonormal rows)

# Properties:
# 1. Singular values are unique and non-negative
# 2. rank(A) = number of non-zero singular values
# 3. ||A||‚ÇÇ = œÉ‚ÇÅ (largest singular value)
# 4. ||A||_F = ‚àö(œÉ‚ÇÅ¬≤ + ... + œÉ_r¬≤) (Frobenius norm)
# 5. cond(A) = œÉ‚ÇÅ/œÉ_r (condition number)

# Reconstruction:
A_reconstructed = U @ np.diag(S) @ VT
# A_reconstructed ‚âà A (within numerical precision ~1e-15)

# Best rank-k approximation (Eckart-Young theorem):
# A_k = Œ£_{i=1}^k œÉ_i u_i v_i^T minimizes ||A - A_k||‚ÇÇ and ||A - A_k||_F
```

### SVD: Complete Theory and Proofs

**Theorem 3 (Singular Value Decomposition - Existence and Uniqueness):**

For any A ‚àà ‚Ñù·µêÀ£‚Åø with rank r, there exist:
- Orthogonal U ‚àà ‚Ñù·µêÀ£·µê (U·µÄU = I_m)
- Orthogonal V ‚àà ‚Ñù‚ÅøÀ£‚Åø (V·µÄV = I_n)
- Œ£ ‚àà ‚Ñù·µêÀ£‚Åø with diagonal entries œÉ‚ÇÅ ‚â• œÉ‚ÇÇ ‚â• ... ‚â• œÉ·µ£ > 0

Such that: A = UŒ£V·µÄ

**Uniqueness:** Singular values {œÉ·µ¢} are unique. If œÉ·µ¢ are distinct, then u·µ¢ and v·µ¢ are unique up to sign.

**Proof (Constructive):**

**Step 1:** Compute eigendecomposition of A·µÄA

A·µÄA ‚àà ‚Ñù‚ÅøÀ£‚Åø is symmetric and positive semi-definite.

By spectral theorem: A·µÄA = VŒõV·µÄ where Œõ = diag(Œª‚ÇÅ, ..., Œª‚Çô) with Œª‚ÇÅ ‚â• ... ‚â• Œª‚Çô ‚â• 0

**Step 2:** Define singular values

Define œÉ·µ¢ = ‚àöŒª·µ¢ for i = 1, ..., r (where Œª·µ£ > 0, Œª_{r+1} = 0)

**Step 3:** Construct left singular vectors

For i = 1, ..., r:
```
u·µ¢ = (1/œÉ·µ¢) Av·µ¢
```

**Verification that u·µ¢ are orthonormal:**
```
u·µ¢·µÄu‚±º = (1/(œÉ·µ¢œÉ‚±º)) v·µ¢·µÄA·µÄAv‚±º
      = (1/(œÉ·µ¢œÉ‚±º)) v·µ¢·µÄ(Œª‚±ºv‚±º)
      = (Œª‚±º/(œÉ·µ¢œÉ‚±º)) Œ¥·µ¢‚±º
      = (œÉ‚±º¬≤/(œÉ·µ¢œÉ‚±º)) Œ¥·µ¢‚±º
      = Œ¥·µ¢‚±º  ‚úì
```

**Step 4:** Extend to complete orthonormal basis

Extend {u‚ÇÅ, ..., u·µ£} to orthonormal basis {u‚ÇÅ, ..., u‚Çò} of ‚Ñù·µê

Extend {v‚ÇÅ, ..., v·µ£} to orthonormal basis {v‚ÇÅ, ..., v‚Çô} of ‚Ñù‚Åø (if needed)

**Step 5:** Verify A = UŒ£V·µÄ

For any x = Œ£‚±º c‚±ºv‚±º:
```
Ax = A(Œ£‚±º c‚±ºv‚±º)
   = Œ£‚±º‚Çå‚ÇÅ ≥ c‚±ºAv‚±º
   = Œ£‚±º‚Çå‚ÇÅ ≥ c‚±ºœÉ‚±ºu‚±º
   = U(Œ£v‚±ºc‚±ºe‚±º)
   = UŒ£V·µÄx  ‚úì
```

**Theorem 4 (Eckart-Young-Mirsky Theorem):**

Let A = UŒ£V·µÄ be SVD with singular values œÉ‚ÇÅ ‚â• ... ‚â• œÉ·µ£ > 0.

Define rank-k approximation:
```
A_k = Œ£·µ¢‚Çå‚ÇÅ·µè œÉ·µ¢u·µ¢v·µ¢·µÄ = U_kŒ£_kV_k·µÄ
```

Then A_k is the **optimal rank-k approximation** in both spectral and Frobenius norms:
```
||A - A_k||‚ÇÇ = min_{rank(B)‚â§k} ||A - B||‚ÇÇ = œÉ_{k+1}
||A - A_k||_F = min_{rank(B)‚â§k} ||A - B||_F = ‚àö(œÉ¬≤_{k+1} + ... + œÉ¬≤·µ£)
```

**Proof (Frobenius norm case):**

**Lower bound:** Any rank-k matrix B can be written as B = XY·µÄ where X ‚àà ‚Ñù·µêÀ£·µè, Y ‚àà ‚Ñù‚ÅøÀ£·µè.

The null space of B has dimension at least n - k.

Consider subspace S spanned by {v‚ÇÅ, ..., v_{k+1}} (dimension k+1).

By dimension counting: S ‚à© null(B) ‚â† {0}

Let z ‚àà S ‚à© null(B) with ||z|| = 1:
```
z = Œ£·µ¢‚Çå‚ÇÅ·µè‚Å∫¬π c·µ¢v·µ¢

||A - B||¬≤_F ‚â• ||(A - B)z||¬≤
            = ||Az||¬≤  (since Bz = 0)
            = ||UŒ£V·µÄz||¬≤
            = ||Œ£(V·µÄz)||¬≤
            = Œ£·µ¢‚Çå‚ÇÅ·µè‚Å∫¬π œÉ·µ¢¬≤c·µ¢¬≤  (since V·µÄz = (c‚ÇÅ, ..., c_{k+1}, 0, ...))
            ‚â• œÉ¬≤_{k+1} Œ£·µ¢ c·µ¢¬≤
            = œÉ¬≤_{k+1}  (since ||z|| = 1)
```

**Upper bound:** For A_k:
```
||A - A_k||¬≤_F = ||Œ£·µ¢‚Çå‚Çñ‚Çä‚ÇÅ ≥ œÉ·µ¢u·µ¢v·µ¢·µÄ||¬≤_F
                = Œ£·µ¢‚Çå‚Çñ‚Çä‚ÇÅ ≥ œÉ·µ¢¬≤  (orthonormality of u·µ¢, v·µ¢)
```

For k = r-1:
```
||A - A_{r-1}||¬≤_F = œÉ·µ£¬≤
```

Therefore: min_{rank(B)‚â§k} ||A - B||_F = ‚àö(œÉ¬≤_{k+1} + ... + œÉ¬≤·µ£)  ‚úì

**Corollary (Optimal Approximation):**

Fraction of Frobenius norm captured by rank-k approximation:
```
||A_k||¬≤_F / ||A||¬≤_F = (Œ£·µ¢‚Çå‚ÇÅ·µè œÉ·µ¢¬≤) / (Œ£·µ¢‚Çå‚ÇÅ ≥ œÉ·µ¢¬≤)
```

**Practical guideline:** Keep singular values until Œ£·µ¢‚Çå‚ÇÅ·µè œÉ·µ¢¬≤ ‚â• 0.9 Œ£·µ¢‚Çå‚ÇÅ ≥ œÉ·µ¢¬≤ (90% variance explained)

**Theorem 5 (SVD Properties and Applications):**

1. **Rank:** rank(A) = number of non-zero singular values

2. **Norms:**
   - Spectral norm: ||A||‚ÇÇ = œÉ‚ÇÅ (largest singular value)
   - Frobenius norm: ||A||_F = ‚àö(œÉ‚ÇÅ¬≤ + ... + œÉ·µ£¬≤)
   - Nuclear norm: ||A||* = œÉ‚ÇÅ + ... + œÉ·µ£

3. **Condition number:** Œ∫(A) = œÉ‚ÇÅ/œÉ·µ£ (ratio of largest to smallest singular value)
   - Œ∫(A) = 1: perfectly conditioned (A is multiple of orthogonal matrix)
   - Œ∫(A) >> 1: ill-conditioned (small changes in input cause large output changes)

4. **Moore-Penrose pseudoinverse:**
   ```
   A‚Å∫ = VŒ£‚Å∫U·µÄ
   ```
   where Œ£‚Å∫ has diagonal entries 1/œÉ·µ¢ for i = 1, ..., r and 0 otherwise.

   **Properties:**
   - AA‚Å∫A = A
   - A‚Å∫AA‚Å∫ = A‚Å∫
   - (AA‚Å∫)·µÄ = AA‚Å∫ (AA‚Å∫ is symmetric)
   - (A‚Å∫A)·µÄ = A‚Å∫A (A‚Å∫A is symmetric)

5. **Least squares solution:**
   For Ax = b, the minimum-norm solution is:
   ```
   x* = A‚Å∫b = VŒ£‚Å∫U·µÄb
   ```

   **Proof:**
   ```
   ||Ax - b||¬≤ is minimized when x minimizes ||Ax - b||¬≤

   Setting gradient to zero:
   A·µÄAx = A·µÄb

   Solution:
   x* = (A·µÄA)‚Å∫A·µÄb = A‚Å∫b  ‚úì
   ```

**Application: Matrix Completion and Recommender Systems**

Given partially observed matrix M ‚àà ‚Ñù·µêÀ£‚Åø (e.g., user-item ratings with missing entries):

**Problem:** Find low-rank matrix X minimizing:
```
min_X Œ£_{(i,j)‚ààŒ©} (M_{ij} - X_{ij})¬≤ + Œª||X||*
```

where Œ© is set of observed entries and ||X||* is nuclear norm.

**Rationale:**
- Low-rank assumption: Users have few latent preferences
- Nuclear norm regularization promotes low rank
- SVD gives optimal low-rank approximation

**Solution via SVD:**
1. Initialize missing entries (e.g., column means)
2. Compute SVD: X = UŒ£V·µÄ
3. Keep top k singular values: X_k
4. Update missing entries from X_k
5. Repeat until convergence

```python
def matrix_completion_svd(M_observed, mask, k, max_iter=100):
    """
    Matrix completion via SVD

    Args:
        M_observed: Partially observed matrix
        mask: Boolean matrix (True = observed)
        k: Target rank
    """
    M = M_observed.copy()

    # Initialize missing with column means
    col_means = np.nanmean(M_observed, axis=0)
    for j in range(M.shape[1]):
        M[~mask[:, j], j] = col_means[j]

    for iteration in range(max_iter):
        M_old = M.copy()

        # SVD and truncate to rank k
        U, S, VT = np.linalg.svd(M, full_matrices=False)
        M_approx = U[:, :k] @ np.diag(S[:k]) @ VT[:k, :]

        # Update only missing entries
        M[~mask] = M_approx[~mask]

        # Check convergence
        change = np.linalg.norm(M - M_old) / np.linalg.norm(M_old)
        if change < 1e-4:
            break

    return M, iteration

# ML applications:

# 1. Low-rank approximation
def low_rank_approximation(A, k):
    """Keep only top k singular values"""
    U, S, VT = np.linalg.svd(A, full_matrices=False)
    return U[:, :k] @ np.diag(S[:k]) @ VT[:k, :]

# 2. PCA implementation
def pca_via_svd(X, n_components):
    """PCA using SVD"""
    # Center data
    X_centered = X - X.mean(axis=0)

    # SVD
    U, S, VT = np.linalg.svd(X_centered, full_matrices=False)

    # Principal components
    components = VT[:n_components]

    # Transformed data
    X_transformed = X_centered @ components.T

    return X_transformed, components

# 3. Recommender systems (Matrix Factorization)
# User-item matrix ‚âà U @ Œ£ @ V^T
# Latent factors representation
```

**Matrix Norms**:
```python
A = np.array([[1, 2],
              [3, 4]])

# Formal definition: ||A|| is a norm if it satisfies:
# 1. ||A|| ‚â• 0, equality iff A = 0 (positive definiteness)
# 2. ||cA|| = |c| ||A|| for scalar c (absolute homogeneity)
# 3. ||A + B|| ‚â§ ||A|| + ||B|| (triangle inequality)
# 4. ||AB|| ‚â§ ||A|| ||B|| (submultiplicativity)

# Frobenius norm (element-wise L2)
# ||A||_F = ‚àö(Œ£_{i,j} a_{ij}¬≤) = ‚àö(tr(A^T A))
frobenius_norm = np.linalg.norm(A, 'fro')
# = ‚àö(1¬≤ + 2¬≤ + 3¬≤ + 4¬≤) = ‚àö30 ‚âà 5.477

# Induced p-norms: ||A||_p = max_{x‚â†0} ||Ax||_p / ||x||_p

# L1 norm (maximum absolute column sum)
# ||A||‚ÇÅ = max_j Œ£_i |a_{ij}|
l1_norm = np.linalg.norm(A, 1)  # max(|1|+|3|, |2|+|4|) = 6

# L‚àû norm (maximum absolute row sum)
# ||A||_‚àû = max_i Œ£_j |a_{ij}|
linf_norm = np.linalg.norm(A, np.inf)  # max(|1|+|2|, |3|+|4|) = 7

# L2 norm / Spectral norm (largest singular value)
# ||A||‚ÇÇ = œÉ_max(A) = ‚àö(Œª_max(A^T A))
l2_norm = np.linalg.norm(A, 2)  # ‚âà 5.465

# Nuclear norm (sum of singular values)
# ||A||_* = Œ£_i œÉ_i
nuclear_norm = np.linalg.norm(A, 'nuc')

# ML applications:
# 1. L2 regularization (weight decay): min L(Œ∏) + Œª||Œ∏||‚ÇÇ¬≤
# 2. Frobenius norm for matrix regularization: ||W||_F¬≤
# 3. Nuclear norm for low-rank matrix completion: ||M||_*
# 4. Spectral norm for Lipschitz constraint: ||‚àáf|| ‚â§ L
# 5. Condition number: Œ∫(A) = ||A||‚ÇÇ ¬∑ ||A‚Åª¬π||‚ÇÇ = œÉ_max/œÉ_min
```

### Practical ML Examples

**Linear Regression (Matrix Form)**:
```python
# y = Xw + noise
# Closed-form solution: w = (X^T X)^(-1) X^T y

def linear_regression_normal_equation(X, y):
    """
    Solve linear regression analytically

    X: (n_samples, n_features)
    y: (n_samples,)
    """
    # Add intercept term (column of ones)
    X_with_intercept = np.column_stack([np.ones(len(X)), X])

    # Normal equation
    XTX = X_with_intercept.T @ X_with_intercept
    XTy = X_with_intercept.T @ y

    # Solve: w = (X^T X)^(-1) X^T y
    w = np.linalg.solve(XTX, XTy)

    return w

# Example
X = np.random.rand(100, 5)
y = X @ np.array([1, 2, 3, 4, 5]) + np.random.randn(100) * 0.1

weights = linear_regression_normal_equation(X, y)
print(f"Learned weights: {weights}")
# Should be close to [1, 2, 3, 4, 5]
```

**PCA (Dimensionality Reduction)**:
```python
def pca(X, n_components=2):
    """
    Principal Component Analysis

    Steps:
    1. Center data
    2. Compute covariance matrix
    3. Find eigenvectors (principal components)
    4. Project data onto top components
    """
    # 1. Center data (mean = 0)
    X_centered = X - np.mean(X, axis=0)

    # 2. Covariance matrix
    # Cov = (1/n) X^T X
    n_samples = X_centered.shape[0]
    cov_matrix = (X_centered.T @ X_centered) / n_samples

    # 3. Eigendecomposition
    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)

    # Sort by eigenvalue (descending)
    idx = eigenvalues.argsort()[::-1]
    eigenvalues = eigenvalues[idx]
    eigenvectors = eigenvectors[:, idx]

    # 4. Select top k eigenvectors
    principal_components = eigenvectors[:, :n_components]

    # 5. Project data
    X_transformed = X_centered @ principal_components

    # Explained variance
    explained_variance_ratio = eigenvalues[:n_components] / eigenvalues.sum()

    return X_transformed, principal_components, explained_variance_ratio

# Example: Compress 784-dimensional MNIST to 2D
from sklearn.datasets import load_digits
digits = load_digits()
X, y = digits.data, digits.target

X_2d, components, var_ratio = pca(X, n_components=2)

print(f"Explained variance: {var_ratio.sum():.2%}")
# Typically 20-30% for 2 components

import matplotlib.pyplot as plt
plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y, cmap='tab10')
plt.xlabel(f'PC1 ({var_ratio[0]:.1%})')
plt.ylabel(f'PC2 ({var_ratio[1]:.1%})')
plt.title('MNIST Digits in 2D (PCA)')
plt.colorbar()
plt.show()
```

**Neural Network Forward Pass**:
```python
def neural_network_forward(X, weights, biases):
    """
    2-layer neural network using matrix operations

    X: (batch_size, input_dim)
    weights: list of weight matrices
    biases: list of bias vectors
    """
    # Layer 1: X (n, d_in) @ W1 (d_in, d_hidden) = H (n, d_hidden)
    z1 = X @ weights[0] + biases[0]
    a1 = np.maximum(0, z1)  # ReLU activation

    # Layer 2: H (n, d_hidden) @ W2 (d_hidden, d_out) = Y (n, d_out)
    z2 = a1 @ weights[1] + biases[1]

    # Softmax for classification
    exp_scores = np.exp(z2 - np.max(z2, axis=1, keepdims=True))  # Numerical stability
    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)

    return probs

# Example: MNIST classifier
batch_size = 32
input_dim = 784  # 28√ó28 images
hidden_dim = 128
output_dim = 10  # digits 0-9

W1 = np.random.randn(input_dim, hidden_dim) * 0.01
b1 = np.zeros(hidden_dim)
W2 = np.random.randn(hidden_dim, output_dim) * 0.01
b2 = np.zeros(output_dim)

X_batch = np.random.randn(batch_size, input_dim)
predictions = neural_network_forward(X_batch, [W1, W2], [b1, b2])

print(f"Predictions shape: {predictions.shape}")  # (32, 10)
print(f"Sample prediction: {predictions[0]}")  # Probabilities sum to 1
```

---

## üìà Calculus

Calculus enables optimization - the core of training ML models. Understanding derivatives and gradients is essential for backpropagation.

### Derivatives

**Definition**: Rate of change of a function

**Formal definition:**
```
f'(x) = lim_{h‚Üí0} [f(x+h) - f(x)] / h

Equivalent formulation:
f'(x‚ÇÄ) = lim_{x‚Üíx‚ÇÄ} [f(x) - f(x‚ÇÄ)] / (x - x‚ÇÄ)
```

**Interpretation:**
- Geometric: Slope of tangent line to f at x
- Physical: Instantaneous rate of change
- ML: Sensitivity of loss to parameter changes

**Existence conditions:**
- f is differentiable at x if the limit exists and is finite
- Differentiable ‚üπ Continuous (but not vice versa)
- Counter-example: f(x) = |x| is continuous but not differentiable at x=0

**Higher-order derivatives:**
- f''(x): Second derivative (curvature, acceleration)
- f'''(x): Third derivative (jerk)
- f^(n)(x): n-th derivative

**Applications in ML:**
- First derivative ‚àáL: Direction of steepest ascent
- Second derivative H (Hessian): Curvature, used in Newton's method
- Lipschitz constant L: max ||f'(x)|| bounds learning rate
```

**Common Derivatives**:
```python
# Power rule: d/dx [x^n] = n √ó x^(n-1)
# d/dx [x¬≤] = 2x
# d/dx [x¬≥] = 3x¬≤

# Exponential: d/dx [e^x] = e^x

# Logarithm: d/dx [ln(x)] = 1/x

# Trigonometric:
# d/dx [sin(x)] = cos(x)
# d/dx [cos(x)] = -sin(x)

# Chain rule: d/dx [f(g(x))] = f'(g(x)) √ó g'(x)
# Example: d/dx [sin(x¬≤)] = cos(x¬≤) √ó 2x

# Product rule: d/dx [f(x)g(x)] = f'(x)g(x) + f(x)g'(x)

# Quotient rule: d/dx [f(x)/g(x)] = [f'(x)g(x) - f(x)g'(x)] / g(x)¬≤
```

**Numerical Differentiation** (for verification):
```python
def numerical_derivative(f, x, h=1e-5):
    """Compute derivative using finite differences"""
    return (f(x + h) - f(x - h)) / (2 * h)

# Example
f = lambda x: x**2
f_prime_numerical = numerical_derivative(f, x=3.0)
f_prime_analytical = 2 * 3.0

print(f"Numerical: {f_prime_numerical:.6f}")  # 6.000000
print(f"Analytical: {f_prime_analytical:.6f}")  # 6.000000
```

### Gradients (Multivariate Derivatives)

**Gradient**: Vector of partial derivatives

**Formal definition:**
```
For f: ‚Ñù‚Åø ‚Üí ‚Ñù, the gradient is:

‚àáf(x) = [‚àÇf/‚àÇx‚ÇÅ, ‚àÇf/‚àÇx‚ÇÇ, ..., ‚àÇf/‚àÇx‚Çô]^T ‚àà ‚Ñù‚Åø

Example: f(x, y) = x¬≤ + y¬≤
‚àáf = [‚àÇf/‚àÇx, ‚àÇf/‚àÇy]^T = [2x, 2y]^T
```

**Geometric interpretation:**
- Direction: Points in direction of steepest increase of f
- Magnitude: ||‚àáf|| is the rate of increase in that direction
- Orthogonality: ‚àáf ‚üÇ level curves {x : f(x) = c}

**Mathematical properties:**
1. Directional derivative: D_v f(x) = ‚àáf(x) ¬∑ v for unit vector v
2. Maximum directional derivative: max_{||v||=1} D_v f = ||‚àáf||
3. At local minimum/maximum: ‚àáf = 0 (critical point)
4. Convex function: f(y) ‚â• f(x) + ‚àáf(x)^T(y-x) for all x,y

**Gradient descent update:**
```
x_{k+1} = x_k - Œ±‚àáf(x_k)

where:
- Œ±: Learning rate (step size)
- -‚àáf: Negative gradient (direction of steepest decrease)

Convergence guarantee (for L-smooth, Œº-strongly convex f):
||x_k - x*||¬≤ ‚â§ (1 - Œº/L)^k ||x_0 - x*||¬≤

where Œ∫ = L/Œº is condition number
```

**ML interpretation:**
- Loss L(Œ∏): Gradient ‚àáL tells us how to adjust parameters Œ∏
- Backpropagation: Efficient algorithm to compute ‚àáL
- Batch gradient: ‚àáL = (1/n)Œ£_i ‚àáL_i (average over samples)
- Stochastic gradient: ‚àáÃÇL = ‚àáL_i (estimate from single sample)
```

**Gradient Descent**:
```python
def gradient_descent(f, grad_f, x0, learning_rate=0.1, num_iterations=100,
                    tolerance=1e-6, verbose=False):
    """
    Minimize function f using gradient descent

    Parameters:
    -----------
    f: function to minimize
    grad_f: gradient of f
    x0: initial point (numpy array)
    learning_rate: step size Œ± (also called learning rate)
    tolerance: convergence threshold

    Convergence Conditions (for convex f with L-Lipschitz gradient):
    - If Œ± ‚â§ 1/L: Guaranteed convergence to global minimum
    - Convergence rate: O(1/k) where k is iteration number

    For strongly convex functions (with parameter Œº):
    - Convergence rate improves to O(exp(-k¬∑Œº/L))
    """
    x = x0.copy()
    history = [x.copy()]
    grad_norms = []

    for i in range(num_iterations):
        # Compute gradient
        grad = grad_f(x)
        grad_norm = np.linalg.norm(grad)
        grad_norms.append(grad_norm)

        # Check convergence
        if grad_norm < tolerance:
            if verbose:
                print(f"Converged at iteration {i}, gradient norm: {grad_norm:.2e}")
            break

        # Update: x^(k+1) = x^(k) - Œ±‚àáf(x^(k))
        x = x - learning_rate * grad

        history.append(x.copy())

        if verbose and i % 10 == 0:
            print(f"Iter {i}: f(x) = {f(x):.6f}, ||‚àáf|| = {grad_norm:.2e}")

    return x, np.array(history), np.array(grad_norms)

# Example: Minimize f(x,y) = x¬≤ + y¬≤
f = lambda x: x[0]**2 + x[1]**2
grad_f = lambda x: np.array([2*x[0], 2*x[1]])

x_min, history = gradient_descent(f, grad_f, x0=np.array([5.0, 5.0]))

print(f"Minimum found at: {x_min}")  # Should be near [0, 0]
print(f"Minimum value: {f(x_min):.6f}")  # Should be near 0

# Visualize optimization path
import matplotlib.pyplot as plt

x = np.linspace(-6, 6, 100)
y = np.linspace(-6, 6, 100)
X, Y = np.meshgrid(x, y)
Z = X**2 + Y**2

plt.contour(X, Y, Z, levels=20)
plt.plot(history[:, 0], history[:, 1], 'ro-', markersize=3)
plt.plot(history[0, 0], history[0, 1], 'go', markersize=10, label='Start')
plt.plot(history[-1, 0], history[-1, 1], 'r*', markersize=15, label='End')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Gradient Descent Optimization')
plt.legend()
plt.show()
```

### Backpropagation (Chain Rule in Action)

**Chain Rule for Neural Networks**:
```python
# Network: x ‚Üí f ‚Üí g ‚Üí h ‚Üí loss
# x ‚Üí f ‚Üí a
# a ‚Üí g ‚Üí b
# b ‚Üí h ‚Üí loss

# Chain rule:
# dL/dx = (dL/db) √ó (db/da) √ó (da/dx)

# Backpropagation computes these efficiently!

def backpropagation_example():
    """
    Simple 2-layer network with explicit backprop

    x ‚Üí W1 ‚Üí a ‚Üí ReLU ‚Üí h ‚Üí W2 ‚Üí y ‚Üí Loss
    """
    # Forward pass
    x = np.array([[1.0, 2.0]])  # (1, 2)
    W1 = np.array([[0.1, 0.2, 0.3],
                   [0.4, 0.5, 0.6]])  # (2, 3)
    W2 = np.array([[0.7],
                   [0.8],
                   [0.9]])  # (3, 1)
    target = np.array([[1.0]])

    # Layer 1
    a = x @ W1  # (1, 3)
    # a = [1√ó0.1 + 2√ó0.4, 1√ó0.2 + 2√ó0.5, 1√ó0.3 + 2√ó0.6]
    #   = [0.9, 1.2, 1.5]

    # ReLU
    h = np.maximum(0, a)  # (1, 3)
    # h = [0.9, 1.2, 1.5] (all positive, no change)

    # Layer 2
    y = h @ W2  # (1, 1)
    # y = [0.9√ó0.7 + 1.2√ó0.8 + 1.5√ó0.9]
    #   = [0.63 + 0.96 + 1.35] = [2.94]

    # Loss (MSE)
    loss = 0.5 * (y - target)**2
    # loss = 0.5 √ó (2.94 - 1.0)¬≤ = 0.5 √ó 3.76 = 1.88

    print(f"Forward pass:")
    print(f"  a = {a}")
    print(f"  h = {h}")
    print(f"  y = {y}")
    print(f"  loss = {loss}")

    # Backward pass (compute gradients)

    # dL/dy
    dL_dy = y - target  # (1, 1)
    # dL/dy = 2.94 - 1.0 = 1.94

    # dL/dW2 = h^T @ dL/dy
    dL_dW2 = h.T @ dL_dy  # (3, 1)
    # = [[0.9], [1.2], [1.5]] √ó 1.94
    # = [[1.746], [2.328], [2.910]]

    # dL/dh = dL/dy @ W2^T
    dL_dh = dL_dy @ W2.T  # (1, 3)
    # = 1.94 √ó [0.7, 0.8, 0.9]
    # = [1.358, 1.552, 1.746]

    # dL/da (through ReLU derivative)
    # ReLU'(x) = 1 if x > 0 else 0
    dL_da = dL_dh * (a > 0)  # (1, 3)
    # = [1.358, 1.552, 1.746] √ó [1, 1, 1]
    # = [1.358, 1.552, 1.746]

    # dL/dW1 = x^T @ dL/da
    dL_dW1 = x.T @ dL_da  # (2, 3)
    # = [[1], [2]] √ó [1.358, 1.552, 1.746]
    # = [[1.358, 1.552, 1.746],
    #    [2.716, 3.104, 3.492]]

    print(f"\nBackward pass (gradients):")
    print(f"  dL/dW2 =\n{dL_dW2}")
    print(f"  dL/dW1 =\n{dL_dW1}")

    # Gradient descent update
    learning_rate = 0.01
    W1_new = W1 - learning_rate * dL_dW1
    W2_new = W2 - learning_rate * dL_dW2

    print(f"\nUpdated weights:")
    print(f"  W1_new =\n{W1_new}")
    print(f"  W2_new =\n{W2_new}")

backpropagation_example()
```

**Automatic Differentiation** (PyTorch):
```python
import torch

# PyTorch computes gradients automatically!

# Forward pass
x = torch.tensor([[1.0, 2.0]], requires_grad=False)
W1 = torch.tensor([[0.1, 0.2, 0.3],
                   [0.4, 0.5, 0.6]], requires_grad=True)
W2 = torch.tensor([[0.7],
                   [0.8],
                   [0.9]], requires_grad=True)
target = torch.tensor([[1.0]])

a = x @ W1
h = torch.relu(a)
y = h @ W2
loss = 0.5 * (y - target)**2

# Backward pass (automatic!)
loss.backward()

print(f"Gradients (automatic):")
print(f"  W1.grad =\n{W1.grad}")
print(f"  W2.grad =\n{W2.grad}")

# Update weights
learning_rate = 0.01
with torch.no_grad():
    W1 -= learning_rate * W1.grad
    W2 -= learning_rate * W2.grad

    # Clear gradients for next iteration
    W1.grad.zero_()
    W2.grad.zero_()
```

### Important ML Derivatives

**Sigmoid**:
```python
# œÉ(x) = 1 / (1 + e^(-x))
# œÉ'(x) = œÉ(x) √ó (1 - œÉ(x))

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    s = sigmoid(x)
    return s * (1 - s)

# Efficient for backprop: Already have œÉ(x) from forward pass!
```

**Softmax**:
```python
# Softmax: S_i = e^(x_i) / Œ£ e^(x_j)

# Derivative w.r.t. x_i:
# ‚àÇS_i/‚àÇx_i = S_i √ó (1 - S_i)
# ‚àÇS_i/‚àÇx_j = -S_i √ó S_j  (i ‚â† j)

# Combined with Cross-Entropy loss:
# ‚àÇL/‚àÇx = S - y  (beautiful simplification!)
# where y is one-hot encoded target

def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))  # Numerical stability
    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)

# Cross-entropy loss
def cross_entropy(predictions, targets):
    return -np.sum(targets * np.log(predictions + 1e-8))

# Gradient (simplified!)
def softmax_cross_entropy_gradient(predictions, targets):
    return predictions - targets

# Example
logits = np.array([[2.0, 1.0, 0.1]])
target = np.array([[1, 0, 0]])  # First class

probs = softmax(logits)
loss = cross_entropy(probs, target)
grad = softmax_cross_entropy_gradient(probs, target)

print(f"Probabilities: {probs}")
print(f"Loss: {loss:.4f}")
print(f"Gradient: {grad}")
```

**ReLU**:
```python
# ReLU(x) = max(0, x)
# ReLU'(x) = 1 if x > 0 else 0

def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return (x > 0).astype(float)

# Dead ReLU problem: If x < 0 always, gradient is 0, neuron never updates
# Solutions: Leaky ReLU, PReLU, ELU
```

---

## üé≤ Probability Theory

Probability underpins machine learning - from Bayesian inference to generative models to uncertainty quantification.

### Fundamentals

**Probability Axioms (Kolmogorov, 1933)**:
```
Formal Framework:
(Œ©, F, P) is a probability space where:
- Œ©: Sample space (set of all possible outcomes)
- F: œÉ-algebra on Œ© (collection of events, closed under complements and countable unions)
- P: Probability measure satisfying:

Axiom 1 (Non-negativity): P(A) ‚â• 0 for all A ‚àà F

Axiom 2 (Normalization): P(Œ©) = 1

Axiom 3 (Countable Additivity): For countable disjoint events A‚ÇÅ, A‚ÇÇ, ...
P(‚ãÉ·µ¢‚Çå‚ÇÅ^‚àû A·µ¢) = Œ£·µ¢‚Çå‚ÇÅ^‚àû P(A·µ¢)

Derived Properties:
1. P(‚àÖ) = 0 (probability of impossible event)
2. P(A·∂ú) = 1 - P(A) (complement rule)
3. If A ‚äÜ B, then P(A) ‚â§ P(B) (monotonicity)
4. P(A ‚à™ B) = P(A) + P(B) - P(A ‚à© B) (inclusion-exclusion)
5. P(A) ‚â§ 1 for all events A (boundedness)

Measure Theory Connection:
Probability is a normalized measure (total measure = 1)
Integration: E[X] = ‚à´ X dP (expectation as integral)
```

**Conditional Probability**:
```
Definition: P(A|B) = P(A ‚à© B) / P(B) for P(B) > 0

Interpretation: Probability of A given B has occurred

Properties:
1. P(A|B) ‚â• 0 (non-negative)
2. P(Œ©|B) = 1 (normalization)
3. P(¬∑|B) is a valid probability measure

Independence:
Events A and B are independent if and only if:
P(A ‚à© B) = P(A) √ó P(B)

Equivalent characterizations:
- P(A|B) = P(A) (knowing B doesn't change probability of A)
- P(B|A) = P(B) (knowing A doesn't change probability of B)

Pairwise vs Mutual Independence:
- Pairwise: P(A·µ¢ ‚à© A‚±º) = P(A·µ¢)P(A‚±º) for all i ‚â† j
- Mutual: P(‚ãÇ·µ¢‚ààS A·µ¢) = ‚àè·µ¢‚ààS P(A·µ¢) for all subsets S
- Mutual ‚üπ Pairwise, but not vice versa!

Chain Rule (Law of Multiplication):
P(A‚ÇÅ ‚à© A‚ÇÇ ‚à© ... ‚à© A‚Çô) = P(A‚ÇÅ) √ó P(A‚ÇÇ|A‚ÇÅ) √ó P(A‚ÇÉ|A‚ÇÅ‚à©A‚ÇÇ) √ó ... √ó P(A‚Çô|A‚ÇÅ‚à©...‚à©A‚Çô‚Çã‚ÇÅ)

Example: Email spam detection
P(spam | contains "FREE") = P(spam ‚à© contains "FREE") / P(contains "FREE")
```

**Bayes' Theorem**:
```
P(A|B) = P(B|A) √ó P(A) / P(B)

Where:
- P(A|B): Posterior probability (what we want to compute)
- P(B|A): Likelihood (probability of observing B given A)
- P(A): Prior probability (initial belief about A)
- P(B): Evidence or marginal likelihood (normalization constant)

In ML terms:
P(Œ∏ | D) = P(D | Œ∏) √ó P(Œ∏) / P(D)

Posterior = Likelihood √ó Prior / Evidence

Where:
- Œ∏: Model parameters (hypothesis)
- D: Observed data
- P(Œ∏ | D): Posterior distribution over parameters given data
- P(D | Œ∏): Likelihood of data given parameters
- P(Œ∏): Prior distribution over parameters
- P(D) = ‚à´ P(D | Œ∏) P(Œ∏) dŒ∏: Marginal likelihood (often intractable)

This is the foundation of Bayesian machine learning!

Key Properties:
1. Law of Total Probability: P(B) = Œ£ P(B|A_i) P(A_i)
2. Bayes' Rule is exact, not an approximation
3. Prior √ó Likelihood = Unnormalized Posterior
4. Conjugate priors simplify posterior computation
```

**Example: Medical Diagnosis**:
```python
def bayes_theorem_example():
    """
    Disease testing with Bayes' Theorem

    Disease prevalence: 1% (prior)
    Test sensitivity: 95% (true positive rate)
    Test specificity: 90% (true negative rate)

    Question: If test is positive, what's probability of having disease?
    """
    # Prior
    P_disease = 0.01
    P_no_disease = 0.99

    # Likelihood
    P_positive_given_disease = 0.95  # Sensitivity
    P_positive_given_no_disease = 0.10  # 1 - Specificity

    # Total probability of positive test
    P_positive = (P_positive_given_disease * P_disease +
                  P_positive_given_no_disease * P_no_disease)

    # Bayes' theorem
    P_disease_given_positive = (P_positive_given_disease * P_disease) / P_positive

    print(f"Probability of disease given positive test: {P_disease_given_positive:.1%}")
    # Only 8.7%! (because disease is rare)

    # Intuition: Many false positives because disease is rare

bayes_theorem_example()
# Output: Probability of disease given positive test: 8.7%
```

### Random Variables

**Discrete Random Variable**:
```python
# Probability Mass Function (PMF)

# Example: Rolling a die
outcomes = [1, 2, 3, 4, 5, 6]
probabilities = [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]

# Expected value (mean)
E_X = sum(x * p for x, p in zip(outcomes, probabilities))
# E[X] = 1√ó(1/6) + 2√ó(1/6) + ... + 6√ó(1/6) = 3.5

# Variance
Var_X = sum((x - E_X)**2 * p for x, p in zip(outcomes, probabilities))
# Var[X] = (1-3.5)¬≤√ó(1/6) + ... + (6-3.5)¬≤√ó(1/6) ‚âà 2.92

print(f"Expected value: {E_X}")
print(f"Variance: {Var_X:.2f}")
print(f"Standard deviation: {np.sqrt(Var_X):.2f}")
```

**Continuous Random Variable**:
```python
# Probability Density Function (PDF)

# Normal distribution: N(Œº, œÉ¬≤)
def normal_pdf(x, mu=0, sigma=1):
    return (1 / (sigma * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mu) / sigma)**2)

# Visualize
x = np.linspace(-5, 5, 1000)
y = normal_pdf(x, mu=0, sigma=1)

plt.plot(x, y)
plt.xlabel('x')
plt.ylabel('Probability Density')
plt.title('Standard Normal Distribution N(0,1)')
plt.grid(True)
plt.show()

# Cumulative Distribution Function (CDF)
from scipy.stats import norm

# P(X ‚â§ x)
cdf_value = norm.cdf(1.96)  # P(X ‚â§ 1.96) ‚âà 0.975 (95%)
print(f"P(X ‚â§ 1.96) = {cdf_value:.4f}")

# Inverse CDF (quantile function)
quantile = norm.ppf(0.95)  # Value where 95% of data is below
print(f"95th percentile: {quantile:.4f}")  # ‚âà 1.645
```

### Common Distributions

**Bernoulli Distribution** (Single coin flip):
```python
# X ‚àà {0, 1}
# P(X=1) = p

p = 0.7  # Probability of heads

X = np.random.binomial(n=1, p=p, size=1000)  # 1000 flips

print(f"Proportion of heads: {X.mean():.3f}")  # ‚âà 0.700
# E[X] = p = 0.7
# Var[X] = p(1-p) = 0.7√ó0.3 = 0.21

# ML application: Binary classification output
```

**Binomial Distribution** (n coin flips):
```python
# Number of successes in n trials
# X ~ Binomial(n, p)

n = 10  # 10 flips
p = 0.5  # Fair coin

X = np.random.binomial(n=n, p=p, size=10000)

plt.hist(X, bins=range(12), density=True, alpha=0.7, edgecolor='black')
plt.xlabel('Number of heads')
plt.ylabel('Probability')
plt.title(f'Binomial Distribution (n={n}, p={p})')
plt.show()

# E[X] = np = 5
# Var[X] = np(1-p) = 2.5
```

**Normal (Gaussian) Distribution**:
```python
# X ~ N(Œº, œÉ¬≤)
# Most important distribution in ML!

mu, sigma = 0, 1
X = np.random.normal(mu, sigma, size=10000)

plt.hist(X, bins=50, density=True, alpha=0.7, edgecolor='black')
x = np.linspace(-4, 4, 100)
plt.plot(x, norm.pdf(x, mu, sigma), 'r-', linewidth=2, label='PDF')
plt.xlabel('x')
plt.ylabel('Density')
plt.title('Normal Distribution N(0,1)')
plt.legend()
plt.show()

# Properties:
# - 68% of data within 1 std
# - 95% within 2 std
# - 99.7% within 3 std

# ML applications:
# - Weight initialization: N(0, 1/sqrt(n))
# - Noise model in regression
# - Gaussian processes
# - Variational autoencoders
```

**Multivariate Normal Distribution**:
```python
# X ~ N(Œº, Œ£)
# Œº: mean vector
# Œ£: covariance matrix

mean = [0, 0]
cov = [[1, 0.5],
       [0.5, 1]]  # Correlation = 0.5

X = np.random.multivariate_normal(mean, cov, size=1000)

plt.scatter(X[:, 0], X[:, 1], alpha=0.5)
plt.xlabel('X1')
plt.ylabel('X2')
plt.title('Bivariate Normal Distribution')
plt.axis('equal')
plt.grid(True)
plt.show()

# ML applications:
# - Gaussian mixture models
# - Generative models
# - Bayesian inference
```

### Maximum Likelihood Estimation (MLE)

```python
# Find parameters that maximize P(data | parameters)

def mle_normal_distribution(data):
    """
    Estimate Œº and œÉ¬≤ for normal distribution

    Likelihood: L(Œº, œÉ¬≤) = Œ† (1/‚àö(2œÄœÉ¬≤)) exp(-(x_i - Œº)¬≤/(2œÉ¬≤))

    Log-likelihood: log L = -n/2 log(2œÄœÉ¬≤) - Œ£(x_i - Œº)¬≤/(2œÉ¬≤)

    MLE estimates:
    Œº_MLE = sample mean
    œÉ¬≤_MLE = sample variance
    """
    mu_mle = np.mean(data)
    sigma2_mle = np.var(data, ddof=0)  # ddof=0 for MLE (ddof=1 for unbiased)

    return mu_mle, sigma2_mle

# Generate data
true_mu, true_sigma = 5, 2
data = np.random.normal(true_mu, true_sigma, size=1000)

# Estimate parameters
mu_hat, sigma2_hat = mle_normal_distribution(data)

print(f"True parameters: Œº={true_mu}, œÉ¬≤={true_sigma**2}")
print(f"MLE estimates: Œº={mu_hat:.3f}, œÉ¬≤={sigma2_hat:.3f}")
# MLE estimates should be very close to true parameters!
```

**MLE for Linear Regression**:
```python
# Assume: y = Xw + Œµ, where Œµ ~ N(0, œÉ¬≤)
# Likelihood: P(y | X, w, œÉ¬≤) = Œ† N(y_i | x_i^T w, œÉ¬≤)

# Maximizing likelihood ‚â° Minimizing MSE!

# log L = -n/2 log(2œÄœÉ¬≤) - Œ£(y_i - x_i^T w)¬≤/(2œÉ¬≤)
# Maximizing log L w.r.t. w ‚â° Minimizing Œ£(y_i - x_i^T w)¬≤
# This is exactly MSE!

# MLE for linear regression = Least squares solution
```

### Convergence Theorems (Limit Laws)

These fundamental theorems justify many ML practices like averaging predictions and empirical risk minimization.

**Law of Large Numbers (LLN)**:
```
Weak Law of Large Numbers (WLLN):
Let X‚ÇÅ, X‚ÇÇ, ..., X‚Çô be i.i.d. random variables with E[X·µ¢] = Œº and Var(X·µ¢) = œÉ¬≤ < ‚àû

Sample mean: XÃÑ‚Çô = (1/n) Œ£·µ¢‚Çå‚ÇÅ‚Åø X·µ¢

Then: XÃÑ‚Çô ‚Üí·µñ Œº as n ‚Üí ‚àû

(Convergence in probability: lim P(|XÃÑ‚Çô - Œº| > Œµ) = 0 for all Œµ > 0)

Strong Law of Large Numbers (SLLN):
XÃÑ‚Çô ‚Üí Œº almost surely (a.s.)

(P(lim XÃÑ‚Çô = Œº) = 1)

Proof Sketch (Weak Law via Chebyshev):
By Chebyshev inequality: P(|XÃÑ‚Çô - Œº| ‚â• Œµ) ‚â§ Var(XÃÑ‚Çô)/Œµ¬≤

Var(XÃÑ‚Çô) = Var((1/n)Œ£ X·µ¢) = (1/n¬≤) √ó n √ó œÉ¬≤ = œÉ¬≤/n

Therefore: P(|XÃÑ‚Çô - Œº| ‚â• Œµ) ‚â§ œÉ¬≤/(nŒµ¬≤) ‚Üí 0 as n ‚Üí ‚àû  ‚úì

ML Applications:
1. Empirical Risk Minimization (ERM):
   Sample loss (1/n)Œ£ L(Œ∏; x·µ¢) ‚Üí Expected loss E[L(Œ∏; X)] as n ‚Üí ‚àû

2. Monte Carlo Estimation:
   Sample average ‚Üí True expectation

3. Ensemble Methods:
   Average of predictions ‚Üí Expected prediction
```

**Central Limit Theorem (CLT)**:
```
Let X‚ÇÅ, X‚ÇÇ, ..., X‚Çô be i.i.d. with E[X·µ¢] = Œº and Var(X·µ¢) = œÉ¬≤ < ‚àû

Sample mean: XÃÑ‚Çô = (1/n) Œ£·µ¢‚Çå‚ÇÅ‚Åø X·µ¢

Standardized sum: Z‚Çô = ‚àön(XÃÑ‚Çô - Œº)/œÉ

Then: Z‚Çô ‚Üí·µà N(0,1) as n ‚Üí ‚àû

(Convergence in distribution to standard normal)

Equivalently: XÃÑ‚Çô ‚âà N(Œº, œÉ¬≤/n) for large n

Berry-Esseen Theorem (Quantitative CLT):
Let œÅ = E[|X·µ¢ - Œº|¬≥] < ‚àû

Then: sup_x |P(Z‚Çô ‚â§ x) - Œ¶(x)| ‚â§ CœÅ/(œÉ¬≥‚àön)

where Œ¶ is standard normal CDF, C ‚âà 0.4748

Convergence Rate: O(1/‚àön)

Practical Rule: n ‚â• 30 usually sufficient for normal approximation

ML Applications:
1. Confidence Intervals:
   XÃÑ‚Çô ¬± 1.96 √ó œÉ/‚àön gives 95% CI for Œº

2. Hypothesis Testing:
   Test statistic ‚àön(XÃÑ‚Çô - Œº‚ÇÄ)/œÉ ~ N(0,1) under H‚ÇÄ

3. Bootstrap Distribution:
   Sampling distribution of statistics approximately normal

4. Gradient Descent Noise:
   Stochastic gradient ‚àáÃÇL ‚âà N(‚àáL, Œ£/n) for large batch size n

5. Neural Network Outputs:
   Sum of many small contributions ‚Üí approximately normal
```

**Example Application**:
```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# Demonstrate CLT with non-normal distribution
def clt_demo(n_samples, n_means=10000, distribution='uniform'):
    """
    Demonstrate Central Limit Theorem

    Even for non-normal distributions, sample means are approximately normal!
    """
    # Generate data from various distributions
    sample_means = []

    for _ in range(n_means):
        if distribution == 'uniform':
            # Uniform [0, 1]: Œº=0.5, œÉ¬≤=1/12
            sample = np.random.uniform(0, 1, n_samples)
            mu, sigma2 = 0.5, 1/12
        elif distribution == 'exponential':
            # Exponential(Œª=1): Œº=1, œÉ¬≤=1
            sample = np.random.exponential(1, n_samples)
            mu, sigma2 = 1.0, 1.0
        elif distribution == 'bernoulli':
            # Bernoulli(p=0.3): Œº=0.3, œÉ¬≤=0.21
            sample = np.random.binomial(1, 0.3, n_samples)
            mu, sigma2 = 0.3, 0.3*0.7

        sample_means.append(np.mean(sample))

    sample_means = np.array(sample_means)

    # Theoretical normal distribution
    theoretical_mean = mu
    theoretical_std = np.sqrt(sigma2 / n_samples)

    # Plot
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.hist(sample_means, bins=50, density=True, alpha=0.7, edgecolor='black')
    x = np.linspace(sample_means.min(), sample_means.max(), 100)
    plt.plot(x, norm.pdf(x, theoretical_mean, theoretical_std),
             'r-', linewidth=2, label=f'N({theoretical_mean:.2f}, {theoretical_std:.4f}¬≤)')
    plt.xlabel('Sample Mean')
    plt.ylabel('Density')
    plt.title(f'CLT Demo: {distribution.capitalize()} Distribution (n={n_samples})')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Q-Q plot
    plt.subplot(1, 2, 2)
    from scipy.stats import probplot
    probplot(sample_means, dist="norm", plot=plt)
    plt.title('Q-Q Plot (Normal)')
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    # Numerical verification
    empirical_mean = np.mean(sample_means)
    empirical_std = np.std(sample_means, ddof=1)

    print(f"Distribution: {distribution}")
    print(f"Sample size n = {n_samples}")
    print(f"Theoretical: Œº = {theoretical_mean:.4f}, œÉ = {theoretical_std:.4f}")
    print(f"Empirical:   Œº = {empirical_mean:.4f}, œÉ = {empirical_std:.4f}")
    print(f"Error: |Œº_emp - Œº_theory| = {abs(empirical_mean - theoretical_mean):.6f}")

# Try with different sample sizes
for n in [5, 30, 100]:
    print(f"\n{'='*60}")
    clt_demo(n_samples=n, distribution='exponential')
    # As n increases, empirical matches theoretical better!
```

---

## üìä Statistics

Statistics provides tools for inference, hypothesis testing, and understanding data.

### Descriptive Statistics

**Measures of Central Tendency**:
```python
data = np.array([1, 2, 3, 4, 5, 100])  # Note outlier

# Mean (sensitive to outliers)
mean = np.mean(data)  # 19.17

# Median (robust to outliers)
median = np.median(data)  # 3.5

# Mode (most frequent value)
from scipy.stats import mode
mode_result = mode(data, keepdims=True)  # No mode (all unique)

print(f"Mean: {mean:.2f}")
print(f"Median: {median:.2f}")

# For skewed distributions (with outliers), median is better
```

**Measures of Spread**:
```python
# Variance
variance = np.var(data, ddof=1)  # Sample variance (ddof=1)

# Standard deviation
std = np.std(data, ddof=1)

# Interquartile range (IQR) - robust to outliers
Q1 = np.percentile(data, 25)
Q3 = np.percentile(data, 75)
IQR = Q3 - Q1

print(f"Variance: {variance:.2f}")
print(f"Std dev: {std:.2f}")
print(f"IQR: {IQR:.2f}")
```

**Covariance and Correlation**:
```python
# Two variables: height and weight

height = np.array([160, 165, 170, 175, 180])
weight = np.array([55, 60, 65, 70, 75])

# Covariance: How much do they vary together?
cov = np.cov(height, weight)[0, 1]  # 25.0

# Correlation: Normalized covariance (-1 to 1)
corr = np.corrcoef(height, weight)[0, 1]  # 1.0 (perfect positive correlation)

print(f"Covariance: {cov:.2f}")
print(f"Correlation: {corr:.3f}")

# Interpretation:
# corr = 1: Perfect positive correlation
# corr = 0: No linear correlation
# corr = -1: Perfect negative correlation

# ML application: Feature selection
# Remove highly correlated features (multicollinearity)
def remove_correlated_features(X, threshold=0.95):
    corr_matrix = np.corrcoef(X.T)
    to_remove = set()

    for i in range(len(corr_matrix)):
        for j in range(i+1, len(corr_matrix)):
            if abs(corr_matrix[i, j]) > threshold:
                to_remove.add(j)

    mask = np.ones(X.shape[1], dtype=bool)
    mask[list(to_remove)] = False
    return X[:, mask]
```

### Statistical Inference Theory

**Confidence Intervals**:
```
Definition: A (1-Œ±)100% confidence interval for parameter Œ∏ is an interval [L, U] where:

P(L ‚â§ Œ∏ ‚â§ U) = 1 - Œ±

Common values: Œ± = 0.05 (95% CI), Œ± = 0.01 (99% CI)

Interpretation (Frequentist):
- If we repeat the experiment many times and construct CIs each time,
  approximately (1-Œ±)100% of intervals will contain true Œ∏
- NOT: "Œ∏ has 95% probability of being in this interval" (Œ∏ is fixed, not random!)

For Sample Mean (œÉ known):
XÃÑ‚Çô ~ N(Œº, œÉ¬≤/n) by CLT

95% CI: XÃÑ‚Çô ¬± z_{Œ±/2} √ó œÉ/‚àön
where z_{0.025} = 1.96 for 95% CI

For Sample Mean (œÉ unknown):
Use sample std s: 95% CI: XÃÑ‚Çô ¬± t_{n-1,Œ±/2} √ó s/‚àön
where t_{n-1,Œ±/2} is t-distribution critical value with n-1 degrees of freedom

Standard Error (SE):
SE(XÃÑ‚Çô) = œÉ/‚àön (known variance)
SE(XÃÑ‚Çô) = s/‚àön (estimated variance)

Margin of Error: ME = z_{Œ±/2} √ó SE

Precision: ‚àù 1/‚àön (quadruple sample size ‚Üí double precision)
```

**Hypothesis Testing Framework**:
```
Formal Structure:

1. Null Hypothesis H‚ÇÄ: Default assumption (e.g., "no effect", "Œº = Œº‚ÇÄ")
2. Alternative Hypothesis H‚ÇÅ: What we want to test (e.g., "effect exists", "Œº ‚â† Œº‚ÇÄ")
3. Test Statistic T: Function of data that measures evidence against H‚ÇÄ
4. Significance Level Œ±: Threshold for rejecting H‚ÇÄ (typically 0.05 or 0.01)
5. p-value: P(observe T as extreme as observed | H‚ÇÄ is true)
6. Decision Rule: Reject H‚ÇÄ if p-value < Œ±

Types of Tests:
- Two-sided: H‚ÇÅ: Œº ‚â† Œº‚ÇÄ (detect any difference)
- One-sided: H‚ÇÅ: Œº > Œº‚ÇÄ or H‚ÇÅ: Œº < Œº‚ÇÄ (directional)

Error Types:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ             ‚îÇ H‚ÇÄ True          ‚îÇ H‚ÇÄ False         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Reject H‚ÇÄ   ‚îÇ Type I Error (Œ±) ‚îÇ Correct (Power)  ‚îÇ
‚îÇ             ‚îÇ False Positive   ‚îÇ True Positive    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Fail to     ‚îÇ Correct (1-Œ±)    ‚îÇ Type II Error (Œ≤)‚îÇ
‚îÇ Reject H‚ÇÄ   ‚îÇ True Negative    ‚îÇ False Negative   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Definitions:
- Significance Level Œ± = P(Type I Error) = P(Reject H‚ÇÄ | H‚ÇÄ true)
- Type II Error Rate Œ≤ = P(Type II Error) = P(Fail to reject H‚ÇÄ | H‚ÇÅ true)
- Statistical Power = 1 - Œ≤ = P(Reject H‚ÇÄ | H‚ÇÅ true)

Power Analysis:
Power depends on:
1. Sample size n (larger n ‚Üí higher power)
2. Effect size Œ¥ = |Œº‚ÇÅ - Œº‚ÇÄ|/œÉ (larger effect ‚Üí higher power)
3. Significance level Œ± (larger Œ± ‚Üí higher power, but more false positives)

For t-test with known œÉ:
Power = Œ¶(‚àön √ó Œ¥/œÉ - z_{Œ±/2}) + Œ¶(-‚àön √ó Œ¥/œÉ - z_{Œ±/2})

To achieve power = 0.8 for two-sided test (Œ± = 0.05):
n ‚âà 16 √ó (œÉ/Œ¥)¬≤ (rule of thumb)

Example: To detect effect size Œ¥ = 0.5œÉ with 80% power:
n ‚âà 16 √ó (œÉ/0.5œÉ)¬≤ = 64 samples per group

p-value Interpretation:
- p < 0.001: Very strong evidence against H‚ÇÄ
- 0.001 ‚â§ p < 0.01: Strong evidence against H‚ÇÄ
- 0.01 ‚â§ p < 0.05: Moderate evidence against H‚ÇÄ
- 0.05 ‚â§ p < 0.10: Weak evidence against H‚ÇÄ
- p ‚â• 0.10: Little to no evidence against H‚ÇÄ

Common Misconceptions:
‚úó p-value is NOT P(H‚ÇÄ is true | data)
‚úì p-value IS P(data as extreme | H‚ÇÄ is true)

‚úó p < 0.05 does NOT mean "important" or "large effect"
‚úì Statistical significance ‚â† Practical significance

‚úó p > 0.05 does NOT prove H‚ÇÄ is true
‚úì Absence of evidence ‚â† Evidence of absence

Multiple Testing Correction:
When testing m hypotheses:
- Bonferroni: Use Œ±/m for each test (conservative)
- Holm-Bonferroni: Sequential procedure (less conservative)
- False Discovery Rate (FDR): Control expected proportion of false positives
  Benjamini-Hochberg: Ensures FDR ‚â§ q (e.g., q = 0.05)

Family-Wise Error Rate (FWER):
P(at least one Type I error) = 1 - (1-Œ±)^m ‚âà m√óŒ± for small Œ±
Example: m=20 tests, Œ±=0.05 ‚Üí FWER ‚âà 0.64 (64% chance of false positive!)
```

**Sufficient Statistics and Fisher Information**:
```
Sufficient Statistic T(X):
A statistic T(X) is sufficient for Œ∏ if and only if:
P(X | T(X), Œ∏) = P(X | T(X))

(Data X contains no more information about Œ∏ beyond T(X))

Factorization Theorem (Neyman-Fisher):
T(X) is sufficient for Œ∏ ‚ü∫ p(x|Œ∏) = g(T(x), Œ∏) √ó h(x)

Example: For X‚ÇÅ, ..., X‚Çô ~ N(Œº, œÉ¬≤):
- T(X) = XÃÑ is sufficient for Œº (when œÉ¬≤ known)
- T(X) = (XÃÑ, S¬≤) is sufficient for (Œº, œÉ¬≤)

Fisher Information:
I(Œ∏) = E[(‚àÇ/‚àÇŒ∏ log p(X|Œ∏))¬≤] = -E[‚àÇ¬≤/‚àÇŒ∏¬≤ log p(X|Œ∏)]

Measures: Information about Œ∏ contained in single observation

Properties:
1. I(Œ∏) ‚â• 0 (non-negative)
2. For n i.i.d. observations: I‚Çô(Œ∏) = n √ó I(Œ∏) (additivity)
3. Under reparameterization Œ∏ ‚Üí œÜ(Œ∏): I_œÜ = I_Œ∏ √ó (dŒ∏/dœÜ)¬≤

Cram√©r-Rao Lower Bound:
For any unbiased estimator Œ∏ÃÇ of Œ∏:

Var(Œ∏ÃÇ) ‚â• 1/I(Œ∏)

Efficient Estimator: Achieves equality (minimum variance among unbiased estimators)

Example: For X ~ N(Œº, œÉ¬≤) with œÉ¬≤ known:
Fisher information: I(Œº) = 1/œÉ¬≤
Sample mean XÃÑ‚Çô: Var(XÃÑ‚Çô) = œÉ¬≤/n = 1/(n√óI(Œº))
Therefore XÃÑ‚Çô is efficient! ‚úì

ML Application:
Fisher information appears in:
- Natural gradient descent: Œ∏_{t+1} = Œ∏_t - Œ± √ó I(Œ∏)‚Åª¬π √ó ‚àáL
- Uncertainty quantification in neural networks
- Bayesian posterior approximation (Laplace approximation)
```

### Hypothesis Testing

**t-test** (Compare two means):
```python
from scipy.stats import ttest_ind

# A/B test: Did the new feature increase conversion?
control = np.random.binomial(1, 0.10, size=1000)  # 10% conversion
treatment = np.random.binomial(1, 0.12, size=1000)  # 12% conversion

# Two-sample t-test
t_stat, p_value = ttest_ind(treatment, control)

print(f"t-statistic: {t_stat:.3f}")
print(f"p-value: {p_value:.4f}")

alpha = 0.05
if p_value < alpha:
    print("Result: Statistically significant! ‚úÖ")
    print("The new feature improved conversion.")
else:
    print("Result: Not statistically significant ‚ùå")
    print("Cannot conclude the new feature is better.")

# Confidence interval
from scipy.stats import t as t_dist

mean_diff = treatment.mean() - control.mean()
se = np.sqrt(treatment.var()/len(treatment) + control.var()/len(control))
ci_lower = mean_diff - 1.96 * se
ci_upper = mean_diff + 1.96 * se

print(f"\nMean difference: {mean_diff:.4f}")
print(f"95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]")
```

**Chi-Square Test** (Categorical data):
```python
from scipy.stats import chi2_contingency

# Are gender and product preference independent?
#              Product A   Product B
# Male            30          70
# Female          50          50

observed = np.array([[30, 70],
                     [50, 50]])

chi2, p_value, dof, expected = chi2_contingency(observed)

print(f"Chi-square statistic: {chi2:.3f}")
print(f"p-value: {p_value:.4f}")
print(f"Expected frequencies:\n{expected}")

if p_value < 0.05:
    print("Gender and product preference are dependent")
else:
    print("Gender and product preference are independent")
```

---

## üéØ Optimization

Optimization is how we train ML models - finding parameters that minimize loss.

### Convex Optimization

**Convex Function**:
```
f is convex if: f(Œªx + (1-Œª)y) ‚â§ Œªf(x) + (1-Œª)f(y) for all Œª ‚àà [0,1]

Geometric meaning: Line segment between any two points lies above the function

Good news: Local minimum = Global minimum!
```

**Examples**:
```python
# Convex functions:
# - f(x) = x¬≤
# - f(x) = |x|
# - f(x) = exp(x)
# - f(x) = -log(x)

# Non-convex functions:
# - f(x) = x¬≥
# - f(x) = sin(x)
# - Neural networks (non-convex!)

# Visualize
x = np.linspace(-3, 3, 100)

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# Convex
axes[0].plot(x, x**2, label='x¬≤')
axes[0].set_title('Convex Function')
axes[0].legend()
axes[0].grid(True)

# Non-convex
axes[1].plot(x, x**3 - 3*x, label='x¬≥ - 3x')
axes[1].set_title('Non-Convex Function')
axes[1].legend()
axes[1].grid(True)

plt.show()
```

### Optimization Algorithms

**Gradient Descent** (covered earlier):
```python
# w_new = w_old - learning_rate √ó gradient
```

#### Convergence Theory for Gradient Descent Variants

**Mathematical Framework:**

**Assumptions and Definitions:**
```
L-Lipschitz Continuous Gradient:
||‚àáf(x) - ‚àáf(y)|| ‚â§ L¬∑||x - y|| for all x, y

Convexity:
f(y) ‚â• f(x) + ‚àáf(x)^T(y - x) for all x, y

Strong Convexity (with parameter Œº > 0):
f(y) ‚â• f(x) + ‚àáf(x)^T(y - x) + (Œº/2)||y - x||¬≤ for all x, y

Smoothness implies: f(y) ‚â§ f(x) + ‚àáf(x)^T(y - x) + (L/2)||y - x||¬≤
```

**Theorem 1: Gradient Descent Convergence for Smooth Convex Functions**
```
Problem: min_{x‚àà‚Ñù^d} f(x), where f is convex and L-smooth

Algorithm: x_{k+1} = x_k - Œ±¬∑‚àáf(x_k)

Step Size: Œ± ‚â§ 1/L

Convergence Rate:
f(x_k) - f(x*) ‚â§ (2L¬∑||x_0 - x*||¬≤) / (k + 4)
             = O(1/k)

Result: Sublinear convergence, need O(1/Œµ) iterations for Œµ-accuracy
```

**Proof Sketch:**
```
Key Lemma (Descent Lemma):
For Œ± ‚â§ 1/L and L-smooth f:
f(x_{k+1}) ‚â§ f(x_k) - (Œ±/2)||‚àáf(x_k)||¬≤

Proof of Lemma:
1. By L-smoothness:
   f(x_{k+1}) ‚â§ f(x_k) + ‚àáf(x_k)^T(x_{k+1} - x_k) + (L/2)||x_{k+1} - x_k||¬≤

2. Substitute x_{k+1} = x_k - Œ±¬∑‚àáf(x_k):
   f(x_{k+1}) ‚â§ f(x_k) - Œ±¬∑||‚àáf(x_k)||¬≤ + (LŒ±¬≤/2)||‚àáf(x_k)||¬≤
              = f(x_k) - Œ±(1 - LŒ±/2)||‚àáf(x_k)||¬≤

3. If Œ± ‚â§ 1/L, then 1 - LŒ±/2 ‚â• 1/2:
   f(x_{k+1}) ‚â§ f(x_k) - (Œ±/2)||‚àáf(x_k)||¬≤  ‚úì

Main Convergence Proof:
1. By convexity: f(x_k) - f(x*) ‚â§ ‚àáf(x_k)^T(x_k - x*)

2. Expand ||x_{k+1} - x*||¬≤:
   ||x_{k+1} - x*||¬≤ = ||x_k - Œ±¬∑‚àáf(x_k) - x*||¬≤
                     = ||x_k - x*||¬≤ - 2Œ±¬∑‚àáf(x_k)^T(x_k - x*) + Œ±¬≤||‚àáf(x_k)||¬≤

3. Rearrange:
   2Œ±¬∑‚àáf(x_k)^T(x_k - x*) = ||x_k - x*||¬≤ - ||x_{k+1} - x*||¬≤ + Œ±¬≤||‚àáf(x_k)||¬≤
                           ‚â• ||x_k - x*||¬≤ - ||x_{k+1} - x*||¬≤  (drop positive term)

4. Therefore:
   f(x_k) - f(x*) ‚â§ (||x_k - x*||¬≤ - ||x_{k+1} - x*||¬≤) / (2Œ±)

5. Sum from k=0 to K-1:
   Œ£_{k=0}^{K-1} [f(x_k) - f(x*)] ‚â§ (||x_0 - x*||¬≤ - ||x_K - x*||¬≤) / (2Œ±)
                                   ‚â§ ||x_0 - x*||¬≤ / (2Œ±)

6. Since f(x_k) is decreasing (by Descent Lemma):
   K¬∑[f(x_K) - f(x*)] ‚â§ Œ£_{k=0}^{K-1} [f(x_k) - f(x*)] ‚â§ ||x_0 - x*||¬≤ / (2Œ±)

7. Final bound:
   f(x_K) - f(x*) ‚â§ ||x_0 - x*||¬≤ / (2Œ±K)

   With Œ± = 1/L:
   f(x_K) - f(x*) ‚â§ L¬∑||x_0 - x*||¬≤ / (2K) = O(1/K)  ‚úì
```

**Theorem 2: Gradient Descent for Strongly Convex Functions**
```
Problem: min f(x), where f is Œº-strongly convex and L-smooth

Step Size: Œ± ‚â§ 2/(Œº + L) (or simply Œ± = 1/L)

Convergence Rate:
||x_k - x*||¬≤ ‚â§ (1 - Œº/L)^k ¬∑ ||x_0 - x*||¬≤
f(x_k) - f(x*) ‚â§ (L/2)(1 - Œº/L)^k ¬∑ ||x_0 - x*||¬≤

Result: Linear (exponential) convergence!
Need O(log(1/Œµ)) iterations for Œµ-accuracy
```

**Proof Sketch:**
```
Key Property: For Œº-strongly convex and L-smooth f:
||‚àáf(x)||¬≤ ‚â• 2Œº[f(x) - f(x*)]

Proof:
1. By strong convexity at x*:
   f(x) ‚â• f(x*) + ‚àáf(x*)^T(x - x*) + (Œº/2)||x - x*||¬≤
        = f(x*) + (Œº/2)||x - x*||¬≤  (since ‚àáf(x*) = 0)

2. By smoothness at x:
   f(x*) ‚â• f(x) + ‚àáf(x)^T(x* - x) - (L/2)||x* - x||¬≤

3. Combine:
   ||‚àáf(x)||¬≤ = ||‚àáf(x) - ‚àáf(x*)||¬≤ ‚â• ŒºL¬∑||x - x*||¬≤  (PL inequality)
              ‚â• 2Œº[f(x) - f(x*)]  ‚úì

Main Convergence:
1. Start with:
   ||x_{k+1} - x*||¬≤ = ||x_k - x*||¬≤ - 2Œ±¬∑‚àáf(x_k)^T(x_k - x*) + Œ±¬≤||‚àáf(x_k)||¬≤

2. By strong convexity:
   ‚àáf(x_k)^T(x_k - x*) ‚â• f(x_k) - f(x*) + (Œº/2)||x_k - x*||¬≤

3. Substitute Œ± = 1/L:
   ||x_{k+1} - x*||¬≤ ‚â§ ||x_k - x*||¬≤[1 - Œº/L] + (1/L¬≤)||‚àáf(x_k)||¬≤[1 - Œº/L]
                     ‚â§ (1 - Œº/L)||x_k - x*||¬≤

4. Iterate:
   ||x_k - x*||¬≤ ‚â§ (1 - Œº/L)^k ¬∑ ||x_0 - x*||¬≤  ‚úì

Condition Number: Œ∫ = L/Œº
- If Œ∫ is small (well-conditioned): Fast convergence
- If Œ∫ is large (ill-conditioned): Slow convergence
```

**Theorem 3: Stochastic Gradient Descent (SGD) Convergence**
```
Problem: min f(x) = E_{Œæ}[f(x; Œæ)]
         Where Œæ represents random data samples

Algorithm: x_{k+1} = x_k - Œ±_k¬∑‚àáf(x_k; Œæ_k)
          ‚àáf(x_k; Œæ_k) is unbiased: E[‚àáf(x_k; Œæ_k)] = ‚àáf(x_k)

Robbins-Monro Conditions (for learning rate Œ±_k):
1. Œ£_{k=1}^‚àû Œ±_k = ‚àû        (step sizes sum to infinity)
2. Œ£_{k=1}^‚àû Œ±_k¬≤ < ‚àû       (step sizes squared sum is finite)

Example: Œ±_k = Œ±_0/‚àök satisfies both conditions

Convergence Result (for convex f):
E[f(x_k)] - f(x*) = O(1/‚àök)

Result: Slower than batch GD (O(1/k)), but much cheaper per iteration!
```

**Proof Intuition:**
```
Key Inequality:
E[||x_{k+1} - x*||¬≤] = E[||x_k - Œ±_k¬∑g_k - x*||¬≤]
                     = ||x_k - x*||¬≤ - 2Œ±_k¬∑‚àáf(x_k)^T(x_k - x*) + Œ±_k¬≤¬∑E[||g_k||¬≤]

where g_k = ‚àáf(x_k; Œæ_k) is stochastic gradient with variance œÉ¬≤

Trade-off:
- Term 1: -2Œ±_k¬∑‚àáf(x_k)^T(x_k - x*) ‚Üí Progress towards optimum
- Term 2: +Œ±_k¬≤¬∑œÉ¬≤ ‚Üí Variance from stochastic gradient

As k ‚Üí ‚àû:
- Œ±_k ‚Üí 0 makes variance term ‚Üí 0 (condition 2)
- But Œ£Œ±_k = ‚àû ensures we reach optimum (condition 1)
```

**Theorem 4: Momentum Convergence (Nesterov Accelerated Gradient)**
```
Algorithm:
v_{k+1} = Œ≤¬∑v_k + ‚àáf(x_k)
x_{k+1} = x_k - Œ±¬∑v_{k+1}

Convergence for Convex L-smooth:
f(x_k) - f(x*) = O(1/k¬≤)  (compared to O(1/k) for vanilla GD!)

Convergence for Strongly Convex:
||x_k - x*|| = O((1 - ‚àö(Œº/L))^k)  (improved constant)

Result: Optimal first-order method for smooth convex optimization
```

**Theorem 5: Adam Convergence (Sketch)**
```
Algorithm:
m_k = Œ≤_1¬∑m_{k-1} + (1-Œ≤_1)¬∑g_k         (first moment)
v_k = Œ≤_2¬∑v_{k-1} + (1-Œ≤_2)¬∑g_k¬≤        (second moment)
mÃÇ_k = m_k / (1 - Œ≤_1^k)                 (bias correction)
vÃÇ_k = v_k / (1 - Œ≤_2^k)                 (bias correction)
x_{k+1} = x_k - Œ±¬∑mÃÇ_k / (‚àövÃÇ_k + Œµ)

Typical values: Œ≤_1 = 0.9, Œ≤_2 = 0.999, Œµ = 10^(-8)

Convergence (for convex case):
Regret bound: R_T = O(‚àöT)
Average convergence: (1/T)Œ£_{k=1}^T [f(x_k) - f(x*)] = O(1/‚àöT)

Note: Adam may NOT converge for some convex problems!
Fix: AMSGrad variant with max(v_1, ..., v_k) instead of v_k
```

**Summary Table: Convergence Rates**
```
Algorithm        | Convex      | Strongly Convex    | Per-iteration Cost
-----------------|-------------|--------------------|-----------------
GD               | O(1/k)      | O(exp(-Œºk/L))     | O(nd)
SGD              | O(1/‚àök)     | O(1/k)            | O(d)
Momentum (NAG)   | O(1/k¬≤)     | O(exp(-‚àöŒº/L¬∑k))   | O(nd)
Adam/RMSprop     | O(1/‚àök)     | O(1/‚àök)           | O(d)
Newton           | O(1/k¬≤)     | Quadratic         | O(nd¬≤+d¬≥)

Where:
- n: dataset size
- d: dimension
- k: iteration number
- Œº: strong convexity parameter
- L: smoothness parameter
- Œ∫ = L/Œº: condition number
```

**Newton's Method** (uses second derivative):
```python
def newtons_method(f, grad_f, hess_f, x0, num_iterations=10):
    """
    Newton's method for optimization

    Uses second-order information (Hessian)
    Update: x_new = x_old - H^(-1) @ grad

    Advantages: Faster convergence (quadratic vs linear)
    Disadvantages: Expensive (need to compute and invert Hessian)
    """
    x = x0.copy()

    for i in range(num_iterations):
        grad = grad_f(x)
        hess = hess_f(x)

        # Newton step
        delta = np.linalg.solve(hess, grad)  # H^(-1) @ grad
        x = x - delta

        print(f"Iteration {i+1}: x = {x}, f(x) = {f(x):.6f}")

    return x

# Example: f(x) = x^2 + 2x + 1
f = lambda x: x**2 + 2*x + 1
grad_f = lambda x: 2*x + 2
hess_f = lambda x: np.array([[2.0]])  # Second derivative

x_min = newtons_method(f, grad_f, hess_f, x0=np.array([5.0]))
# Converges in 1 iteration! (function is quadratic)
```

**Constrained Optimization (Lagrange Multipliers)**:
```python
# Minimize f(x) subject to g(x) = 0

# Example: Maximize f(x,y) = xy subject to x + y = 1

# Lagrangian: L(x, y, Œª) = xy + Œª(1 - x - y)

# Set derivatives to 0:
# ‚àÇL/‚àÇx = y - Œª = 0
# ‚àÇL/‚àÇy = x - Œª = 0
# ‚àÇL/‚àÇŒª = 1 - x - y = 0

# Solve:
# y = Œª, x = Œª, x + y = 1
# 2Œª = 1 ‚Üí Œª = 0.5
# x = y = 0.5

# Maximum: f(0.5, 0.5) = 0.25

def lagrange_example():
    from scipy.optimize import minimize

    # Objective: Minimize -xy (negative for maximization)
    def objective(vars):
        x, y = vars
        return -x * y

    # Constraint: x + y - 1 = 0
    constraint = {'type': 'eq', 'fun': lambda vars: vars[0] + vars[1] - 1}

    # Initial guess
    x0 = [0, 0]

    # Optimize
    result = minimize(objective, x0, constraints=constraint)

    print(f"Optimal solution: x={result.x[0]:.3f}, y={result.x[1]:.3f}")
    print(f"Maximum value: {-result.fun:.3f}")

lagrange_example()
# Output: x=0.500, y=0.500, maximum=0.250
```

### Advanced Constrained Optimization

**Karush-Kuhn-Tucker (KKT) Conditions**:
```
General Constrained Problem:
minimize f(x)
subject to:
  g·µ¢(x) ‚â§ 0  for i = 1, ..., m  (inequality constraints)
  h‚±º(x) = 0  for j = 1, ..., p  (equality constraints)

Lagrangian:
L(x, Œª, Œº) = f(x) + Œ£·µ¢ Œª·µ¢g·µ¢(x) + Œ£‚±º Œº‚±ºh‚±º(x)

where Œª·µ¢ ‚â• 0 (inequality multipliers), Œº‚±º ‚àà ‚Ñù (equality multipliers)

KKT Necessary Conditions (for x* to be optimal):
Assume f, g·µ¢, h‚±º are differentiable and constraint qualification holds

1. Stationarity:
   ‚àáf(x*) + Œ£·µ¢ Œª·µ¢*‚àág·µ¢(x*) + Œ£‚±º Œº‚±º*‚àáh‚±º(x*) = 0

2. Primal Feasibility:
   g·µ¢(x*) ‚â§ 0  for all i
   h‚±º(x*) = 0  for all j

3. Dual Feasibility:
   Œª·µ¢* ‚â• 0  for all i

4. Complementary Slackness:
   Œª·µ¢* √ó g·µ¢(x*) = 0  for all i
   (Either constraint is inactive (g·µ¢ < 0) or multiplier is positive (Œª·µ¢ > 0))

KKT Sufficient Conditions:
If f and g·µ¢ are convex, h‚±º are affine, and x* satisfies KKT conditions,
then x* is a global minimum!

Constraint Qualification:
Common conditions ensuring KKT conditions are necessary:
- Linear Independence Constraint Qualification (LICQ)
- Slater's Condition (for convex problems)
- Mangasarian-Fromovitz Constraint Qualification (MFCQ)

Example: Support Vector Machines (SVM)
Primal Problem:
minimize  (1/2)||w||¬≤ + C Œ£·µ¢ Œæ·µ¢
subject to:
  y·µ¢(w¬∑x·µ¢ + b) ‚â• 1 - Œæ·µ¢  for all i
  Œæ·µ¢ ‚â• 0  for all i

Dual Problem (via KKT):
maximize  Œ£·µ¢ Œ±·µ¢ - (1/2) Œ£·µ¢‚±º Œ±·µ¢Œ±‚±ºy·µ¢y‚±º(x·µ¢¬∑x‚±º)
subject to:
  Œ£·µ¢ Œ±·µ¢y·µ¢ = 0
  0 ‚â§ Œ±·µ¢ ‚â§ C  for all i

Complementary Slackness:
Œ±·µ¢[y·µ¢(w¬∑x·µ¢ + b) - 1 + Œæ·µ¢] = 0
- Œ±·µ¢ = 0 ‚Üí Non-support vector (correctly classified with margin)
- 0 < Œ±·µ¢ < C ‚Üí Support vector on margin (Œæ·µ¢ = 0)
- Œ±·µ¢ = C ‚Üí Support vector inside margin or misclassified (Œæ·µ¢ > 0)
```

**Subgradient Methods (Non-smooth Optimization)**:
```
Problem: Many ML loss functions are non-differentiable
- L1 norm: |x| not differentiable at x = 0
- Hinge loss: max(0, 1 - y¬∑f(x)) not differentiable at y¬∑f(x) = 1
- ReLU: max(0, x) not differentiable at x = 0

Subgradient:
For convex function f, g is a subgradient at x if:
f(y) ‚â• f(x) + g^T(y - x)  for all y

Subdifferential:
‚àÇf(x) = {all subgradients of f at x}

Properties:
1. If f is differentiable at x: ‚àÇf(x) = {‚àáf(x)} (singleton)
2. For L1 norm at 0: ‚àÇ|x| = [-1, 1]
3. For max(f, g): ‚àÇmax(f,g) ‚äÜ ‚àÇf ‚à™ ‚àÇg

Subgradient Descent:
x_{k+1} = x_k - Œ±_k √ó g_k

where g_k ‚àà ‚àÇf(x_k) is any subgradient

Convergence (for convex f):
With diminishing step sizes Œ±_k such that:
- Œ£ Œ±_k = ‚àû (step sizes sum to infinity)
- Œ£ Œ±_k¬≤ < ‚àû (squared step sizes converge)

Then: f(x_k) - f(x*) = O(1/‚àök)

Example: Œ±_k = c/‚àök satisfies both conditions

Note: Slower than gradient descent (O(1/‚àök) vs O(1/k))
But works for non-smooth functions!

Proximal Gradient Method:
For f(x) = g(x) + h(x) where g is smooth, h is non-smooth convex:

x_{k+1} = prox_{Œ±_k h}(x_k - Œ±_k‚àág(x_k))

Proximal operator:
prox_h(y) = argmin_x [h(x) + (1/2)||x - y||¬≤]

Example: Lasso regression
f(w) = ||Xw - y||¬≤ + Œª||w||‚ÇÅ
      ‚îî‚îÄ‚îÄg(w)‚îÄ‚îÄ‚îò   ‚îî‚îÄh(w)‚îÄ‚îò

Proximal operator of h(w) = Œª||w||‚ÇÅ:
prox_h(w)·µ¢ = sign(w·µ¢) √ó max(|w·µ¢| - Œª, 0)  (soft thresholding)

Convergence: O(1/k) like gradient descent!
```

**Practical Optimization Tips**:
```
1. Choose Right Algorithm:
   - Smooth unconstrained ‚Üí Gradient Descent / Adam
   - Non-smooth ‚Üí Subgradient / Proximal Gradient
   - Constrained ‚Üí Projected Gradient / Interior Point
   - Large scale ‚Üí Stochastic methods (SGD, Adam)

2. Hyperparameter Tuning:
   - Learning rate: Most important! Use grid search or learning rate schedules
   - Batch size: Larger ‚Üí more stable but slower, smaller ‚Üí noisy but faster
   - Momentum: 0.9 or 0.99 usually good defaults

3. Convergence Diagnostics:
   - Monitor: Loss, gradient norm, parameter changes
   - Plot: Loss vs iteration (should decrease monotonically for convex)
   - Check: KKT residuals for constrained problems

4. Numerical Stability:
   - Scale features to similar ranges
   - Use numerically stable formulations (log-sum-exp trick)
   - Add small epsilon to denominators (e.g., Adam: 1e-8)

5. Initialization:
   - Random initialization to break symmetry
   - Xavier/He initialization for neural networks
   - Warm start from previous solutions

6. Regularization:
   - L2 (Ridge): Improves conditioning, smooth optimization landscape
   - L1 (Lasso): Sparse solutions, use proximal gradient
   - Early stopping: Implicit regularization
```

---

## üì° Information Theory

Information theory quantifies information, essential for understanding entropy, KL divergence, and mutual information in ML.

### Entropy

**Shannon Entropy**: Measure of uncertainty/surprise
```
H(X) = -Œ£ P(x) log‚ÇÇ P(x)

Units: bits (if log base 2)

Interpretation: Average number of bits needed to encode X
```

**Example**:
```python
def entropy(probabilities):
    """
    Compute Shannon entropy

    H(X) = -Œ£ p(x) log‚ÇÇ p(x)
    """
    # Remove zeros (0 log 0 = 0 by definition)
    p = np.array(probabilities)
    p = p[p > 0]

    return -np.sum(p * np.log2(p))

# Fair coin: P(H) = P(T) = 0.5
H_fair = entropy([0.5, 0.5])
print(f"Entropy of fair coin: {H_fair:.3f} bits")  # 1.000 bit

# Biased coin: P(H) = 0.9, P(T) = 0.1
H_biased = entropy([0.9, 0.1])
print(f"Entropy of biased coin: {H_biased:.3f} bits")  # 0.469 bits
# Less entropy = more predictable!

# Uniform distribution (maximum entropy)
H_uniform = entropy([0.25, 0.25, 0.25, 0.25])
print(f"Entropy of uniform (4 outcomes): {H_uniform:.3f} bits")  # 2.000 bits

# Deterministic (minimum entropy)
H_deterministic = entropy([1.0])
print(f"Entropy of deterministic: {H_deterministic:.3f} bits")  # 0.000 bits
```

**Cross-Entropy** (ML loss function):
```python
def cross_entropy(p, q):
    """
    Cross-entropy H(p, q) = -Œ£ p(x) log q(x)

    p: true distribution
    q: predicted distribution

    Measures: How many bits needed to encode p using q
    """
    q = np.clip(q, 1e-10, 1.0)  # Numerical stability
    return -np.sum(p * np.log(q))

# Example: 3-class classification
true_distribution = np.array([1, 0, 0])  # Class 0
predicted_distribution = np.array([0.7, 0.2, 0.1])

CE = cross_entropy(true_distribution, predicted_distribution)
print(f"Cross-Entropy: {CE:.3f}")  # 0.357

# Perfect prediction
perfect_prediction = np.array([1.0, 0.0, 0.0])
CE_perfect = cross_entropy(true_distribution, perfect_prediction)
print(f"Cross-Entropy (perfect): {CE_perfect:.3f}")  # ‚âà 0.000
```

**KL Divergence** (Relative Entropy):
```python
def kl_divergence(p, q):
    """
    KL(p || q) = Œ£ p(x) log(p(x) / q(x))

    Measures: How different q is from p
    Properties:
    - KL(p || q) ‚â• 0
    - KL(p || q) = 0 iff p = q
    - Not symmetric: KL(p || q) ‚â† KL(q || p)
    """
    p = np.array(p)
    q = np.array(q)
    q = np.clip(q, 1e-10, 1.0)

    return np.sum(p * np.log(p / q))

# Example
p = [0.5, 0.3, 0.2]
q1 = [0.5, 0.3, 0.2]  # Same as p
q2 = [0.4, 0.4, 0.2]  # Different from p

print(f"KL(p || q1): {kl_divergence(p, q1):.4f}")  # 0.0000
print(f"KL(p || q2): {kl_divergence(p, q2):.4f}")  # 0.0085

# ML application: Variational inference
# Minimize KL(q || p) to approximate true posterior p with q
```

**Mutual Information**:
```python
# I(X; Y) = H(X) + H(Y) - H(X, Y)
# Measures: How much knowing Y reduces uncertainty about X

def mutual_information(X, Y):
    """
    Estimate mutual information between X and Y
    """
    from sklearn.metrics import mutual_info_score

    return mutual_info_score(X, Y)

# Example: Feature selection
# Select features with high mutual information with target

from sklearn.feature_selection import mutual_info_classif

X = np.random.rand(1000, 10)
y = (X[:, 0] + X[:, 1] > 1).astype(int)  # Target depends on features 0 and 1

mi_scores = mutual_info_classif(X, y)

print("Mutual information scores:")
for i, score in enumerate(mi_scores):
    print(f"  Feature {i}: {score:.4f}")
# Features 0 and 1 should have highest MI with target
```

---

## üß† Neural Network Initialization Theory

Proper weight initialization is critical for successful neural network training. Poor initialization can lead to vanishing/exploding gradients, slow convergence, or complete training failure.

### The Initialization Problem

**Why Initialization Matters:**
```
Problem: Neural networks are highly non-convex
- Different initializations ‚Üí different local minima
- Bad initialization ‚Üí vanishing/exploding gradients
- Good initialization ‚Üí faster convergence, better final performance

Key Insight: Initialize weights to preserve signal variance across layers
```

**Naive Approaches (Don't Do This!):**
```python
# ‚ùå All zeros: Symmetry problem
W = np.zeros((n_out, n_in))
# All neurons learn the same function!
# Gradient for all neurons is identical
# Network effectively has only one neuron per layer

# ‚ùå All same value: Same problem
W = np.ones((n_out, n_in)) * 0.5

# ‚ùå Too large values
W = np.random.randn(n_out, n_in) * 10
# Output variance explodes: Var(output) = (n_in √ó 10¬≤) √ó Var(input)
# Gradients explode

# ‚ùå Too small values
W = np.random.randn(n_out, n_in) * 0.001
# Output variance vanishes: Var(output) ‚âà 0
# Gradients vanish
```

### Xavier/Glorot Initialization (2010)

**Mathematical Foundation:**

**Goal:** Preserve variance of activations and gradients across layers

**Assumption:** Linear activation (or near-linear like tanh around 0)

**Forward Pass Analysis:**
```
Layer computation: y = W¬∑x + b

For one neuron: y_i = Œ£_{j=1}^{n_in} w_{ij} x_j

Assumptions:
1. x_j are i.i.d. with mean 0 and variance œÉ¬≤_x
2. w_{ij} are i.i.d. with mean 0 and variance œÉ¬≤_w
3. x and w are independent

Variance of output:
Var(y_i) = Var(Œ£_j w_{ij} x_j)
         = Œ£_j Var(w_{ij} x_j)           (independence)
         = Œ£_j E[w_{ij}¬≤] E[x_j¬≤]         (independence)
         = Œ£_j Var(w_{ij}) Var(x_j)       (mean 0)
         = n_in ¬∑ œÉ¬≤_w ¬∑ œÉ¬≤_x

To preserve variance (Var(y_i) = œÉ¬≤_x):
n_in ¬∑ œÉ¬≤_w = 1
œÉ¬≤_w = 1 / n_in
```

**Backward Pass Analysis:**
```
Gradient backprop: ‚àÇL/‚àÇx = W^T ¬∑ ‚àÇL/‚àÇy

By similar analysis:
Var(‚àÇL/‚àÇx_j) = n_out ¬∑ œÉ¬≤_w ¬∑ Var(‚àÇL/‚àÇy)

To preserve gradient variance:
n_out ¬∑ œÉ¬≤_w = 1
œÉ¬≤_w = 1 / n_out
```

**Xavier/Glorot Compromise:**
```
Problem: Forward wants œÉ¬≤_w = 1/n_in, backward wants œÉ¬≤_w = 1/n_out

Solution: Average them!
œÉ¬≤_w = 2 / (n_in + n_out)

Xavier Uniform:
W ~ U[-‚àö(6/(n_in + n_out)), ‚àö(6/(n_in + n_out))]

Xavier Normal:
W ~ N(0, 2/(n_in + n_out))

Note: U[-a, a] has variance a¬≤/3, so a = ‚àö(3¬∑2/(n_in+n_out)) = ‚àö(6/(n_in+n_out))
```

**Implementation:**
```python
def xavier_uniform(n_in, n_out):
    """
    Xavier/Glorot uniform initialization

    Used for: tanh, sigmoid activations
    """
    limit = np.sqrt(6.0 / (n_in + n_out))
    return np.random.uniform(-limit, limit, size=(n_out, n_in))

def xavier_normal(n_in, n_out):
    """
    Xavier/Glorot normal initialization

    Used for: tanh, sigmoid activations
    """
    std = np.sqrt(2.0 / (n_in + n_out))
    return np.random.randn(n_out, n_in) * std
```

### He Initialization (2015)

**Motivation:** Xavier assumes linear activation, but ReLU is non-linear!

**ReLU Analysis:**
```
ReLU(x) = max(0, x)

Property: Kills half the neurons (negative values ‚Üí 0)

Effect on variance:
- Input variance: œÉ¬≤
- After ReLU: œÉ¬≤/2 (approximately, for zero-mean input)

Derivation:
For x ~ N(0, œÉ¬≤):
E[ReLU(x)] = E[x | x > 0] ¬∑ P(x > 0) = (œÉ/‚àö(2œÄ)) ¬∑ 0.5

Var(ReLU(x)) = E[ReLU(x)¬≤] - E[ReLU(x)]¬≤
             = E[x¬≤ | x > 0] ¬∑ P(x > 0) - (œÉ/‚àö(2œÄ) ¬∑ 0.5)¬≤
             = œÉ¬≤/2 - small term
             ‚âà œÉ¬≤/2

So ReLU halves the variance!
```

**He Initialization:**
```
Forward pass with ReLU:
Var(y_i) = n_in ¬∑ œÉ¬≤_w ¬∑ œÉ¬≤_x / 2  (ReLU kills half)

To preserve variance (Var(y_i) = œÉ¬≤_x):
n_in ¬∑ œÉ¬≤_w / 2 = 1
œÉ¬≤_w = 2 / n_in

He Normal (most common):
W ~ N(0, 2/n_in)

He Uniform:
W ~ U[-‚àö(6/n_in), ‚àö(6/n_in)]
```

**Implementation:**
```python
def he_normal(n_in, n_out):
    """
    He initialization (Kaiming initialization)

    Used for: ReLU, Leaky ReLU, PReLU activations

    Reference: He et al., "Delving Deep into Rectifiers: Surpassing
    Human-Level Performance on ImageNet Classification", ICCV 2015
    """
    std = np.sqrt(2.0 / n_in)
    return np.random.randn(n_out, n_in) * std

def he_uniform(n_in, n_out):
    """He uniform initialization"""
    limit = np.sqrt(6.0 / n_in)
    return np.random.uniform(-limit, limit, size=(n_out, n_in))
```

### Comparison and Guidelines

**Initialization Summary:**
```
Activation       | Forward Preserve | Backward Preserve | Recommended
-----------------|------------------|-------------------|-------------
Linear/None      | Var = 1/n_in    | Var = 1/n_out    | Xavier
tanh             | Var = 1/n_in    | Var = 1/n_out    | Xavier
sigmoid          | Var = 1/n_in    | Var = 1/n_out    | Xavier
ReLU             | Var = 2/n_in    | Var = 2/n_out    | He
Leaky ReLU       | Var ‚âà 2/n_in    | Var ‚âà 2/n_out    | He
ELU              | Var ‚âà 1.5/n_in  | Var ‚âà 1.5/n_out  | He or Xavier
SELU             | Special         | Special          | LeCun*

*LeCun Normal: W ~ N(0, 1/n_in)
```

**Modern PyTorch/TensorFlow Defaults:**
```python
import torch.nn as nn

# Linear layer
nn.Linear(n_in, n_out)
# Default: Xavier uniform (Glorot)

# Conv2D layer
nn.Conv2d(in_channels, out_channels, kernel_size)
# Default: He (Kaiming) uniform for ReLU

# LSTM/GRU
nn.LSTM(input_size, hidden_size)
# Default: Xavier uniform (Glorot)
```

**Complete Initialization Example:**
```python
class NeuralNetwork:
    """Neural network with proper initialization"""

    def __init__(self, layers, activation='relu'):
        """
        Args:
            layers: [n_input, n_hidden1, n_hidden2, ..., n_output]
            activation: 'relu', 'tanh', 'sigmoid'
        """
        self.weights = []
        self.biases = []

        for i in range(len(layers) - 1):
            n_in, n_out = layers[i], layers[i+1]

            # Initialize weights
            if activation == 'relu':
                # He initialization
                W = np.random.randn(n_out, n_in) * np.sqrt(2.0 / n_in)
            elif activation in ['tanh', 'sigmoid']:
                # Xavier initialization
                W = np.random.randn(n_out, n_in) * np.sqrt(2.0 / (n_in + n_out))
            else:
                # Default: small random
                W = np.random.randn(n_out, n_in) * 0.01

            # Initialize biases to zero (common practice)
            b = np.zeros((n_out, 1))

            self.weights.append(W)
            self.biases.append(b)

    def forward(self, x, activation='relu'):
        """Forward pass with specified activation"""
        a = x
        for W, b in zip(self.weights[:-1], self.biases[:-1]):
            z = W @ a + b
            if activation == 'relu':
                a = np.maximum(0, z)
            elif activation == 'tanh':
                a = np.tanh(z)
            elif activation == 'sigmoid':
                a = 1 / (1 + np.exp(-z))

        # Output layer (no activation for regression, or apply softmax for classification)
        z = self.weights[-1] @ a + self.biases[-1]
        return z

# Example usage
model = NeuralNetwork([784, 512, 256, 10], activation='relu')
print(f"Layer 1 weights std: {model.weights[0].std():.4f}")
print(f"Expected std: {np.sqrt(2.0/784):.4f}")
```

### Advanced Initialization Strategies

**1. LSUV (Layer-Sequential Unit-Variance, 2016):**
```python
def lsuv_init(model, data_sample):
    """
    Initialize weights then adjust to unit variance

    1. Initialize with orthogonal matrices
    2. Forward pass with sample data
    3. Scale weights to make output variance = 1
    4. Repeat for each layer
    """
    x = data_sample

    for layer in model.layers:
        # Initialize with orthogonal matrix
        W = np.linalg.qr(np.random.randn(layer.n_out, layer.n_in))[0]
        layer.W = W

        # Forward pass
        z = layer.forward(x)

        # Adjust to unit variance
        std = z.std()
        layer.W = layer.W / std

        x = layer.activation(z)
```

**2. Fixup Initialization (2019):**
```
For very deep networks (ResNets):
- Initialize most layers with He/Xavier
- Scale residual branches by 1/‚àöL (L = depth)
- No batch normalization needed!
```

**3. Batch Normalization Alternative:**
```
Instead of careful initialization:
- Use Batch Normalization after each layer
- BN normalizes activations to mean=0, std=1
- Makes network more robust to initialization
- Trade-off: BN adds computation and complexity
```

### Theoretical Guarantees

**Theorem (He et al., 2015):**
```
For ReLU networks with He initialization:
- Forward signal does not vanish or explode
- Backward gradient does not vanish or explode
- Enables training of networks with 30+ layers

Mathematically:
E[||y^(l)||¬≤] = E[||x^(0)||¬≤]  (forward)
E[||‚àÇL/‚àÇx^(0)||¬≤] = E[||‚àÇL/‚àÇy^(L)||¬≤]  (backward)

where l = layer index, L = total layers
```

**Condition for Gradient Flow:**
```
For stable training:

Forward: œÉ¬≤_out = œÉ¬≤_in  (variance preservation)
Backward: œÉ¬≤_grad = constant across layers

This requires: œÉ¬≤_w = O(1/n_in)

Violation leads to:
- œÉ¬≤_w too large ‚Üí exploding gradients
- œÉ¬≤_w too small ‚Üí vanishing gradients
```

### Summary and Best Practices

**Quick Reference:**
```python
import torch.nn as nn

# For ReLU/Leaky ReLU (most common):
nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='relu')
nn.init.zeros_(layer.bias)

# For tanh/sigmoid:
nn.init.xavier_normal_(layer.weight)
nn.init.zeros_(layer.bias)

# For LSTM/GRU:
nn.init.orthogonal_(layer.weight_ih)
nn.init.orthogonal_(layer.weight_hh)
nn.init.zeros_(layer.bias)

# General tip: Biases usually initialized to zero
# Exception: LSTM forget gate bias can be initialized to 1
```

**Key Insights:**
```
1. **Never initialize all weights to same value** (breaks symmetry)

2. **Match initialization to activation:**
   - ReLU family ‚Üí He initialization
   - tanh/sigmoid ‚Üí Xavier initialization

3. **Consider network depth:**
   - Very deep networks (>50 layers): Use Fixup or normalization layers
   - Moderate depth (10-30): He/Xavier sufficient

4. **Empirical tuning:**
   - Monitor activation/gradient statistics during training
   - Activation std should stay ‚âà1 across layers
   - Gradient norm should not explode or vanish

5. **Modern best practice:**
   - Use He/Xavier + Batch/Layer Normalization
   - This combination is very robust
```

---

## üîë Key Takeaways

**For Linear Algebra**:
- Vectors and matrices represent data and transformations
- Matrix multiplication is the core operation in neural networks
- Eigenvalues/eigenvectors are central to PCA and spectral methods
- SVD is a swiss army knife for dimensionality reduction and matrix factorization

**For Calculus**:
- Derivatives measure rates of change
- Gradients point in direction of steepest increase
- Chain rule enables backpropagation
- Optimization is gradient descent on steroids

**For Probability**:
- Bayes' theorem is fundamental for inference
- MLE connects optimization to statistics
- Normal distribution appears everywhere
- Conditional probability models dependencies

**For Statistics**:
- Descriptive statistics summarize data
- Hypothesis testing quantifies uncertainty
- Correlation ‚â† causation
- Always visualize before analyzing

**For Optimization**:
- Convex problems have unique global minima
- Neural networks are non-convex (local minima exist)
- Second-order methods converge faster but are expensive
- Constrained optimization uses Lagrange multipliers

**For Information Theory**:
- Entropy quantifies uncertainty
- Cross-entropy is the ML loss function
- KL divergence measures distribution difference
- Mutual information measures dependence

---

## üìö Further Reading

**Books**:
1. **Deisenroth, M. P., Faisal, A. A., & Ong, C. S.** (2020). *Mathematics for Machine Learning*. Cambridge University Press. Available free at: https://mml-book.github.io/
2. **Goodfellow, I., Bengio, Y., & Courville, A.** (2016). *Deep Learning*. MIT Press. Available at: https://www.deeplearningbook.org/
3. **Bishop, C. M.** (2006). *Pattern Recognition and Machine Learning*. Springer.
4. **Hastie, T., Tibshirani, R., & Friedman, J.** (2009). *The Elements of Statistical Learning* (2nd ed.). Springer. Available free at: https://hastie.su.stanford.edu/ElemStatLearn/
5. **Strang, G.** (2016). *Introduction to Linear Algebra* (5th ed.). Wellesley-Cambridge Press.
6. **Boyd, S., & Vandenberghe, L.** (2004). *Convex Optimization*. Cambridge University Press. Available free at: https://web.stanford.edu/~boyd/cvxbook/

**Seminal Papers**:
1. **Rumelhart, D. E., Hinton, G. E., & Williams, R. J.** (1986). "Learning representations by back-propagating errors." *Nature*, 323(6088), 533-536.
2. **Robbins, H., & Monro, S.** (1951). "A stochastic approximation method." *The Annals of Mathematical Statistics*, 22(3), 400-407.
3. **Kingma, D. P., & Ba, J.** (2015). "Adam: A method for stochastic optimization." *ICLR 2015*. arXiv:1412.6980

**Online Resources**:
- **3Blue1Brown** (YouTube): Visual explanations of linear algebra and calculus
  - "Essence of Linear Algebra" series
  - "Essence of Calculus" series
- **Khan Academy**: Foundations of probability and statistics
- **MIT OpenCourseWare**:
  - 18.06 Linear Algebra (Gilbert Strang)
  - 18.01 Single Variable Calculus
- **Stanford CS229**: Machine Learning (Andrew Ng)
  - Linear Algebra and Calculus review notes

**Practice**:
- Implement algorithms from scratch using only NumPy
- Derive gradients by hand before using automatic differentiation
- Visualize mathematical concepts with matplotlib
- Solve problems on Project Euler and Brilliant.org
- Work through exercises in the books listed above

---

## üìñ References

Key concepts and formulations in this guide are based on:

- **Linear Algebra**: Strang (2016), Deisenroth et al. (2020, Chapter 2)
- **Calculus & Optimization**: Boyd & Vandenberghe (2004), Goodfellow et al. (2016, Chapter 4)
- **Probability Theory**: Bishop (2006, Chapter 1-2), Deisenroth et al. (2020, Chapter 6)
- **Statistics**: Hastie et al. (2009, Chapter 2), James et al. (2021)
- **Information Theory**: Cover & Thomas (2006), Goodfellow et al. (2016, Chapter 3)
- **Backpropagation**: Rumelhart et al. (1986), Goodfellow et al. (2016, Chapter 6)
- **Gradient Descent**: Robbins & Monro (1951), Bottou (2010)

---

*Master these mathematical foundations and you'll have deep understanding of how and why ML algorithms work!*
