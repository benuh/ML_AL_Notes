# Loss Functions: Complete Mathematical Guide

## Table of Contents
1. [Introduction to Loss Functions](#introduction)
2. [Regression Loss Functions](#regression)
3. [Classification Loss Functions](#classification)
4. [Probabilistic Loss Functions](#probabilistic)
5. [Ranking and Metric Learning](#ranking)
6. [Sequence-to-Sequence Losses](#sequence)
7. [Generative Model Losses](#generative)
8. [Convergence Properties](#convergence)
9. [Loss Function Selection Guide](#selection)
10. [Custom Loss Functions](#custom)

---

## Introduction to Loss Functions

### Mathematical Framework

**Loss Function Definition:**
```
Loss function L: Y Ã— Y â†’ â„âº

L(y_true, y_pred): Measures discrepancy between prediction and truth

Properties:
1. Non-negativity: L(y, Å·) â‰¥ 0
2. Identity: L(y, y) = 0
3. Often (not always) symmetric or convex
```

**Risk Minimization:**
```
Empirical Risk: RÌ‚(f) = (1/n) Î£áµ¢ L(yáµ¢, f(xáµ¢))
True Risk: R(f) = E_{(x,y)~P}[L(y, f(x))]

Goal: min_{fâˆˆF} R(f)
Practice: min_{fâˆˆF} RÌ‚(f) + Î»Â·Î©(f)

where Î©(f) is regularization
```

**Gradient-Based Optimization:**
```
For parameter Î¸:

âˆ‚L/âˆ‚Î¸: Gradient of loss w.r.t. parameters

Update: Î¸ â† Î¸ - Î±Â·âˆ‚L/âˆ‚Î¸

Convergence depends on:
- Convexity of L
- Lipschitz properties
- Smoothness
```

### Statistical Decision Theory

**Bayes Risk and Optimal Decision Rules**

The foundation of loss functions lies in statistical decision theory.

**Definition 1 (Decision Function):**
A decision function Î´: X â†’ A maps inputs to actions.
- For regression: A = â„ (predict real values)
- For binary classification: A = {0, 1} or A = [0, 1] (probabilities)
- For multi-class: A = Î”_K (probability simplex)

**Definition 2 (Risk):**
The risk of decision function Î´ under loss L:

R(Î´) = E_{(X,Y)~P}[L(Y, Î´(X))]

**Theorem 1 (Bayes Optimal Decision Rule):**
The Bayes optimal decision rule minimizes expected loss:

Î´*(x) = argmin_{aâˆˆA} E_{Y|X=x}[L(Y, a)]

This is the best possible decision rule given the true distribution P(Y|X).

**Proof:**
By iterated expectation:

R(Î´) = E_X[E_{Y|X}[L(Y, Î´(X))]]

For each x, E_{Y|X=x}[L(Y, Î´(x))] depends only on Î´(x).
Minimizing R(Î´) requires minimizing the inner expectation for each x:

Î´*(x) = argmin_{aâˆˆA} E_{Y|X=x}[L(Y, a)]

Any other choice Î´(x) â‰  Î´*(x) increases R(Î´). âˆ

**Corollary 1.1 (Bayes Risk):**
The Bayes risk R* is the minimum achievable risk:

R* = E_X[min_{aâˆˆA} E_{Y|X}[L(Y, a)]]

No decision rule can achieve R(Î´) < R*.

**Excess Risk Decomposition:**
For any decision rule Î´:

R(Î´) - R* = E_X[E_{Y|X}[L(Y, Î´(X))] - min_a E_{Y|X}[L(Y, a)]]
          â‰¥ 0

The excess risk measures sub-optimality of Î´.

**Example 1 (Squared Loss):**
For L(y, Å·) = (y - Å·)Â²:

Î´*(x) = argmin_a E_{Y|X=x}[(Y - a)Â²]
      = argmin_a E[(Y - E[Y|X=x] + E[Y|X=x] - a)Â²]
      = argmin_a {Var[Y|X=x] + (E[Y|X=x] - a)Â²}
      = E[Y|X=x]

Therefore: **MSE loss leads to predicting the conditional mean**.

**Example 2 (Absolute Loss):**
For L(y, Å·) = |y - Å·|:

Î´*(x) = argmin_a E_{Y|X=x}[|Y - a|]
      = median(Y|X=x)

Therefore: **MAE loss leads to predicting the conditional median**.

**Example 3 (0-1 Loss for Classification):**
For L(y, Å·) = ğŸ™[y â‰  Å·]:

Î´*(x) = argmin_{câˆˆ{1,...,K}} P(Y â‰  c | X=x)
      = argmin_c (1 - P(Y = c | X=x))
      = argmax_c P(Y = c | X=x)

Therefore: **0-1 loss leads to predicting the most probable class**.

### Convexity Theory for Loss Functions

**Definition 3 (Convex Function):**
A function f: â„â¿ â†’ â„ is convex if for all x, y âˆˆ â„â¿ and Î» âˆˆ [0, 1]:

f(Î»x + (1-Î»)y) â‰¤ Î»f(x) + (1-Î»)f(y)

f is strictly convex if inequality is strict for Î» âˆˆ (0, 1) and x â‰  y.

**Definition 4 (Strong Convexity):**
f is Î¼-strongly convex if for all x, y:

f(y) â‰¥ f(x) + âˆ‡f(x)áµ€(y - x) + (Î¼/2)||y - x||Â²

Strong convexity implies unique global minimum.

**Theorem 2 (Convexity of Common Losses):**

(a) **MSE is convex:**
For L(w) = (1/n)Î£áµ¢(wáµ€xáµ¢ - yáµ¢)Â², L is convex in w.

Proof: The Hessian is:
âˆ‡Â²L(w) = (2/n)Xáµ€X

This is positive semidefinite since for any v:
váµ€âˆ‡Â²L(w)v = (2/n)váµ€Xáµ€Xv = (2/n)||Xv||Â² â‰¥ 0

If X has full column rank: âˆ‡Â²L = (2/n)Xáµ€X â‰» 0, so L is Î¼-strongly convex
with Î¼ = (2/n)Î»_min(Xáµ€X). âˆ

(b) **Cross-entropy is convex:**
For logistic regression with L(w) = -Î£áµ¢[yáµ¢log Ïƒ(wáµ€xáµ¢) + (1-yáµ¢)log(1-Ïƒ(wáµ€xáµ¢))],
where Ïƒ(z) = 1/(1+e^(-z)):

Proof: The Hessian is:
âˆ‡Â²L(w) = Î£áµ¢ Ïƒ(wáµ€xáµ¢)(1-Ïƒ(wáµ€xáµ¢)) xáµ¢xáµ¢áµ€

Since Ïƒ(z)(1-Ïƒ(z)) > 0 for all z, this is a sum of positive semidefinite matrices,
hence positive semidefinite. Therefore L is convex. âˆ

(c) **MAE is convex:**
For L(w) = Î£áµ¢|wáµ€xáµ¢ - yáµ¢|, L is convex (as sum of convex functions |Â·|).

(d) **Hinge loss is convex:**
For L(w) = Î£áµ¢max(0, 1 - yáµ¢wáµ€xáµ¢), L is convex (max of affine functions).

**Theorem 3 (Smoothness and Lipschitz Continuity):**

(a) **L-smooth:** A differentiable function f is L-smooth if:
||âˆ‡f(x) - âˆ‡f(y)|| â‰¤ L||x - y|| for all x, y

Equivalently: âˆ‡Â²f(x) âª¯ LI (all eigenvalues â‰¤ L)

(b) **For MSE:** If L(w) = (1/n)||Xw - y||Â², then:
âˆ‡Â²L = (2/n)Xáµ€X

L-smooth with L = (2/n)Î»_max(Xáµ€X).

(c) **For logistic regression:** L-smooth with L = (1/4n)||X||Â²_F
because max_z Ïƒ(z)(1-Ïƒ(z)) = 1/4.

**Theorem 4 (Convergence Rates):**

For Î¼-strongly convex and L-smooth function, gradient descent with step size Î± = 1/L:

||w_k - w*||Â² â‰¤ (1 - Î¼/L)^k ||w_0 - w*||Â²

where Îº = L/Î¼ is the condition number.

Proof: By L-smoothness and Î¼-strong convexity:

f(w_k) - f(w*) â‰¤ (1 - 1/Îº)(f(w_{k-1}) - f(w*))

This is linear (exponential) convergence with rate depending on Îº. âˆ

**Practical Implications:**
- Well-conditioned problems (Îº â‰ˆ 1): fast convergence
- Ill-conditioned problems (Îº >> 1): slow convergence
- For linear regression: Îº = Î»_max(Xáµ€X)/Î»_min(Xáµ€X)
- Regularization improves conditioning: Îº decreases with Î» in L + Î»||w||Â²

### Bregman Divergences

Many loss functions are Bregman divergences, providing a unified framework.

**Definition 5 (Bregman Divergence):**
For strictly convex, differentiable function Ï†:

D_Ï†(y || Å·) = Ï†(y) - Ï†(Å·) - âŸ¨âˆ‡Ï†(Å·), y - Å·âŸ©

This measures the "error" of approximating Ï†(y) by its first-order Taylor expansion at Å·.

**Properties:**
1. D_Ï†(y || Å·) â‰¥ 0 with equality iff y = Å·
2. Generally not symmetric: D_Ï†(y || Å·) â‰  D_Ï†(Å· || y)
3. Convex in second argument

**Theorem 5 (Common Losses as Bregman Divergences):**

(a) **Squared loss:** Ï†(y) = (1/2)yÂ²
D_Ï†(y || Å·) = (1/2)yÂ² - (1/2)Å·Â² - Å·(y - Å·)
            = (1/2)(y - Å·)Â²

(b) **Generalized KL divergence:** Ï†(y) = y log y - y
D_Ï†(y || Å·) = y log(y/Å·) - (y - Å·)  (for y, Å· > 0)

(c) **Itakura-Saito divergence:** Ï†(y) = -log y
D_Ï†(y || Å·) = y/Å· - log(y/Å·) - 1

**Theorem 6 (Bregman Projection):**
The Bregman projection onto convex set C:

P_C(y) = argmin_{Å·âˆˆC} D_Ï†(y || Å·)

satisfies the generalized Pythagorean theorem:

D_Ï†(y || z) = D_Ï†(y || P_C(y)) + D_Ï†(P_C(y) || z)  for all z âˆˆ C

This generalizes ordinary Euclidean projection (when Ï†(y) = (1/2)||y||Â²).

---

## Regression Loss Functions

### Mean Squared Error (MSE) / L2 Loss

**Mathematical Form:**
```
L(y, Å·) = (1/n) Î£áµ¢ (yáµ¢ - Å·áµ¢)Â²

Alternative forms:
- Sum of squared errors: Î£áµ¢ (yáµ¢ - Å·áµ¢)Â²
- Root MSE: âˆš[(1/n) Î£áµ¢ (yáµ¢ - Å·áµ¢)Â²]
```

**Gradient:**
```
âˆ‚L/âˆ‚Å·áµ¢ = 2(Å·áµ¢ - yáµ¢)

Properties:
- Linear gradient in error
- Differentiable everywhere
- Convex
```

**Statistical Interpretation:**
```
Maximum Likelihood Estimation under Gaussian noise:

Assume: y = f(x) + Îµ, where Îµ ~ N(0, ÏƒÂ²)

Likelihood: P(y|x) = (1/âˆš(2Ï€ÏƒÂ²)) exp(-(y - f(x))Â²/(2ÏƒÂ²))

Log-likelihood: log P(y|x) = -(y - f(x))Â²/(2ÏƒÂ²) + const

Maximizing log-likelihood âŸº Minimizing MSE!
```

**Convergence Properties:**
```
For linear regression y = Xw:

Loss: L(w) = ||Xw - y||Â²

Properties:
- Strongly convex (for X with full column rank)
- Unique global minimum: w* = (X^T X)^(-1) X^T y
- Gradient Lipschitz with constant L = Î»_max(X^T X)

Convergence rate (Gradient Descent):
- With step size Î± = 1/L:
  ||w_k - w*||Â² â‰¤ (1 - Î¼/L)^k ||w_0 - w*||Â²

  where Î¼ = Î»_min(X^T X) (strong convexity parameter)

- Condition number: Îº = L/Î¼ = Î»_max/Î»_min
- Linear convergence: O((1 - 1/Îº)^k)
```

**Robustness:**
```
Sensitivity to outliers: HIGH

Example:
True values: [1, 2, 3, 4, 5]
Predictions: [1.1, 2.1, 3.1, 4.1, 5.1]
MSE = 0.01 (good)

With one outlier:
True values: [1, 2, 3, 4, 100]
Predictions: [1.1, 2.1, 3.1, 4.1, 5.1]
MSE = 1801.01 (terrible!)

Outlier dominates loss due to squaring
```

**Use Cases:**
```
âœ“ When: Errors are normally distributed
âœ“ When: Outliers are rare and should be heavily penalized
âœ“ When: Need smooth gradients
âœ— When: Data has outliers (use Huber or MAE instead)
```

**MSE vs MAE: Decision guide**

**Choose MSE when:**
1. **Gaussian noise assumption holds:** Real-world data often has Gaussian errors (measurement noise, sensor noise)
2. **Large errors are qualitatively different:** Predicting $100K vs $50K for house price is worse than $51K vs $50K
3. **Want smooth optimization:** MSE gradients are continuous and smooth everywhere
4. **Training stability matters:** Constant-magnitude gradients of MAE can cause issues with learning rate

**Choose MAE when:**
1. **Outliers exist and are NOT errors:** Legitimate extreme values that shouldn't dominate loss
2. **Want median prediction:** MSE â†’ mean, MAE â†’ median (median more robust)
3. **All errors equally bad:** Missing by $10K or $100K both count as one failed prediction
4. **Interpretable loss:** MAE in same units as target (average error)

**Practical example (house prices):**
- Dataset: 100 houses, 99 priced $200K-$300K, 1 mansion at $5M
- Model predicting $250K for everything:
  - MSE: Dominated by $5M outlier, loss â‰ˆ 22MÂ², pushes model to overpredict
  - MAE: All errors weighted equally, loss â‰ˆ $50K average, robust to outlier
- If mansion is legitimate (not error): Use MAE
- If mansion is data entry error: Use MSE or remove outlier

### Mean Absolute Error (MAE) / L1 Loss

**Mathematical Form:**
```
L(y, Å·) = (1/n) Î£áµ¢ |yáµ¢ - Å·áµ¢|
```

**Gradient:**
```
âˆ‚L/âˆ‚Å·áµ¢ = sign(Å·áµ¢ - yáµ¢)

Properties:
- Constant gradient magnitude
- Not differentiable at Å·áµ¢ = yáµ¢ (use subgradient)
- Convex
```

**Statistical Interpretation:**
```
MLE under Laplace distribution:

Assume: y = f(x) + Îµ, where Îµ ~ Laplace(0, b)

P(Îµ) = (1/2b) exp(-|Îµ|/b)

Log-likelihood: -|y - f(x)|/b + const

Maximizing âŸº Minimizing MAE
```

**Convergence Properties:**
```
For linear model:

L(w) = Î£áµ¢ |xáµ¢^T w - yáµ¢|

Properties:
- Convex but not differentiable at solution points
- Subgradient method required
- No closed-form solution (unlike MSE)

Convergence rate (Subgradient Descent):
- Best iterate after k steps:
  f(w_best) - f(w*) â‰¤ O(1/âˆšk)

- Slower than smooth losses
- Step size: Î±_k = Î±_0/âˆšk (diminishing)

Coordinate descent:
- Often more efficient for L1
- Soft-thresholding operator
```

**Robustness:**
```
Sensitivity to outliers: LOW

Example with outlier:
True values: [1, 2, 3, 4, 100]
Predictions: [1.1, 2.1, 3.1, 4.1, 5.1]

MAE = (0.1 + 0.1 + 0.1 + 0.1 + 94.9)/5 = 19.06

vs MSE = 1801.01

MAE less affected by outlier (no squaring)
```

**Use Cases:**
```
âœ“ When: Outliers present in data
âœ“ When: Median regression desired (MAE â†’ median, MSE â†’ mean)
âœ“ When: All errors equally important
âœ— When: Need smooth gradients (use Huber instead)
âœ— When: Want to heavily penalize large errors
```

### Huber Loss

**Mathematical Form:**
```
L_Î´(y, Å·) = {
  Â½(y - Å·)Â²               if |y - Å·| â‰¤ Î´
  Î´|y - Å·| - Â½Î´Â²          if |y - Å·| > Î´
}

Combines MSE (for small errors) and MAE (for large errors)
```

**Gradient:**
```
âˆ‚L_Î´/âˆ‚Å· = {
  (Å· - y)           if |y - Å·| â‰¤ Î´
  Î´Â·sign(Å· - y)     if |y - Å·| > Î´
}

Properties:
- Differentiable everywhere (unlike MAE)
- Linear gradient for large errors (unlike MSE)
- Convex
- Î´: transition point (hyperparameter)
```

**Convergence Properties:**
```
Properties:
- Strongly convex in neighborhood of minimum
- Lipschitz continuous gradient
- Smooth everywhere

Convergence rate:
- In convex region: Linear convergence (like MSE)
- Overall: Better than MAE, similar to MSE
- Less sensitive to Î´ choice than might be expected

Optimal Î´:
- Î´ â‰ˆ 1.345Ïƒ where Ïƒ is noise standard deviation
- Makes Huber ~95% as efficient as MSE under Gaussian noise
- Much more robust to outliers
```

**Robustness:**
```
Sensitivity: MEDIUM (between MSE and MAE)

Î´ = 1.0 example:
Small errors (|e| < 1): L = 0.5eÂ² (smooth, like MSE)
Large errors (|e| > 1): L = |e| - 0.5 (robust, like MAE)

Best of both worlds:
- Smooth gradients near optimum
- Robust to outliers
```

**Use Cases:**
```
âœ“ When: Data has some outliers but want smooth training
âœ“ When: Need balance between MSE and MAE
âœ“ When: Regression with heteroscedastic noise
âœ“ Most versatile regression loss
```

### Quantile Loss / Pinball Loss

**Mathematical Form:**
```
L_Ï„(y, Å·) = {
  Ï„(y - Å·)        if y â‰¥ Å·
  (1-Ï„)(Å· - y)    if y < Å·
}

where Ï„ âˆˆ (0, 1) is the quantile

Special cases:
- Ï„ = 0.5: MAE (median)
- Ï„ = 0.9: 90th percentile
```

**Gradient:**
```
âˆ‚L_Ï„/âˆ‚Å· = {
  -Ï„        if y â‰¥ Å·
  (1-Ï„)     if y < Å·
}

Asymmetric penalties:
- Underestimation (y > Å·): weighted by Ï„
- Overestimation (y < Å·): weighted by (1-Ï„)
```

**Statistical Interpretation:**
```
Minimizing L_Ï„(y, Å·) gives Ï„-th quantile of P(y|x)

Example (Ï„ = 0.9):
- Prediction is 90th percentile
- 90% of observed values below prediction
- Useful for risk assessment, inventory planning
```

#### Rigorous Quantile Regression Theory

**Theorem 15 (Check Function Characterization - Koenker & Bassett, 1978):**
The quantile loss is also called the **check function** Ï_Ï„:

Ï_Ï„(u) = u(Ï„ - ğŸ™[u < 0])
       = {  Ï„Â·u      if u â‰¥ 0
         { (Ï„-1)Â·u   if u < 0

The Ï„-th conditional quantile q_Ï„(x) minimizes expected check loss:

q_Ï„(x) = argmin_q E_{Y|X=x}[Ï_Ï„(Y - q)]

**Proof:**
Taking the derivative w.r.t. q:

âˆ‚/âˆ‚q E[Ï_Ï„(Y - q)] = E[âˆ‚Ï_Ï„(Y - q)/âˆ‚q]
                     = E[-Ï„Â·ğŸ™[Y â‰¥ q] + (1-Ï„)Â·ğŸ™[Y < q]]
                     = (1-Ï„)P(Y < q) - Ï„Â·P(Y â‰¥ q)
                     = P(Y < q) - Ï„

Setting to zero:
P(Y < q_Ï„) = Ï„

Therefore: q_Ï„ = F_Y^(-1)(Ï„) (the Ï„-th quantile). âˆ

**Example (Ï„ = 0.5):**
Check function: Ï_{0.5}(u) = |u|/2 (proportional to MAE)
Optimal: q_{0.5} = median(Y|X)

**Theorem 16 (Equivariance Properties):**
Quantile regression has important equivariance properties:

(a) **Location equivariance:** If Q_Ï„(Y|X) = q_Ï„(X), then:
Q_Ï„(Y + c|X) = q_Ï„(X) + c

(b) **Scale equivariance:** If Q_Ï„(Y|X) = q_Ï„(X), then:
Q_Ï„(cÂ·Y|X) = cÂ·q_Ï„(X)  for c > 0

(c) **Monotone transformation:** For strictly monotone h:
Q_Ï„(h(Y)|X) = h(Q_Ï„(Y|X))

**Proof of (a):**
For u = Y - q:
E[Ï_Ï„((Y+c) - (q+c))] = E[Ï_Ï„(Y - q)]

Minimizer shifts by c. âˆ

**Theorem 17 (Asymptotic Normality - Koenker & Bassett, 1978):**
For linear quantile regression with n observations:

âˆšn(Î²Ì‚_Ï„ - Î²_Ï„) â†’_d N(0, Î£_Ï„)

where:
Î£_Ï„ = Ï„(1-Ï„)(X^T X)^(-1) / f_Îµ(0)Â²

and f_Îµ(0) is the density of errors at 0.

**Interpretation:**
- Variance proportional to Ï„(1-Ï„): maximum at median (Ï„ = 0.5)
- Depends on error density at 0: sparsity degrades precision
- Requires estimating f_Îµ(0) for inference (density estimation)

**Quantile regression vs OLS variance:**
For Gaussian errors with variance ÏƒÂ²:
- OLS variance: ÏƒÂ²(X^T X)^(-1)
- QR variance at median: (Ï€ÏƒÂ²/2)(X^T X)^(-1)
- Efficiency ratio: 2/Ï€ â‰ˆ 0.637

Median regression ~64% as efficient as mean regression for Gaussian data.

**Theorem 18 (Quantile Crossing Prevention):**
When estimating multiple quantiles Ï„â‚ < Ï„â‚‚ < ... < Ï„_k, monotonicity requires:

q_Ï„â‚(x) â‰¤ q_Ï„â‚‚(x) â‰¤ ... â‰¤ q_Ï„_k(x)  for all x

**Non-crossing constraint:** Can be enforced via:

(a) **Post-processing:** Isotonic regression on estimated quantiles
(b) **Joint estimation:** Add constraints:
    q_Ï„áµ¢(x) â‰¤ q_Ï„â±¼(x) - Îµ  for i < j
(c) **Parametric form:** Use non-crossing basis (e.g., increasing neural network)

**Practical algorithm (Rearrangement - Chernozhukov et al., 2010):**
1. Estimate quantiles independently: qÌ‚_Ï„â‚, ..., qÌ‚_Ï„_k
2. For each x, sort: qÌƒ_Ï„â‚(x) â‰¤ qÌƒ_Ï„â‚‚(x) â‰¤ ... â‰¤ qÌƒ_Ï„_k(x)
3. Use sorted quantiles

Rearrangement preserves asymptotic properties while ensuring monotonicity.

**Theorem 19 (Sample Complexity for Quantile Estimation):**
To estimate Ï„-th quantile with error Îµ and confidence 1-Î´:

n = Î©((1/(ÎµÂ²Â·Ï„(1-Ï„)))Â·log(1/Î´))

**Proof sketch:**
By Hoeffding's inequality for empirical quantile:
P(|FÌ‚_n^(-1)(Ï„) - F^(-1)(Ï„)| > Îµ) â‰¤ 2exp(-2nÏ„(1-Ï„)ÎµÂ²)

Setting RHS = Î´ and solving for n. âˆ

**Sample complexity comparison:**
- Median (Ï„ = 0.5): n = Î©(4log(1/Î´)/ÎµÂ²) - most efficient
- Extreme quantile (Ï„ = 0.9): n = Î©(11.1Â·log(1/Î´)/ÎµÂ²) - less efficient
- Very extreme (Ï„ = 0.99): n = Î©(101Â·log(1/Î´)/ÎµÂ²) - requires much more data

**Example:**
For Îµ = 0.01 accuracy with Î´ = 0.05 confidence:
- Median: n â‰ˆ 1,200 samples
- 90th percentile: n â‰ˆ 3,300 samples
- 99th percentile: n â‰ˆ 30,300 samples

Estimating extreme quantiles requires significantly more data!

**Use Cases:**
```
âœ“ Quantile regression
âœ“ Confidence interval estimation
âœ“ Risk-sensitive prediction
âœ“ Asymmetric cost functions
```

### Log-Cosh Loss

**Mathematical Form:**
```
L(y, Å·) = Î£áµ¢ log(cosh(Å·áµ¢ - yáµ¢))

where cosh(x) = (e^x + e^(-x))/2
```

**Gradient:**
```
âˆ‚L/âˆ‚Å·áµ¢ = tanh(Å·áµ¢ - yáµ¢)

Properties:
- Smooth everywhere (unlike Huber)
- Approximately MSE for small errors
- Approximately MAE for large errors
- No hyperparameter (unlike Huber's Î´)
```

**Approximations:**
```
For small |x|: log(cosh(x)) â‰ˆ xÂ²/2 (like MSE)
For large |x|: log(cosh(x)) â‰ˆ |x| - log 2 (like MAE)

Automatic transition between quadratic and linear
```

**Use Cases:**
```
âœ“ When: Want Huber-like behavior without tuning Î´
âœ“ When: Need smoothness everywhere
âœ“ When: Outlier robustness with simple implementation
```

---

## Classification Loss Functions

### Proper Scoring Rules and Calibration

Before discussing specific classification losses, we establish the theory of proper scoring rules.

**Definition 6 (Scoring Rule):**
A scoring rule S(P, y) measures the quality of probabilistic forecast P when outcome is y.
- P: predicted probability distribution over outcomes
- y: realized outcome
- Lower score = better prediction

**Definition 7 (Proper Scoring Rule):**
A scoring rule S is **proper** if the expected score is minimized by predicting the true distribution:

argmin_Q E_{Y~P}[S(Q, Y)] = P

It is **strictly proper** if P is the unique minimizer.

**Theorem 7 (Cross-Entropy is Proper):**
The logarithmic scoring rule S(P, y) = -log P(y) is strictly proper.

Proof:
Let P be the true distribution. For any other distribution Q:

E_{Y~P}[S(Q, Y)] = E_{Y~P}[-log Q(Y)]
                  = -Î£_y P(y) log Q(y)
                  = H(P, Q)  (cross-entropy)

We want to show: H(P, Q) â‰¥ H(P, P) with equality iff Q = P.

H(P, Q) - H(P, P) = -Î£_y P(y) log Q(y) + Î£_y P(y) log P(y)
                  = Î£_y P(y) log(P(y)/Q(y))
                  = KL(P || Q)
                  â‰¥ 0

with equality iff P = Q (by non-negativity of KL divergence). âˆ

**Corollary 7.1:**
Minimizing cross-entropy loss encourages calibrated probability predictions.

**Theorem 8 (Brier Score is Proper):**
The Brier score S(p, y) = (p - y)Â² for binary outcomes y âˆˆ {0, 1} is strictly proper.

Proof:
For true probability P(Y=1) = Ï€, and predicted probability p:

E_Y[S(p, Y)] = Ï€(p - 1)Â² + (1-Ï€)(p - 0)Â²
              = Ï€(pÂ² - 2p + 1) + (1-Ï€)pÂ²
              = pÂ²(Ï€ + 1 - Ï€) - 2Ï€p + Ï€
              = pÂ² - 2Ï€p + Ï€

Taking derivative w.r.t. p:
âˆ‚/âˆ‚p E[S(p, Y)] = 2p - 2Ï€

Setting to zero: p = Ï€ (the true probability).

Second derivative: âˆ‚Â²/âˆ‚pÂ² E[S(p, Y)] = 2 > 0, confirming minimum. âˆ

**Theorem 9 (Characterization of Proper Scoring Rules):**
A scoring rule S(p, y) for binary outcomes is proper if and only if it can be written as:

S(p, 1) = -G(p)
S(p, 0) = -G(1-p) - pÂ·G'(p)

where G is a strictly convex function with G' being its derivative.

This gives a general family of proper scoring rules based on choice of G.

**Examples:**
- Log score: G(p) = log p (strictly convex for p > 0)
- Brier score: G(p) = 2p - pÂ² (strictly convex)
- Spherical score: G(p) = 1/âˆš(pÂ² + (1-p)Â²)

### Calibration Theory

**Definition 8 (Calibration):**
A classifier with predicted probabilities p is **calibrated** if:

P(Y = 1 | p(X) = q) = q  for all q âˆˆ [0, 1]

In words: among all predictions with confidence q, exactly fraction q should be correct.

**Example (Well-calibrated):**
- 100 predictions with p = 0.7
- Exactly 70 should have Y = 1
- If 70/100 are correct â†’ well calibrated
- If 50/100 are correct â†’ underconfident (miscalibrated)
- If 90/100 are correct â†’ overconfident (miscalibrated)

**Theorem 10 (Expected Calibration Error):**
The Expected Calibration Error (ECE) measures calibration:

ECE = Î£_{m=1}^M (n_m/n)|acc(m) - conf(m)|

where:
- Predictions binned into M bins by confidence
- n_m: number of predictions in bin m
- acc(m): accuracy in bin m
- conf(m): average confidence in bin m

Perfect calibration: ECE = 0.

**Theorem 11 (Temperature Scaling for Calibration):**
Post-hoc calibration via temperature scaling:

p_calibrated = softmax(z/T)

where z are logits and T > 0 is temperature.

Finding optimal T:
T* = argmin_T NLL(softmax(z/T), y) on validation set

This preserves model accuracy while improving calibration.

**Proof of effectiveness:**
Temperature scaling is a monotonic transformation of probabilities:
- Preserves argmax â†’ same predicted classes
- T > 1: reduces confidence (spreads probability mass)
- T < 1: increases confidence (concentrates probability mass)
- Optimizing T via NLL finds best calibration on validation data. âˆ

### Surrogate Loss Bounds

The 0-1 loss L_{0-1}(y, f(x)) = ğŸ™[y â‰  sign(f(x))] is non-convex and discontinuous.
Classification algorithms use **surrogate losses** that are convex and differentiable.

**Definition 9 (Ï†-surrogate Loss):**
A surrogate loss Ï†: â„ â†’ â„â‚Š replaces 0-1 loss:

L_Ï†(y, f(x)) = Ï†(yÂ·f(x))

where y âˆˆ {-1, +1} and f(x) âˆˆ â„ is the margin.

**Common Surrogates:**
- 0-1 loss: Ï†(z) = ğŸ™[z â‰¤ 0]
- Hinge: Ï†(z) = max(0, 1 - z)
- Logistic: Ï†(z) = log(1 + e^(-z))
- Exponential: Ï†(z) = e^(-z)
- Squared: Ï†(z) = (1 - z)Â²

**Theorem 12 (Classification Calibration):**
A surrogate Ï† is **classification-calibrated** if minimizing Ï†-risk implies minimizing 0-1 risk:

R_Ï†(f_n) â†’ inf_f R_Ï†(f)  âŸ¹  R_{0-1}(f_n) â†’ inf_f R_{0-1}(f)

**Sufficient condition:** Ï† is differentiable, convex, and Ï†'(0) < 0.

All common surrogates (hinge, logistic, exponential) satisfy this.

**Theorem 13 (Bartlett-Jordan-McAuliffe Bound):**
For margin-based surrogates with Ï† convex, decreasing, and Ï†(0) = 1:

R_{0-1}(f) - R*_{0-1} â‰¤ Ïˆ_Ï†^(-1)(R_Ï†(f) - R*_Ï†)

where Ïˆ_Ï† is the calibration function measuring how well Ï† bounds 0-1 loss.

**Explicit bounds:**

(a) **Hinge loss:** Ï†(z) = max(0, 1-z)
R_{0-1}(f) - R* â‰¤ R_Ï†(f) - R*_Ï†

(b) **Logistic loss:** Ï†(z) = log(1 + e^(-z))
R_{0-1}(f) - R* â‰¤ âˆš(2(R_Ï†(f) - R*_Ï†))

(c) **Exponential loss:** Ï†(z) = e^(-z)
R_{0-1}(f) - R* â‰¤ âˆš(2(R_Ï†(f) - R*_Ï†))

**Interpretation:**
- Minimizing surrogate Ï†-risk yields low 0-1 risk
- Logistic and exponential: square root relationship (faster than linear)
- Hinge: linear relationship (tightest bound)

**Theorem 14 (H-Consistency Bounds):**
For hypothesis class H, a loss is **H-consistent** if minimizing empirical Ï†-risk over H
yields optimal 0-1 risk over H.

For finite VC dimension d and n samples:

R_{0-1}(fÌ‚) â‰¤ R*_{0-1} + O(âˆš(d/n)) + Ïˆ_Ï†^(-1)(RÌ‚_Ï†(fÌ‚) - R_Ï†(f*) + O(âˆš(d/n)))

This combines:
- Approximation error: R_Ï†(f*) - R*_Ï†
- Estimation error: RÌ‚_Ï†(fÌ‚) - R_Ï†(fÌ‚)
- Calibration gap: Ïˆ_Ï†^(-1)

### Cross-Entropy Loss (Log Loss)

**Binary Classification:**
```
L(y, Å·) = -[y log(Å·) + (1-y) log(1-Å·)]

where:
- y âˆˆ {0, 1}: true label
- Å· âˆˆ [0, 1]: predicted probability
```

**Multi-Class (Categorical Cross-Entropy):**
```
L(y, Å·) = -Î£_c y_c log(Å·_c)

where:
- y: one-hot encoded true label [0,0,1,0,...,0]
- Å·: predicted probabilities [0.1, 0.2, 0.6, 0.1, ...]
- Î£_c y_c = 1, Î£_c Å·_c = 1
```

**Gradient (with Softmax):**
```
Combined softmax + cross-entropy:

Softmax: Å·_i = exp(z_i) / Î£_j exp(z_j)

Gradient: âˆ‚L/âˆ‚z_i = Å·_i - y_i

Remarkably simple! This is why softmax + cross-entropy is standard.

Derivation:
âˆ‚L/âˆ‚z_i = Î£_c (âˆ‚L/âˆ‚Å·_c)(âˆ‚Å·_c/âˆ‚z_i)
        = -Î£_c (y_c/Å·_c)(âˆ‚Å·_c/âˆ‚z_i)

For softmax:
âˆ‚Å·_j/âˆ‚z_i = Å·_j(Î´_ij - Å·_i)

Combining:
âˆ‚L/âˆ‚z_i = -Î£_c (y_c/Å·_c)Â·Å·_cÂ·(Î´_ic - Å·_i)
        = -y_i + Å·_iÂ·Î£_c y_c
        = Å·_i - y_i  (since Î£_c y_c = 1)
```

**Why this gradient is elegant and perfect:**

1. **Intuitive form:** Gradient = prediction error
   - If Å·_i = 0.9 and y_i = 1 (correct class): gradient = -0.1 (small correction)
   - If Å·_i = 0.1 and y_i = 1: gradient = -0.9 (large correction)
   - Automatic adjustment based on confidence error

2. **No vanishing gradient problem:**
   - Gradient magnitude = |Å·_i - y_i|
   - Even when completely wrong (Å·_i â†’ 0 for true class), gradient stays bounded
   - Compare to squared loss: âˆ‚(Å· - y)Â²/âˆ‚z = 2(Å· - y)Â·Å·(1-Å·) â†’ vanishes when Å· â†’ 0 or 1
   - Cross-entropy: Always provides learning signal when wrong

3. **Natural cancellation of complex terms:**
   - Softmax derivative involves all classes (Jacobian)
   - Cross-entropy derivative has division by prediction
   - Together they magically simplify to Å·_i - y_i
   - No exponentials, no divisions in final gradient!

4. **Probabilistic interpretation:**
   - Gradient points in direction of steepest KL divergence reduction
   - Matches Fisher information matrix for efficient learning
   - Statistically optimal for categorical distributions

**Example showing non-saturation:**
```
True class: i=2, y = [0, 0, 1, 0]
Logits: z = [-5, -5, -5, 1] (very wrong! predicting class 3)

Softmax: Å· â‰ˆ [0.002, 0.002, 0.002, 0.994]

Gradient w.r.t. zâ‚‚ (true class): 0.002 - 1 = -0.998
Large negative gradient â†’ increase zâ‚‚ strongly âœ“

Compare if we used squared loss (Å· - y)Â²:
âˆ‚/âˆ‚zâ‚‚ = 2(Å·â‚‚ - 1)Â·Å·â‚‚Â·(1-Å·â‚‚) â‰ˆ 2(-0.998)Â·0.002Â·0.998 â‰ˆ -0.004
Tiny gradient due to Å·â‚‚ â‰ˆ 0 â†’ training stalls! âœ—
```

**Statistical Interpretation:**
```
Maximum Likelihood under Categorical distribution:

P(y = c | x) = Å·_c

Likelihood: L = Î _i P(y_i | x_i)
Log-likelihood: Î£_i log P(y_i | x_i) = Î£_i log Å·_{y_i}

Negative log-likelihood = Cross-entropy

Minimizing cross-entropy âŸº Maximum likelihood estimation
```

**Convergence Properties:**
```
For logistic regression (binary):

z = w^T x + b
Å· = Ïƒ(z) = 1/(1 + e^(-z))
L(w) = -Î£_i [y_i log Ïƒ(w^T x_i) + (1-y_i) log(1-Ïƒ(w^T x_i))]

Properties:
- Convex in w (assuming linearly separable or regularized)
- Gradient: âˆ‡L(w) = Î£_i (Å·_i - y_i) x_i
- Smooth (unlike hinge loss)

Convergence rate (GD):
- Convex case: O(1/k) for Îµ-accuracy
- Locally strongly convex: O(log(1/Îµ))

**Numerical stability:**
Critical issue: Direct computation causes overflow/underflow.

**Problem:**
```
Softmax: Å·_i = exp(z_i) / Î£_j exp(z_j)

For z = [1000, 1001, 1002]:
exp(1000) â‰ˆ 10^434 â†’ overflow!
exp(-1000) â‰ˆ 10^-434 â†’ underflow!
```

**Solution - Log-Sum-Exp trick:**
```
log(Î£ exp(z_i)) = c + log(Î£ exp(z_i - c))

where c = max_i(z_i)

Implementation:
1. Compute c = max(z)
2. Compute z' = z - c  (now max(z') = 0)
3. Compute log(Î£ exp(z'_i))
4. Result = c + log(Î£ exp(z'_i))

For z = [1000, 1001, 1002]:
z' = [0, 1, 2]  âœ“ (stable!)
exp(z') = [1, 2.718, 7.389]  âœ“
```

**Softmax stability:**
```
Numerically stable implementation:
z' = z - max(z)
Å· = exp(z') / sum(exp(z'))

This ensures:
- Largest logit becomes 0 â†’ exp(0) = 1 (stable)
- All other logits are negative â†’ exp(negative) âˆˆ (0,1) (stable)
- Denominator â‰¥ 1 (no division by tiny numbers)
```

**Cross-entropy stability:**
```
Avoid: L = -log(softmax(z)_y)

Use: L = -z_y + log_sum_exp(z)

This combines operations for better numerical precision.
```
```

**Behavior Analysis:**
```
For binary case:

When y = 1:
- Å· â†’ 1: L â†’ 0 (correct, confident)
- Å· â†’ 0.5: L = -log(0.5) = 0.69 (uncertain)
- Å· â†’ 0: L â†’ âˆ (wrong, confident - heavily penalized!)

Gradient magnitude:
|âˆ‚L/âˆ‚z| = |Å· - y|

Properties:
- Large gradient when very wrong (fast correction)
- Small gradient when correct (stability)
- Never saturates when wrong (unlike squared error)
```

**Use Cases:**
```
âœ“ Multi-class classification (standard choice)
âœ“ Binary classification
âœ“ Probability estimation tasks
âœ“ When: Labels are mutually exclusive
âœ“ When: Need calibrated probabilities
âœ— When: Labels are not exclusive (use BCE with logits)
```

### Hinge Loss (SVM Loss)

**Mathematical Form:**
```
Binary classification (y âˆˆ {-1, +1}):

L(y, z) = max(0, 1 - yÂ·z)

where z = w^T x + b (decision function, not probability)

Multi-class (SVM):
L(y, z) = Î£_{jâ‰ y} max(0, z_j - z_y + Î”)

where Î” is margin (typically Î” = 1)
```

**Gradient:**
```
âˆ‚L/âˆ‚z = {
  -y    if yÂ·z < 1
  0     if yÂ·z â‰¥ 1
}

Properties:
- Non-differentiable at yÂ·z = 1 (use subgradient)
- Zero gradient in margin region
- Constant gradient outside margin
```

**Geometric Interpretation:**
```
Decision boundary: w^T x + b = 0

Margin region: |w^T x + b| â‰¤ 1

Hinge loss:
- Zero for points correctly classified with margin > 1
- Linear penalty for points in margin or misclassified

Goal: Maximize margin while allowing few violations
```

**Convergence Properties:**
```
SVM optimization:

min_{w,b} Î»||w||Â² + (1/n)Î£áµ¢ max(0, 1 - yáµ¢(w^T xáµ¢ + b))

Properties:
- Convex
- Non-smooth (has corners)
- Regularization Î»||w||Â² makes it strongly convex

**Convergence (Subgradient Descent):**
- Rate: O(1/âˆšk) for non-smooth convex functions
- After k iterations: f(wÌ„_k) - f(w*) â‰¤ RÂ·G/âˆšk
  - R = ||w_0 - w*|| (initial distance)
  - G = bound on subgradient norm
  - wÌ„_k = (1/k)Î£ w_i (average iterate)
- Slower than smooth losses (O(1/k) or O((1-1/Îº)^k))
- Requires diminishing step size: Î±_k = Î±_0/âˆšk

**Coordinate descent / SMO (Sequential Minimal Optimization):**
- More efficient for SVMs than subgradient methods
- Exploits structure: updates 2 variables at a time
- Convergence rate:
  - Worst-case: O(1/ÎµÂ²) iterations
  - Typical practice: O(1/Îµ) or better
  - Each iteration: O(n) for linear kernel
- Advantages:
  - No learning rate tuning needed
  - Exact line search per coordinate
  - Handles constraints naturally

**Dual formulation:**
```
Primal: min_{w,b} Â½||w||Â² + CÂ·Î£_i max(0, 1 - y_i(w^T x_i + b))

Dual: max_Î± Î£_i Î±_i - Â½Î£_{i,j} Î±_i Î±_j y_i y_j k(x_i, x_j)
      s.t. 0 â‰¤ Î±_i â‰¤ C, Î£_i Î±_i y_i = 0

- Kernel trick: k(x_i, x_j) = Ï†(x_i)^T Ï†(x_j)
- QP solvers: Polynomial time O(nÂ²) to O(nÂ³)
- Sparse solution: Most Î±_i = 0 (support vectors)
```
```

**Comparison with Cross-Entropy:**
```
Property          | Hinge Loss        | Cross-Entropy
------------------|-------------------|------------------
Output            | Margin z          | Probability Å·
Range             | z âˆˆ â„             | Å· âˆˆ [0,1]
Differentiable    | No (at margin)    | Yes
Far from boundary | 0 gradient        | Small gradient
At boundary       | Constant grad     | Depends on Å·
Outlier effect    | Linear            | Logarithmic
Calibration       | No                | Yes (probabilities)

Hinge: Focuses on decision boundary
Cross-Entropy: Optimizes probability estimates
```

**Use Cases:**
```
âœ“ Support Vector Machines
âœ“ When: Only care about classification, not probabilities
âœ“ When: Want sparse solutions (many support vectors)
âœ— When: Need probability estimates
âœ— When: Using neural networks (use cross-entropy instead)
```

### Focal Loss

**Mathematical Form:**
```
FL(p_t) = -Î±_t(1 - p_t)^Î³ log(p_t)

where:
p_t = {
  p      if y = 1
  1-p    if y = 0
}

Parameters:
- Î³ â‰¥ 0: focusing parameter (typically Î³ = 2)
- Î±_t: class balancing weight

Standard cross-entropy: Î³ = 0, Î±_t = 1
```

**Motivation:**
```
Problem: Class imbalance in object detection
- Background: 99% of examples
- Objects: 1% of examples

Cross-entropy:
- Easy negatives dominate loss
- Model doesn't learn hard examples well

Focal loss solution:
- Down-weight easy examples: (1 - p_t)^Î³
- Focus on hard examples
```

**Behavior:**
```
For Î³ = 2:

Example classified correctly with p = 0.9:
- CE: -log(0.9) = 0.105
- FL: -(1-0.9)Â² log(0.9) = -0.01Â·0.105 = 0.00105
- 100Ã— reduction!

Example misclassified with p = 0.1 (y=1):
- CE: -log(0.1) = 2.303
- FL: -(1-0.1)Â² log(0.1) = -0.81Â·2.303 = 1.865
- Only 20% reduction

Gradient for hard examples >> easy examples
```

**Gradient:**
```
âˆ‚FL/âˆ‚z = Î±_t y(1-p_t)^Î³[Î³p_t log(p_t) + p_t - 1]

Properties:
- Complicated but can be efficiently computed
- Automatic hard example mining
- No need for manual sampling
```

**Convergence Properties:**
```
Properties:
- Non-convex (due to modulating factor)
- Still works well in practice
- Similar convergence to cross-entropy
- May need more iterations

**Hyperparameter sensitivity:**
- Î³: Controls focusing strength
  - Typical range: [0, 5]
  - Robust choice: Î³ = 2 (from RetinaNet paper)
  - Higher Î³ = more aggressive down-weighting
- Î±_t: Balances class importance
  - Formula for positive class: Î± = n_neg/(n_pos + n_neg)
  - Typical range: [0.25, 0.75]
  - Start with 0.25, tune on validation set
- Grid search recommended: Î³ âˆˆ {0.5, 1, 2, 3}, Î± âˆˆ {0.25, 0.5, 0.75}
- Less sensitive than expected: [1.5, 2.5] Ã— [0.2, 0.3] usually works

**Computational overhead:**
- Extra operations: (1-p_t)^Î³ and Î³Â·p_tÂ·log(p_t) terms
- Overhead: ~15-25% vs standard cross-entropy
- Memory: Same as cross-entropy (no extra storage)
- Backward pass: More complex gradient but well-optimized in modern frameworks
```

**Hyperparameter selection guide:**

**Î³ (focusing parameter):**
- Î³ = 0: Standard cross-entropy (no focusing)
- Î³ = 1: Moderate down-weighting of easy examples
- Î³ = 2: Standard choice (aggressive down-weighting)
- Î³ = 5: Very aggressive (may ignore too many examples)

**Effect of Î³ on loss reduction:**
```
Well-classified example (p_t = 0.9):
Î³=0: weight = 1.0     (CE baseline)
Î³=1: weight = 0.1     (10Ã— reduction)
Î³=2: weight = 0.01    (100Ã— reduction)
Î³=5: weight = 0.00001 (100,000Ã— reduction!)

Misclassified example (p_t = 0.1):
Î³=0: weight = 1.0
Î³=1: weight = 0.9     (10% reduction)
Î³=2: weight = 0.81    (19% reduction)
Î³=5: weight = 0.59    (41% reduction)
```

**Î±_t (class balancing):**
- Purpose: Balance positive/negative class importance
- Formula: Î±_t = n_neg / (n_pos + n_neg) for positive class
- Example with 1:99 imbalance: Î±_t = 0.99 for positive class
- Typical range: 0.25-0.75
- Interacts with Î³: start with Î±_t = 0.25, adjust based on validation

**When focal loss helps vs doesn't:**

**Helps when:**
- Extreme imbalance (>100:1)
- Many easy negatives (background in object detection)
- Few hard positives that matter
- One-stage detectors (RetinaNet, FCOS)

**Doesn't help / may hurt when:**
- Balanced dataset (use standard CE)
- Easy positives and hard negatives (focal loss would focus on wrong examples)
- Two-stage methods already handling imbalance via sampling
- Very small datasets (<1K samples) - may overfit to hard examples

**Practical tip:**
Start with CE, switch to focal loss if:
1. Model predicts majority class with high confidence
2. Minority class recall is low despite high precision
3. Training loss plateaus early

#### Rigorous Focal Loss Theory

**Theorem 20 (Modulating Factor Analysis - Lin et al., 2017):**
The focal loss modulating factor (1 - p_t)^Î³ provides automatic hard example mining:

FL(p_t) = -(1 - p_t)^Î³ log(p_t)

**Properties:**
(a) **Gradient amplification:** For Î³ > 0, the gradient magnitude ratio is:

|âˆ‚FL/âˆ‚z| / |âˆ‚CE/âˆ‚z| = (1-p_t)^(Î³-1) Â· |1 - p_t(1 + Î³log(p_t))|

For well-classified examples (p_t â†’ 1):
- CE gradient â†’ 0 linearly: âˆ‚CE/âˆ‚z â‰ˆ 1 - p_t
- FL gradient â†’ 0 faster: âˆ‚FL/âˆ‚z â‰ˆ (1 - p_t)^Î³

(b) **Loss reduction factor:** Compared to cross-entropy:

FL(p_t) / CE(p_t) = (1 - p_t)^Î³

For Î³ = 2 and p_t = 0.9: reduction = 0.01 (100Ã— smaller)
For Î³ = 2 and p_t = 0.5: reduction = 0.25 (4Ã— smaller)

**Proof of gradient:**
Let z be the logit with p_t = Ïƒ(z) for positive class.

FL(z) = -(1 - Ïƒ(z))^Î³ log Ïƒ(z)

âˆ‚FL/âˆ‚z = -(1 - Ïƒ(z))^Î³ Â· (-1/Ïƒ(z)) Â· Ïƒ'(z) + Î³(1 - Ïƒ(z))^(Î³-1) Â· Ïƒ'(z) Â· log Ïƒ(z)

Using Ïƒ'(z) = Ïƒ(z)(1 - Ïƒ(z)):

âˆ‚FL/âˆ‚z = (1 - Ïƒ(z))^Î³ Â· (1 - Ïƒ(z)) - Î³(1 - Ïƒ(z))^Î³ Â· Ïƒ(z) log Ïƒ(z)
        = (1 - Ïƒ(z))^Î³ Â· [1 - Ïƒ(z) - Î³Ïƒ(z) log Ïƒ(z)]

For small 1 - Ïƒ(z), the (1 - Ïƒ(z))^Î³ term dominates. âˆ

**Theorem 21 (Effective Number of Examples - Mukhoti et al., 2020):**
The focal loss effectively weights each example by (1 - p_t)^Î³. The **effective number**
of training examples is:

n_eff = Î£áµ¢ (1 - p_t,i)^Î³

For Î³ = 2 and binary classification with 99% accuracy:
- Actual examples: n = 10,000
- Effective examples: n_eff â‰ˆ 100 (only hard examples contribute!)

This explains why focal loss requires more iterations: fewer "effective" gradients per batch.

**Theorem 22 (Convergence with Focal Loss):**
For focal loss with Î³ > 0, the optimization landscape is **non-convex** even for
linear classifiers due to the modulating factor.

However, under standard assumptions:
(a) **Stationarity:** SGD finds Îµ-stationary point in:
O(1/(ÎµÂ²Â·n_eff))  iterations

(b) **Sample complexity:** For (Îµ, Î´)-PAC learning:
n = Î©((d/ÎµÂ²) Â· log(1/Î´) Â· E[(1 - p_t)^(-Î³)])

The expectation E[(1 - p_t)^(-Î³)] depends on data difficulty distribution.

**Interpretation:**
- Easy examples contribute negligibly: (1 - 0.99)^(-2) = 10,000Ã—
- Hard examples dominate: (1 - 0.5)^(-2) = 4Ã—
- Extremely hard: (1 - 0.1)^(-2) = 1.23Ã—

Sample complexity is effectively determined by hard example distribution!

**Theorem 23 (Optimal Î³ Selection):**
For imbalanced dataset with positive class proportion Ï€, the optimal focusing parameter
Î³* approximately satisfies:

Î³* â‰ˆ log(1/Ï€) / log(1/pÌ„_easy)

where pÌ„_easy is the average confidence on easy negatives.

**Derivation:**
Want (1 - pÌ„_easy)^Î³ â‰ˆ Ï€ to balance contribution of easy negatives:

(1 - pÌ„_easy)^Î³ = Ï€
Î³ log(1 - pÌ„_easy) = log Ï€
Î³ = log Ï€ / log(1 - pÌ„_easy)

For pÌ„_easy â†’ 1: log(1 - pÌ„_easy) â‰ˆ -(1 - pÌ„_easy)

**Example:**
- Imbalance ratio: Ï€ = 0.01 (1% positives)
- Easy negative confidence: pÌ„_easy = 0.95
- Optimal Î³: log(0.01)/log(0.05) â‰ˆ -4.6/-3.0 â‰ˆ 1.5

This is why Î³ = 2 works well for extreme imbalance (1:100 to 1:1000).

**Theorem 24 (Class Balancing Weight Î±):**
For optimal Î±_t, the total expected contribution from each class should be equal:

n_pos Â· E[(1 - p_t)^Î³ | y=1] Â· Î±_pos = n_neg Â· E[(1 - p_t)^Î³ | y=0] Â· Î±_neg

With Î±_neg = 1 - Î±_pos:

Î±_pos* = n_neg Â· E[(1-p_t)^Î³ | y=0] / (n_pos Â· E[(1-p_t)^Î³ | y=1] + n_neg Â· E[(1-p_t)^Î³ | y=0])

**Special case:** If both classes equally difficult:
Î±_pos* = n_neg / (n_pos + n_neg)  (inverse frequency weighting)

**Practical approximation:** Start with Î± = 0.25 for positive class (empirical finding
from RetinaNet), then:
- If positive recall too low: increase Î± â†’ 0.5
- If positive precision too low: decrease Î± â†’ 0.1

**Theorem 25 (Focal Loss Calibration Properties):**
Focal loss is **not** a proper scoring rule for Î³ > 0. The modulating factor
(1 - p_t)^Î³ breaks the proper scoring rule property.

**Proof:**
For proper scoring rule, optimal prediction is true probability:
E_Y[L(p, Y)] minimized at p = P(Y=1)

For focal loss:
E_Y[FL(p, Y)] = -P(Y=1)Â·(1-p)^Î³ log p - P(Y=0)Â·p^Î³ log(1-p)

Taking derivative and setting to zero does NOT yield p = P(Y=1) for Î³ â‰  0.

**Consequence:** Models trained with focal loss may be **miscalibrated**:
- Overconfident on hard examples (by design!)
- Underconfident on easy examples (down-weighted)

**Solution:** Post-hoc calibration (temperature scaling) after training with focal loss.

**Empirical calibration analysis (RetinaNet on COCO):**
Before temperature scaling:
- Easy examples (IoU > 0.7): avg confidence 0.62 (underconfident)
- Hard examples (IoU 0.5-0.7): avg confidence 0.51 (appropriate)
- ECE = 0.15

After temperature scaling with T = 1.5:
- Easy examples: avg confidence 0.78
- Hard examples: avg confidence 0.58
- ECE = 0.05 (much better calibrated!)

**Use Cases:**
```
âœ“ Object detection (RetinaNet)
âœ“ Extreme class imbalance
âœ“ When: Hard examples are important
âœ“ When: Easy negatives dominate
âœ— When: Classes are balanced (use cross-entropy)
âœ— When: Need calibrated probabilities (use CE or calibrate post-hoc)
```

### Label Smoothing

**Mathematical Form:**
```
Standard one-hot: y_i = Î´_ij (1 if i=j, 0 otherwise)

Label smoothing:
y'_i = (1 - Îµ)Î´_ij + Îµ/K

where:
- Îµ: smoothing parameter (typically 0.1)
- K: number of classes

Loss: L = -Î£_i y'_i log(Å·_i)
```

**Effect:**
```
Instead of [0, 0, 1, 0, 0] (K=5)
Use: [0.02, 0.02, 0.92, 0.02, 0.02] with Îµ=0.1

Prevents overfitting to training labels
Encourages model not to be overconfident
```

**Gradient:**
```
âˆ‚L/âˆ‚z_i = Å·_i - y'_i
        = Å·_i - [(1-Îµ)Î´_ij + Îµ/K]

For true class j:
âˆ‚L/âˆ‚z_j = Å·_j - (1-Îµ) - Îµ/K
        = Å·_j - 1 + Îµ(1 - 1/K)

For other classes:
âˆ‚L/âˆ‚z_i = Å·_i - Îµ/K

Model can never drive Å·_j â†’ 1 (always has small error Îµ)
```

**Regularization Effect:**
```
Theorem (MÃ¼ller et al., 2019):
Label smoothing is equivalent to adding KL divergence to uniform:

L_LS = (1-Îµ)L_CE + ÎµÂ·KL(u || Å·)

where u is uniform distribution

Effect:
- Prevents largest logit from becoming much larger than others
- Improves generalization
- Better calibration
```

**Why label smoothing works:**

1. **Prevents overconfidence:**
   - Hard labels encourage model to make predictions arbitrarily confident
   - Softmax with z_correct >> z_wrong â†’ Å·_correct â‰ˆ 1
   - Model achieves this by increasing logit magnitudes: ||z|| â†’ âˆ
   - Label smoothing caps maximum achievable probability: Å·_max = 1 - Îµ + Îµ/K

2. **Implicit regularization on logits:**
   - Without smoothing: logits can grow unbounded
   - With smoothing: bounded logits (can't make perfect prediction)
   - Example with K=10, Îµ=0.1:
     - Target: [0.01, 0.01, 0.91, ..., 0.01]
     - Model can't achieve this by making z_2 â†’ âˆ
     - Forces model to learn reasonable feature magnitudes

3. **Better calibration:**
   - Overconfident models: predicted probability doesn't match true probability
   - Example: Model predicts 99% confidence but is only 90% accurate
   - Label smoothing reduces confidence â†’ better calibrated probabilities
   - Calibration: P(correct | confidence=p) â‰ˆ p

4. **Robustness to label noise:**
   - Real labels may have ~5-10% error rate (human annotation mistakes)
   - Hard labels assume 100% certainty â†’ model fits noise
   - Soft labels admit uncertainty â†’ model less affected by noise

**Optimal Îµ selection:**
```
Theory suggests: Îµ â‰ˆ estimated label noise rate

Practical recommendations:
- Îµ = 0.1: Standard choice for ImageNet (works for most cases)
- Îµ = 0.05: Conservative, when labels are high quality
- Îµ = 0.2: Aggressive, when labels are noisy or dataset is small
- Îµ = 0.3: Very aggressive, rarely used (may hurt)

Rule of thumb: Start with Îµ = 0.1, increase if overfitting, decrease if underfitting
```

**Use Cases:**
```
âœ“ Image classification (standard practice now)
âœ“ When: Want better generalization
âœ“ When: Training labels may be noisy
âœ“ When: Model tends to overfit
âœ“ When: Need calibrated probabilities for decision-making
âœ— When: Dataset is very small (<1K samples)
âœ— When: Need perfect accuracy on training set (e.g., memorization tasks)
âœ— When: Labels are verified perfect (rare in practice)
```

---

## Probabilistic Loss Functions

### f-Divergences: A Unified Framework

Many divergences between probability distributions belong to the family of f-divergences.

**Definition 10 (f-Divergence):**
For convex function f: â„â‚Š â†’ â„ with f(1) = 0:

D_f(P || Q) = E_Q[f(dP/dQ)] = Î£_x Q(x)Â·f(P(x)/Q(x))

where dP/dQ is the Radon-Nikodym derivative (density ratio).

**Properties:**
1. Non-negativity: D_f(P || Q) â‰¥ f(1) = 0 (by Jensen's inequality)
2. Identity: D_f(P || Q) = 0 âŸº P = Q
3. Convexity: D_f(P || Q) is convex in the pair (P, Q)

**Proof of non-negativity:**
By Jensen's inequality (f is convex):

D_f(P || Q) = E_Q[f(dP/dQ)]
            â‰¥ f(E_Q[dP/dQ])
            = f(Î£_x Q(x)Â·(P(x)/Q(x)))
            = f(Î£_x P(x))
            = f(1)
            = 0 âˆ

**Theorem 15 (Common Divergences as f-Divergences):**

(a) **KL Divergence:** f(t) = t log t
D_f(P || Q) = KL(P || Q) = Î£_x P(x) log(P(x)/Q(x))

(b) **Reverse KL:** f(t) = -log t
D_f(P || Q) = KL(Q || P) = Î£_x Q(x) log(Q(x)/P(x))

(c) **Total Variation:** f(t) = (1/2)|t - 1|
D_f(P || Q) = (1/2)Î£_x |P(x) - Q(x)| = TV(P, Q)

(d) **Squared Hellinger:** f(t) = (âˆšt - 1)Â²
D_f(P || Q) = Î£_x Q(x)(âˆš(P(x)/Q(x)) - 1)Â²
            = Î£_x (âˆšP(x) - âˆšQ(x))Â²

(e) **Chi-squared:** f(t) = (t - 1)Â²
D_f(P || Q) = Î£_x Q(x)((P(x)/Q(x)) - 1)Â²
            = Î£_x (P(x) - Q(x))Â²/Q(x)
            = Ï‡Â²(P || Q)

(f) **Jensen-Shannon:** Symmetrized KL
JS(P || Q) = (1/2)KL(P || M) + (1/2)KL(Q || M)
where M = (1/2)(P + Q)

Properties:
- Symmetric: JS(P || Q) = JS(Q || P)
- Bounded: 0 â‰¤ JS(P || Q) â‰¤ log 2
- Square root is a metric: âˆšJS is a proper distance

**Theorem 16 (Variational Representation of f-Divergences):**
For any f-divergence:

D_f(P || Q) = sup_{T: Xâ†’â„} {E_P[T(X)] - E_Q[f*(T(X))]}

where f* is the convex conjugate of f:
f*(y) = sup_t {ty - f(t)}

This variational form is the basis for adversarial training (GANs).

**Example (KL Divergence):**
For f(t) = t log t:
f*(y) = e^(y-1)

Thus:
KL(P || Q) = sup_T {E_P[T] - E_Q[e^(T-1)]}

**Theorem 17 (Data Processing Inequality):**
For any f-divergence and Markov chain X â†’ Y â†’ Z:

D_f(P_X || Q_X) â‰¥ D_f(P_Y || Q_Y) â‰¥ D_f(P_Z || Q_Z)

In words: processing data through any channel cannot increase divergence.

Proof: Uses Jensen's inequality and the Markov property. âˆ

**Practical implication:**
- Feature extraction reduces divergence between distributions
- Information is lost, never gained, through transformations
- Applies to all f-divergences simultaneously

### Kullback-Leibler (KL) Divergence

**Mathematical Form:**
```
KL(P || Q) = Î£_x P(x) log(P(x)/Q(x))
           = E_P[log P(x)] - E_P[log Q(x)]

Continuous case:
KL(P || Q) = âˆ« p(x) log(p(x)/q(x)) dx
```

**Properties:**
```
1. Non-negativity: KL(P || Q) â‰¥ 0
2. Identity: KL(P || Q) = 0 âŸº P = Q (almost everywhere)
3. Asymmetry: KL(P || Q) â‰  KL(Q || P) in general
4. Not a metric (no triangle inequality)

Proof of non-negativity (Jensen's inequality):
KL(P || Q) = -E_P[log(Q/P)]
           â‰¥ -log E_P[Q/P]
           = -log Î£_x P(x)Â·(Q(x)/P(x))
           = -log Î£_x Q(x)
           = -log 1 = 0
```

**Relationship to Cross-Entropy:**
```
H(P, Q) = -E_P[log Q(x)]         (cross-entropy)
H(P) = -E_P[log P(x)]            (entropy)

KL(P || Q) = H(P, Q) - H(P)

In supervised learning:
- P: true distribution (one-hot)
- Q: predicted distribution
- H(P) = 0 for one-hot labels

Therefore: KL(P || Q) = H(P, Q)

Minimizing KL âŸº Minimizing cross-entropy
```

**Forward vs Reverse KL:**
```
Forward KL: KL(P || Q)
- Minimizing Q w.r.t. P
- Mean-seeking: Q covers all modes of P
- Used in: Maximum likelihood

Reverse KL: KL(Q || P)
- Mode-seeking: Q focuses on one mode of P
- Used in: Variational inference

Behavior difference:
P has two modes, Q is unimodal
- Forward KL: Q spreads between modes (bad)
- Reverse KL: Q picks one mode (better for VI)
```

**Intuitive explanation of forward vs reverse:**

**Forward KL: KL(P || Q) = E_P[log P - log Q]**

Behavior:
- Expectation over P: Must have high probability under Q wherever P has probability
- If P(x) > 0 but Q(x) â‰ˆ 0: log Q(x) â†’ -âˆ, KL â†’ âˆ (infinite penalty!)
- Forces Q to cover all of P (Q spreads out to avoid zero probability under P)

Example (mixture of two Gaussians):
```
P = 0.5Â·N(-3, 1) + 0.5Â·N(+3, 1)  (two modes at -3 and +3)
Q = N(Î¼, ÏƒÂ²)                      (single Gaussian)

Forward KL minimization:
- Q must have P(x)>0 â†’ Q(x)>0 everywhere P has mass
- Result: Q = N(0, 4) (wide Gaussian covering both modes)
- Mean Î¼=0 (average of -3 and +3)
- Large variance to cover both modes
```

**Reverse KL: KL(Q || P) = E_Q[log Q - log P]**

Behavior:
- Expectation over Q: Only cares about where Q has probability
- If Q(x) > 0 but P(x) â‰ˆ 0: log P(x) â†’ -âˆ, KL â†’ âˆ
- Forces Q to only put mass where P has high probability (Q is zero-avoiding under P)
- Q ignores low-probability regions of P

Example (same mixture):
```
P = 0.5Â·N(-3, 1) + 0.5Â·N(+3, 1)
Q = N(Î¼, ÏƒÂ²)

Reverse KL minimization:
- Q can put mass anywhere, but gets penalized if P is low there
- Result: Q = N(-3, 1) OR N(+3, 1) (picks one mode!)
- Which mode depends on initialization
- Tight fit to chosen mode
```

**Practical consequences:**

**Use Forward KL (Maximum Likelihood) when:**
- Have samples from P (data distribution)
- Want Q to explain all data variations
- Can't afford to miss any data modes
- Example: Generative modeling (GAN, diffusion)

**Use Reverse KL (Variational Inference) when:**
- Need to sample from Q and evaluate P
- Want tractable approximate posterior
- Okay with approximating one mode well
- Example: VAE, variational Bayes

**Visual intuition:**
```
P: âš«  âš«    (two modes)

Forward KL â†’ Q: â”â”â”â”â” (covers both, wide)
Reverse KL â†’ Q:  âš«    (picks one, tight)
```

**Mathematical reason:**
```
Forward: âˆ« p(x) log[p(x)/q(x)] dx
- Integral weighted by p(x)
- High penalty where p(x) is large but q(x) is small
- q must spread to cover all of p

Reverse: âˆ« q(x) log[q(x)/p(x)] dx
- Integral weighted by q(x)
- Only penalty where q(x) is large
- q can ignore regions where p is small
```

**Gradient (for Variational Inference):**
```
For parameterized Q_Î¸:

âˆ‡_Î¸ KL(Q_Î¸ || P) = âˆ‡_Î¸ E_Q[log Q_Î¸(z) - log P(z)]

Using reparameterization trick:
z = g_Î¸(Îµ, x) where Îµ ~ p(Îµ)

âˆ‡_Î¸ KL = E_Îµ[âˆ‡_Î¸(log Q_Î¸(g_Î¸(Îµ,x)) - log P(g_Î¸(Îµ,x)))]

Enables backpropagation through sampling
```

**Use Cases:**
```
âœ“ Variational inference (VAE, VIB)
âœ“ Knowledge distillation
âœ“ Distribution matching
âœ“ Policy optimization (RL)
âœ— When: Need symmetric divergence (use JS divergence)
```

### Wasserstein Distance (Earth Mover's Distance)

**Mathematical Form:**
```
W_p(P, Q) = (inf_{Î³âˆˆÎ“(P,Q)} E_{(x,y)~Î³}[||x-y||^p])^{1/p}

where Î“(P,Q) is set of joint distributions with marginals P and Q

For p=1: W_1(P, Q) = inf_{Î³} E_{(x,y)~Î³}[||x-y||]

Interpretation: Minimum cost to transport mass from P to Q
```

**Properties:**
```
1. Metric: Satisfies triangle inequality
2. Weaker topology than KL divergence
3. Doesn't suffer from mode collapse
4. Well-defined even when P and Q don't overlap

Advantage over KL:
- KL(P || Q) = âˆ if supports don't overlap
- W(P, Q) is always finite
```

**Kantorovich-Rubinstein Duality:**
```
W_1(P, Q) = sup_{||f||_Lâ‰¤1} [E_P[f(x)] - E_Q[f(x)]]

where ||f||_L â‰¤ 1 means f is 1-Lipschitz

This is computable! (used in WGANs)
```

**Gradient (WGAN):**
```
Discriminator (critic) f must be 1-Lipschitz:

Methods:
1. Weight clipping: clip weights to [-c, c]
2. Gradient penalty: ||âˆ‡_x f(x)||â‚‚ â‰ˆ 1

WGAN-GP loss:
L_D = E_Q[f(x)] - E_P[f(x)] + Î»Â·E_xÌ‚[(||âˆ‡_xÌ‚ f(xÌ‚)||â‚‚ - 1)Â²]

where xÌ‚ = Î±x_real + (1-Î±)x_fake, Î± ~ U[0,1]
```

**Convergence Properties:**
```
WGAN theoretical guarantees:

Theorem: Optimizing W_1 distance:
- Generator converges to data distribution
- Even when distributions have disjoint supports

Gradient behavior:
- Non-saturating (unlike original GAN)
- Meaningful loss values (correlate with quality)
- Stable training

Practical convergence:
- Requires many critic iterations per generator step
- GP version more stable than weight clipping
```

**Use Cases:**
```
âœ“ GANs (WGAN, WGAN-GP)
âœ“ When: Distributions may not overlap
âœ“ When: Need meaningful loss values
âœ“ Optimal transport problems
âœ— When: Computational cost is critical (expensive)
```

---

## Ranking and Metric Learning

### Triplet Loss

**Mathematical Form:**
```
L = max(0, d(a, p) - d(a, n) + margin)

where:
- a: anchor sample
- p: positive sample (same class as anchor)
- n: negative sample (different class)
- d(Â·,Â·): distance metric (usually L2)
- margin: hyperparameter (typically 0.2-1.0)

Goal: d(a, p) + margin < d(a, n)
```

**Gradient:**
```
When loss > 0:

âˆ‚L/âˆ‚a = 2(a - p) - 2(a - n) = 2(n - p)
âˆ‚L/âˆ‚p = 2(p - a)
âˆ‚L/âˆ‚n = 2(n - a)

Properties:
- Pulls anchor to positive
- Pushes anchor from negative
- Zero gradient when margin satisfied
```

**Mining Strategies:**
```
Triplet selection critical!

1. Hard negative mining:
   n = argmax_n d(a, n) s.t. d(a,p) > d(a,n)
   Most violating negative

2. Semi-hard negative:
   d(a, p) < d(a, n) < d(a, p) + margin
   Violates margin but not order

3. Easy negative:
   d(a, n) > d(a, p) + margin
   Don't contribute to loss (skip)

4. Batch-all mining:
   Use all valid triplets in batch
   Can be many: O(batch_sizeÂ³)
```

**Convergence Properties:**
```
Challenges:
- Non-smooth (max function)
- Many triplets satisfy margin (zero gradient)
- Requires good mining strategy

Convergence:
- Depends heavily on mining
- Hard mining: faster but unstable
- Semi-hard: slower but more stable

Practical tips:
- Start with easy/random triplets
- Gradually increase difficulty
- Use online mining within batches
```

#### Rigorous Metric Learning Theory

**Theorem 26 (Triplet Loss as Structured Hinge Loss - Schroff et al., 2015):**
Triplet loss is a structured margin-based loss enforcing relative ordering:

L_triplet = max(0, ||f(a) - f(p)||Â² - ||f(a) - f(n)||Â² + Î±)

This is equivalent to hinge loss on the margin:
L = max(0, margin(a,p,n) + Î±)

where margin(a,p,n) = d(a,p) - d(a,n) is the **relative distance difference**.

**Properties:**
(a) **Non-convex:** Even for linear embeddings f(x) = Wx, loss is non-convex in W
(b) **Lipschitz:** With bounded features ||f(x)|| â‰¤ R:
|L(f) - L(f')| â‰¤ 4RÂ·||f - f'||_âˆ

(c) **Zero gradient region:** When margin > Î±, gradient is zero (no learning signal)

**Theorem 27 (Generalization Bound for Triplet Loss - Cao et al., 2016):**
For hypothesis class F with Rademacher complexity R_n(F), the generalization error for
triplet loss satisfies with probability â‰¥ 1-Î´:

R(f) - RÌ‚_n(f) â‰¤ 4R_n(F) + 3âˆš(log(2/Î´)/(2n))

where RÌ‚_n is empirical risk over n triplets.

**For linear embeddings** f(x) = Wx with ||W||_F â‰¤ B and ||x|| â‰¤ R:
R_n(F) â‰¤ (2BRâˆšd) / âˆšn

**Sample complexity:** For (Îµ, Î´)-accurate embedding:
n = Î©((BÂ²RÂ²d/ÎµÂ²)Â·log(1/Î´))

**Example:**
- Embedding dimension: d = 128
- Feature norm bound: R = 10
- Weight bound: B = 1
- Target error: Îµ = 0.1
- Confidence: Î´ = 0.05

Required triplets: n â‰ˆ 13,000 Â· log(20) â‰ˆ 39,000 triplets!

**Theorem 28 (Triplet Mining and Convergence - Wu et al., 2017):**
The choice of triplet mining strategy critically affects convergence:

(a) **Random triplets:** Convergence rate O(1/âˆšT) but requires ~O(NÂ³) triplets
(b) **Hard negatives:** Convergence rate O(1/T) but may cause instability
(c) **Semi-hard negatives:** Convergence O(1/âˆšT) with better stability

**Formal definition (Semi-hard negative):**
n is semi-hard for anchor a and positive p if:
||f(a) - f(p)||Â² < ||f(a) - f(n)||Â² < ||f(a) - f(p)||Â² + Î±

**Convergence guarantee (semi-hard mining):**
After T iterations with batch size B:
E[margin violation rate] â‰¤ O(âˆš(log B / T))

**Theorem 29 (Embedding Space Properties):**
After training with triplet loss, the learned embedding f: X â†’ â„^d satisfies:

(a) **Triangle inequality:** Not guaranteed! Triplet loss doesn't enforce metric properties.

(b) **Clustering property:** Same-class embeddings form clusters with radius:
r_class â‰¤ âˆšÎ±  (margin parameter bounds cluster radius)

(c) **Separation:** Different-class cluster centers separated by â‰¥ 2âˆšÎ±

**Proof of (c):**
For converged model, all triplets satisfy:
||f(a) - f(p)||Â² + Î± â‰¤ ||f(a) - f(n)||Â²

Taking expectations over class distributions:
E[||f(x_i) - c_i||Â²] + Î± â‰¤ E[||f(x_i) - c_j||Â²]  for i â‰  j

where c_i, c_j are class centroids.

By triangle inequality:
||c_i - c_j||Â² â‰¥ (âˆšE[||f(x_i) - c_j||Â²] - âˆšE[||f(x_i) - c_i||Â²])Â²
              â‰¥ (âˆšÎ±)Â² = Î±

But empirically: ||c_i - c_j|| â‰¥ 2âˆšÎ± (factor of 2 from typical distributions). âˆ

**Theorem 30 (Number of Triplets and Saturation):**
For dataset with N examples and C classes:
- Total valid triplets: O(NÂ³) (combinatorial explosion!)
- After k epochs, fraction satisfying margin: p_sat(k)

**Saturation curve:**
p_sat(k) â‰ˆ 1 - exp(-Î»k)

where Î» depends on data difficulty and model capacity.

**Practical implications:**
- Early training: most triplets violate margin â†’ strong gradients
- Late training: most triplets satisfy margin â†’ sparse gradients
- Solution: Online mining (select hard triplets within each batch)

**Effective batch size for online mining:**
For batch size B, number of valid triplets: O(BÂ³/C)
- B = 32, C = 8: ~2,000 triplets per batch!
- But only ~5-10% violate margin after initial training

**Use Cases:**
```
âœ“ Face recognition (FaceNet)
âœ“ Metric learning
âœ“ Image retrieval
âœ“ When: Need embedding space with metric properties
âœ“ When: Many classes (>100)
âœ— When: Number of classes is small (use softmax)
âœ— When: Cannot form meaningful triplets
```

### Contrastive Loss

**Mathematical Form:**
```
L = (1-y)Â·Â½dÂ² + yÂ·Â½max(0, margin - d)Â²

where:
- y = 1 if same class, 0 if different
- d = ||f(xâ‚) - f(xâ‚‚)||â‚‚

Alternative (SimCLR style):
L = -log(exp(sim(z_i, z_j)/Ï„) / Î£_k exp(sim(z_i, z_k)/Ï„))

where sim(u,v) = uÂ·v / (||u||Â·||v||) (cosine similarity)
```

**Gradient:**
```
For positive pair (y=1):
âˆ‚L/âˆ‚zâ‚ = (zâ‚ - zâ‚‚)Â·max(0, margin - d)/d

For negative pair (y=0):
âˆ‚L/âˆ‚zâ‚ = (zâ‚ - zâ‚‚)Â·d

Behavior:
- Positive: push together until margin
- Negative: push apart always (no limit)
```

**InfoNCE (Contrastive Predictive Coding):**
```
L = -E[log(f(x,c_pos) / (f(x,c_pos) + Î£_i f(x,c_neg,i)))]

where f(x,c) = exp(x^T c / Ï„)

Properties:
- Lower bound on mutual information
- As negatives â†’ âˆ: approaches MI
- Temperature Ï„ controls difficulty
```

#### Rigorous Contrastive Learning Theory

**Theorem 31 (InfoNCE as MI Lower Bound - van den Oord et al., 2018):**
The InfoNCE (Noise Contrastive Estimation) loss provides a lower bound on mutual information:

L_InfoNCE = -E[log(exp(f(x,c_+)/Ï„) / (exp(f(x,c_+)/Ï„) + Î£áµ¢ exp(f(x,c_-,i)/Ï„)))]

where:
- c_+ is positive context (similar to x)
- c_-,i are K negative contexts (dissimilar to x)
- Ï„ is temperature
- f(x,c) is similarity score

**Theorem:** InfoNCE lower-bounds mutual information:

I(X; C) â‰¥ log(K+1) - L_InfoNCE

**Proof:**
Let p_+ = exp(f(x,c_+)/Ï„) / Z be the probability assigned to positive.

InfoNCE = -E[log p_+] = H_cross(p_true, p_model)

By properties of cross-entropy:
H_cross â‰¤ H(p_true) + KL(p_true || p_model)

With K negatives uniformly sampled:
H(p_true) = log(K+1)

Therefore:
I(X; C) = H(C) - H(C|X)
        â‰¥ log(K+1) - L_InfoNCE

**Key insight:** More negatives â†’ tighter bound! âˆ

**Theorem 32 (Temperature Scaling Effects - Wang & Isola, 2020):**
Temperature Ï„ controls the concentration of the distribution:

(a) **Small Ï„ (Ï„ â†’ 0):** Hard assignment (winner-take-all)
- Focuses on hardest negatives
- Can be unstable
- Typical: Ï„ = 0.05-0.1

(b) **Large Ï„ (Ï„ â†’ âˆ):** Uniform distribution
- Treats all negatives equally
- Slower learning
- Typical: Ï„ = 0.5-1.0

**Optimal Ï„:** For data with noise level Ïƒ, optimal temperature:
Ï„* â‰ˆ Ïƒ/âˆšd

where d is embedding dimension.

**Empirical finding (SimCLR):**
- d = 128: Ï„ = 0.07 works best
- d = 512: Ï„ = 0.1 works best
- d = 2048: Ï„ = 0.2 works best

**Theorem 33 (Contrastive Learning Convergence - Arora et al., 2019):**
For contrastive learning with K negatives, the embedding quality after T steps satisfies:

||f*_Î¸ - f*_optimal||Â² â‰¤ O(âˆš(d/(KÂ·T)))

where:
- d is embedding dimension
- K is number of negatives per positive
- T is number of training steps

**Implications:**
(a) **More negatives help:** Convergence âˆ 1/âˆšK
(b) **Diminishing returns:** K = 4096 only âˆš2 better than K = 2048
(c) **Need large batches:** K typically = batch_size - 1

**Example:**
- Target error: Îµ = 0.01
- Embedding dim: d = 128
- Negatives: K = 1024
- Required steps: T = d/(KÂ·ÎµÂ²) = 128/(1024Â·0.0001) â‰ˆ 1,250 steps

With batch size 256: ~5 epochs on 64K examples.

**Theorem 34 (Alignment and Uniformity - Wang & Isola, 2020):**
Good contrastive representations satisfy two properties:

(a) **Alignment:** Positive pairs close together:
L_align = E_{(x,x_+)~p_pos}[||f(x) - f(x_+)||Â²]

(b) **Uniformity:** Features uniformly distributed on hypersphere:
L_uniform = log E_{x,y~p_data}[exp(-||f(x) - f(y)||Â²)]

**Optimal embedding:** Minimizes weighted combination:
L = L_align + Î»Â·L_uniform

**Relationship to contrastive loss:**
InfoNCE implicitly optimizes both:
- Numerator â†’ minimizes L_align
- Denominator â†’ maximizes L_uniform (pushes negatives away)

**Proof sketch:**
âˆ‚L_InfoNCE/âˆ‚f(x) = -f(c_+) + E_neg[f(c_-)]

First term: pulls x toward positive (alignment)
Second term: pushes x from negatives (uniformity) âˆ

**Theorem 35 (Representation Collapse Prevention):**
Without sufficient negatives, representations **collapse** to constant:
f(x) = c for all x

**Collapse condition:** When K < d/log d (too few negatives for embedding dimension)

**Proof (informal):**
For d-dimensional embeddings on unit sphere, need K = Î©(d/log d) random negatives
to distinguish between different points.

**Practical safeguards:**
(a) **Large batch sizes:** K = batch_size - 1 â‰¥ 256
(b) **Momentum encoder:** MoCo queue maintains K = 65,536 negatives
(c) **Explicit uniformity loss:** Add L_uniform regularization
(d) **Batch normalization:** Prevents complete collapse (each feature has variance)

**Empirical collapse indicators:**
- Training loss â†’ 0 but validation performance poor
- Embedding norm ||f(x)|| â†’ 0
- Embedding variance Var[f(x)] â†’ 0
- Cosine similarity matrix rank deficient

**Theorem 36 (Sample Complexity for Contrastive Learning):**
To learn Îµ-optimal representations with Î´ confidence:

n = Î©((dÂ²/ÎµÂ²)Â·log(1/Î´)Â·log(K))

**Comparison to supervised learning:**
Supervised (K classes): n = O((d/ÎµÂ²)Â·KÂ·log(1/Î´))
Contrastive: n = O((dÂ²/ÎµÂ²)Â·log(K)Â·log(1/Î´))

**Key difference:** Contrastive requires O(d) more samples but is label-free!

**Example:**
- d = 128, K = 1000 classes, Îµ = 0.1, Î´ = 0.05
- Supervised: n â‰ˆ 164M samples
- Contrastive: n â‰ˆ 21B samples (128Ã— more!)

But contrastive doesn't need labels â†’ can use unlabeled data at scale!

**Use Cases:**
```
âœ“ Self-supervised learning (SimCLR, MoCo)
âœ“ Representation learning
âœ“ When: Have pairs of similar/dissimilar samples
âœ“ When: Unsupervised or semi-supervised setting
âœ“ When: Large unlabeled dataset available
âœ— When: Cannot create meaningful positive pairs
âœ— When: Small batch size (< 256)
```

---

## Sequence-to-Sequence Losses

### Teacher Forcing with Cross-Entropy

**Mathematical Form:**
```
For sequence y = [yâ‚, yâ‚‚, ..., y_T]:

L = -(1/T) Î£_t log P(y_t | yâ‚, ..., y_{t-1}, x)

Teacher forcing: Use ground truth y_{t-1} during training

Problems:
- Exposure bias: Model sees gold labels in training
- Test time: Uses own predictions â†’ distribution mismatch
```

**Scheduled Sampling:**
```
Mix gold and predicted tokens during training:

With probability p: use y_{t-1} (gold)
With probability 1-p: use Å·_{t-1} (predicted)

Schedule: p decreases over training (e.g., p = 1/(1+k/kâ‚€))

Reduces exposure bias but makes training non-differentiable
```

### Connectionist Temporal Classification (CTC)

**Mathematical Form:**
```
For input X of length T, output Y of length U â‰¤ T:

P(Y | X) = Î£_{Ï€âˆˆAlignments(Y)} Î _t P(Ï€_t | X)

where Alignments(Y) are all valid alignments

Example:
Y = "cat"
Alignments: [c,c,a,t], [c,a,a,t], [c,a,t,t], [-,c,a,t], etc.

CTC loss: L = -log P(Y | X)
```

**Forward-Backward Algorithm:**
```
Efficiently compute sum over exponential alignments:

Forward: Î±(t,s) = P(labels[1:s] | input[1:t])
Backward: Î²(t,s) = P(labels[s:] | input[t:])

Combined: P(Y|X) = Î±(T,|Y|)

Complexity: O(T Â· U) instead of O(T^U)
```

**Gradient:**
```
âˆ‚L/âˆ‚y_{t,k} = p_{t,k} - (1/P(Y|X)) Î£_{s:Ï€_s=k} Î±(t,s)Î²(t,s)

where p_{t,k} = P(label k at time t)

Computed efficiently with forward-backward
```

#### Rigorous CTC Theory

**Theorem 37 (CTC Forward-Backward Correctness - Graves et al., 2006):**
The forward-backward algorithm correctly computes P(Y|X) in O(TÂ·U) time,
where T = input length, U = output length.

**Forward variable:** Î±(t, s) = P(Ï€[1:t] matches Y[1:s] | X[1:t])

Recursion:
Î±(t, s) = [Î±(t-1, s) + Î±(t-1, s-1) + Î±(t-1, s-2)Â·ğŸ™[Y_s = blank]] Â· P(Y_s | X_t)

**Backward variable:** Î²(t, s) = P(Ï€[t:T] matches Y[s:U] | X[t:T])

Recursion:
Î²(t, s) = [Î²(t+1, s) + Î²(t+1, s+1) + Î²(t+1, s+2)Â·ğŸ™[Y_s = blank]] Â· P(Y_s | X_{t+1})

**Total probability:**
P(Y | X) = Î£_s Î±(T, s)Â·Î²(T, s) = Î±(T, U)

**Proof of O(TÂ·U) complexity:**
- State space: T time steps Ã— U label positions = O(TÂ·U) states
- Each state computed in O(1) using 3 previous states
- Total: O(TÂ·U) time and space âˆ

**Comparison to naive enumeration:**
- All possible alignments: |Î£|^T where |Î£| = vocabulary size
- For |Î£| = 30, T = 100: 30^100 â‰ˆ 10^147 alignments!
- Forward-backward: TÂ·U = 100Â·20 = 2,000 operations (10^144Ã— speedup!)

**Theorem 38 (CTC Gradient Computation):**
The gradient of CTC loss w.r.t. logits can be computed efficiently:

âˆ‚L_CTC/âˆ‚log P(k | X_t) = P(k | X_t) - Î£_{s: Y_s=k} Î±(t,s)Â·Î²(t,s) / P(Y|X)

**Interpretation:**
- First term: Model's prediction P(k | X_t)
- Second term: Expected label at time t given Y
- Gradient = prediction error at each time step!

**Computational complexity:**
- Forward pass: O(TÂ·U)
- Backward pass: O(TÂ·U)
- Gradient: O(TÂ·|Î£|) where |Î£| is vocabulary size
- Total per example: O(TÂ·(U + |Î£|))

**Theorem 39 (CTC Blank Label Necessity):**
The blank label is **necessary** for CTC to represent all possible alignments.

**Proof by counterexample:**
Consider Y = "aa" (repeated label) with T = 2 time steps.
- Without blank: only possible alignment is [a, a]
- Cannot distinguish from Y = "a" with alignment [a, a]!

With blank:
- Y = "aa": alignments include [a, blank], [blank, a], [a, a] (via blank)
- Y = "a": alignments are [a, blank], [blank, a], [a, a] (single a)

Blank allows CTC to:
1. Represent repeated labels: a-blank-a â†’ "aa"
2. Handle variable length: blank tokens for timing
3. Model optionality: blank for silence âˆ

**Theorem 40 (CTC Conditional Independence Assumption):**
CTC assumes **conditional independence** of outputs at each time step:

P(Ï€_t | Ï€_{1:t-1}, X) = P(Ï€_t | X_t)

This is a **strong assumption** that limits CTC expressiveness!

**Consequences:**
(a) Cannot model output dependencies: P(Y_i | Y_{i-1}) = const (no language model)
(b) Alignment depends only on acoustics, not on label sequence
(c) Need external language model for good performance

**Comparison to attention:**
- CTC: O(T) independent predictions
- Attention: O(U) autoregressive predictions with full history

**Theorem 41 (CTC Alignment Ambiguity):**
For a given Y, CTC marginalizes over **exponentially many** alignments:

Number of valid alignments â‰ˆ (T choose U)Â·2^U

**Example:**
- Y = "cat" (U = 3 labels)
- T = 10 time steps
- Valid alignments: (10 choose 3)Â·2Â³ = 120Â·8 = 960 alignments!

Each alignment Ï€ has probability:
P(Ï€ | X) = Î _{t=1}^T P(Ï€_t | X_t)

CTC loss:
L = -log Î£_Ï€ P(Ï€ | X)  (sum over all 960 alignments!)

**Theorem 42 (CTC Decoding Complexity):**

(a) **Greedy decoding:** O(TÂ·|Î£|)
- Take argmax at each time step
- Remove blanks and duplicates
- Fast but suboptimal!

(b) **Beam search:** O(TÂ·|Î£|Â·BÂ·log B)
- Maintain top-B hypotheses
- Merge paths with same label sequence
- Much better accuracy
- B = 100 typical

(c) **Optimal decoding:** O(TÂ·UÂ·|Î£|)
- Find most probable label sequence Y* = argmax_Y P(Y|X)
- Uses prefix search beam search
- Exponentially slow without pruning

**Practical beam search:**
For B = 100, |Î£| = 30, T = 100:
- Operations: 100Â·30Â·100Â·log(100) â‰ˆ 20M
- Greedy: 100Â·30 = 3K (6000Ã— faster!)
- Accuracy gap: ~5-10% WER improvement with beam search

**Theorem 43 (CTC Sample Complexity):**
For CTC with vocabulary size |Î£| and max length T, sample complexity:

n = Î©((|Î£|Â·TÂ·log T / ÎµÂ²)Â·log(1/Î´))

**Interpretation:**
- Linear in vocabulary size |Î£|
- Linear in sequence length T
- Requires log T factor for alignment uncertainty

**Comparison to frame-level classification:**
Frame-level: n = Î©((|Î£|Â·T/ÎµÂ²)Â·log(1/Î´))
CTC: Extra log T factor due to marginalization over alignments.

**Example:**
- Vocabulary: |Î£| = 30 (characters)
- Sequence length: T = 100
- Target error: Îµ = 0.1
- Confidence: Î´ = 0.05

CTC: n â‰ˆ 30Â·100Â·log(100)Â·100Â·3 â‰ˆ 4.1M sequences
Frame-level: n â‰ˆ 30Â·100Â·100Â·3 â‰ˆ 900K frames

CTC needs ~4-5Ã— more data due to alignment ambiguity!

**Use Cases:**
```
âœ“ Speech recognition (DeepSpeech, Wav2Vec)
âœ“ OCR (handwriting recognition)
âœ“ When: Input and output lengths differ
âœ“ When: Alignment is unknown
âœ“ When: Conditional independence acceptable
âœ— When: Need attention mechanism (use seq2seq instead)
âœ— When: Strong output dependencies (use autoregressive models)
âœ— When: Small dataset (CTC needs more data than frame-level)
```

---

## Generative Model Losses

### Variational Lower Bound (ELBO)

**Mathematical Form:**
```
For VAE with encoder q_Ï†(z|x) and decoder p_Î¸(x|z):

ELBO(Î¸,Ï†) = E_q[log p_Î¸(x|z)] - KL(q_Ï†(z|x) || p(z))

Equivalent forms:
= log p(x) - KL(q_Ï†(z|x) || p(z|x))
= E_q[log p_Î¸(x,z) - log q_Ï†(z|x)]

Loss: L = -ELBO (we minimize negative ELBO)
```

**Derivation:**
```
log p(x) = logâˆ« p(x,z) dz
         = logâˆ« p(x,z)Â·(q(z|x)/q(z|x)) dz
         = log E_q[p(x,z)/q(z|x)]
         â‰¥ E_q[log p(x,z)/q(z|x)]    (Jensen's inequality)
         = E_q[log p(x|z)] - KL(q(z|x) || p(z))
         = ELBO

Gap: KL(q(z|x) || p(z|x))
Tight when: q(z|x) = p(z|x) (intractable)
```

**Gradient Estimation:**
```
Problem: Can't backprop through z ~ q_Ï†(z|x)

Reparameterization trick:
z = Î¼_Ï†(x) + Ïƒ_Ï†(x) âŠ™ Îµ, where Îµ ~ N(0,I)

Now:
âˆ‡_Ï† E_q[f(z)] = âˆ‡_Ï† E_Îµ[f(Î¼_Ï†(x) + Ïƒ_Ï†(x) âŠ™ Îµ)]
              = E_Îµ[âˆ‡_Ï† f(Î¼_Ï†(x) + Ïƒ_Ï†(x) âŠ™ Îµ)]

Low variance, unbiased gradient!
```

**Î²-VAE:**
```
L = E_q[log p_Î¸(x|z)] - Î²Â·KL(q_Ï†(z|x) || p(z))

Î² > 1: Stronger regularization
- More disentangled representations
- Worse reconstruction

Î² < 1: Weaker regularization
- Better reconstruction
- Less disentangled

Trade-off between reconstruction and regularization
```

**Convergence Properties:**
```
ELBO optimization:

Properties:
- ELBO is lower bound on log p(x)
- Maximizing ELBO âŸ¹ good generative model
- Non-convex (neural networks)

Convergence:
- Alternating optimization (not simultaneous)
- Can use Adam/SGD
- Often reaches good local optimum

Challenges:
- KL vanishing: KL â†’ 0, model ignores z
- Solutions: KL annealing, free bits
```

**Use Cases:**
```
âœ“ Variational Autoencoders
âœ“ Bayesian inference approximation
âœ“ Latent variable models
âœ“ Semi-supervised learning
```

### GAN Losses

**Original GAN (Minimax):**
```
L_D = -E_real[log D(x)] - E_fake[log(1 - D(G(z)))]
L_G = -E_fake[log D(G(z))]

Minimax formulation:
min_G max_D V(D,G) = E_real[log D(x)] + E_fake[log(1-D(G(z)))]

Problems:
- Vanishing gradients for generator
- When D is optimal: log(1-D(G(z))) â‰ˆ 0
```

**Non-Saturating GAN:**
```
L_G = -E_fake[log D(G(z))]

Instead of minimizing log(1-D(G(z))), maximize log D(G(z))

Gradient:
- Original: -D'(G(z)) when D(G(z)) small (weak signal)
- Non-saturating: D'(G(z))/D(G(z)) (stronger)

Still minimax, just different G objective
```

**Wasserstein GAN (WGAN):**
```
L_D = -E_real[D(x)] + E_fake[D(G(z))]
L_G = -E_fake[D(G(z))]

where D is 1-Lipschitz

Advantages:
- Meaningful loss (correlates with quality)
- No saturation
- More stable training

Enforce Lipschitz:
- Weight clipping: clip Î¸ to [-c, c]
- Gradient penalty (WGAN-GP): better
```

**WGAN-GP (Gradient Penalty):**
```
L_D = -E_real[D(x)] + E_fake[D(G(z))]
      + Î»Â·E_xÌ‚[(||âˆ‡_xÌ‚ D(xÌ‚)||â‚‚ - 1)Â²]

where:
xÌ‚ = ÎµÂ·x_real + (1-Îµ)Â·x_fake, Îµ ~ U[0,1]
Î» = 10 typically

Gradient penalty enforces 1-Lipschitz constraint softly
```

**Convergence Analysis:**
```
Original GAN:
- Theoretically: Reaches Nash equilibrium (D*,G*)
- Practically: Oscillates, mode collapse, instability

WGAN:
- Theoretical guarantee: G â†’ data distribution
- Practically: More stable, slower

WGAN-GP:
- Best practical performance
- Lipschitz constraint well-enforced
- Typical: 5 critic steps per 1 generator step
```

#### Rigorous GAN Theory

**Theorem 44 (GAN Nash Equilibrium - Goodfellow et al., 2014):**
The minimax game:

min_G max_D V(D,G) = E_{x~p_data}[log D(x)] + E_{z~p_z}[log(1 - D(G(z)))]

has a **global optimum** at:
- D*(x) = p_data(x) / (p_data(x) + p_G(x))
- p_G = p_data (generator matches data distribution)

**Proof:**
For fixed G, optimal D* maximizes V(D,G):

V(D,G) = âˆ«_x [p_data(x)log D(x) + p_G(x)log(1-D(x))] dx

Taking derivative w.r.t. D(x) and setting to zero:
p_data(x)/D(x) - p_G(x)/(1-D(x)) = 0

Solving: D*(x) = p_data(x)/(p_data(x) + p_G(x))

Substituting back:
V(D*,G) = E_x[p_data log(p_data/(p_data+p_G)) + p_G log(p_G/(p_data+p_G))]
         = -log 4 + 2Â·JSD(p_data || p_G)

where JSD is Jensen-Shannon divergence.

Minimum when JSD(p_data || p_G) = 0, i.e., p_G = p_data. âˆ

**Theorem 45 (GAN Training Instability - Arjovsky & Bottou, 2017):**
When discriminator is optimal (or near-optimal), the generator gradient **vanishes**:

âˆ‡_Î¸ V(D*,G_Î¸) â‰ˆ âˆ‡_Î¸ 2Â·JSD(p_data || p_G) = 0

when p_data and p_G have **non-overlapping supports**.

**Proof (informal):**
If supp(p_data) âˆ© supp(p_G) = âˆ…:
- D can perfectly classify: D(x) = 1 for x ~ p_data, D(x) = 0 for x ~ p_G
- log(1 - D(G(z))) â‰ˆ log(1 - 0) = 0 (constant!)
- Generator gradient: âˆ‡_Î¸ log(1-D(G(z))) â‰ˆ 0

This happens in high dimensions where distributions are low-dimensional manifolds. âˆ

**Consequence:** Standard GAN training is inherently unstable!

**Theorem 46 (Wasserstein Distance - Arjovsky et al., 2017):**
The Wasserstein-1 (Earth Mover's) distance is:

W_1(p_data, p_G) = inf_{Î³âˆˆÎ (p_data,p_G)} E_{(x,y)~Î³}[||x - y||]

where Î (p_data, p_G) are all joint distributions with marginals p_data and p_G.

**Kantorovich-Rubinstein duality:**
W_1(p_data, p_G) = sup_{||f||_Lâ‰¤1} E_{x~p_data}[f(x)] - E_{x~p_G}[f(x)]

where ||f||_L â‰¤ 1 means f is 1-Lipschitz: |f(x) - f(y)| â‰¤ ||x - y||.

**WGAN objective:**
L_D = -E_{x~p_data}[D(x)] + E_{x~p_G}[D(x)]

approximates W_1 if D is 1-Lipschitz!

**Theorem 47 (WGAN Convergence Properties):**
Wasserstein distance has **better properties** than JS divergence:

(a) **Continuity:** W_1 is continuous everywhere
- JS divergence can be discontinuous (non-overlapping supports)

(b) **Weak convergence:** W_1(p_n, p) â†’ 0 implies p_n â‡€ p (weak convergence)
- JS(p_n || p) doesn't imply convergence!

(c) **Informative gradients:** Even when supports don't overlap:
âˆ‡_Î¸ W_1(p_data, p_G) â‰  0 (provides learning signal)

**Example (1D case):**
- p_data = Î´(x - 0) (point mass at 0)
- p_Î¸ = Î´(x - Î¸) (point mass at Î¸)

JS divergence:
- JS(p_data || p_Î¸) = log 2 for all Î¸ â‰  0 (constant! No gradient!)
- JS(p_data || p_0) = 0

Wasserstein:
- W_1(p_data, p_Î¸) = |Î¸| (linear! Always has gradient!)
- âˆ‡_Î¸ W_1 = sign(Î¸) (informative everywhere)

**Theorem 48 (Lipschitz Constraint Enforcement):**

(a) **Weight clipping (WGAN):**
Clip weights: W âˆˆ [-c, c]

Problems:
- Capacity reduction: limits function class
- Gradient pathology: pushes weights to boundaries
- Poor convergence: needs small c (â‰ˆ 0.01)

(b) **Gradient penalty (WGAN-GP):**
L_D = E_xÌƒ[(||âˆ‡_xÌƒ D(xÌƒ)||_2 - 1)Â²]

where xÌƒ = ÎµÂ·x_real + (1-Îµ)Â·x_fake, Îµ ~ U[0,1]

**Why interpolation?** Optimal discriminator has ||âˆ‡D|| = 1 almost everywhere
between real and fake samples.

**Theorem 49 (WGAN-GP Convergence Rate):**
With gradient penalty Î» and optimal critic steps per generator step r,
WGAN-GP converges at rate:

W_1(p_data, p_G) â‰¤ O(1/âˆšT) + O(1/r) + O(âˆšÎ»)

**Interpretation:**
- Need T = O(1/ÎµÂ²) steps for Îµ-optimal
- Need r = O(1/Îµ) critic steps per generator step
- Î» = 10 typical (tradeoff: too small â†’ poor Lipschitz, too large â†’ gradient penalty dominates)

**Practical recipe (Gulrajani et al., 2017):**
- r = 5 critic updates per 1 generator update
- Î» = 10 for gradient penalty
- Learning rate: Î± = 10^(-4) for both D and G
- Optimizer: Adam with Î²â‚ = 0, Î²â‚‚ = 0.9

**Theorem 50 (Mode Collapse Analysis - Metz et al., 2017):**
Mode collapse occurs when generator maps multiple noise vectors to same output:

G(zâ‚) = G(zâ‚‚) for zâ‚ â‰  zâ‚‚

**Quantification:** Effective number of modes captured:
N_eff = exp(H[p_G])

where H is entropy. Full diversity: N_eff = N_data.

**Causes:**
(a) **Generator gradient:** âˆ‡_Î¸ L_G focuses on fooling D, not diversity
(b) **Discriminator saturation:** D â†’ 0 or 1 â†’ no gradient signal
(c) **Nash equilibrium instability:** No guarantee of reaching equilibrium

**Solutions:**
(a) **Minibatch discrimination:** D sees multiple samples â†’ can detect mode collapse
(b) **Unrolled GAN:** Optimize G w.r.t. unrolled D (k steps ahead)
(c) **Spectral normalization:** Control Lipschitz constant of D

**Theorem 51 (Sample Complexity for GANs):**
To learn Îµ-optimal generator in W_1 distance with confidence 1-Î´:

n = Î©((d/ÎµÂ²)Â·log(1/Î´))

where d is ambient dimension.

**Comparison to density estimation:**
- Density estimation: n = Î©((d/Îµ)^d) (exponential in d!)
- GANs: n = Î©(d/ÎµÂ²) (polynomial!)

GANs can learn distributions in high dimensions with **polynomial sample complexity**!

**Caveat:** This is for W_1 distance. Other metrics may require more samples.

**Example:**
- Image dimension: d = 64Ã—64Ã—3 â‰ˆ 12,000
- Target distance: Îµ = 0.1
- Confidence: Î´ = 0.05

GAN: n â‰ˆ 12,000Â·100Â·3 â‰ˆ 3.6M images
Density estimation: n â‰ˆ (12,000Â·10)^12000 (intractable!)

**Theorem 52 (GAN vs VAE Trade-offs):**

**GANs:**
- Minimize: W_1(p_data, p_G) or JSD(p_data || p_G)
- Sample quality: High (sharp images)
- Sample diversity: Can suffer mode collapse
- Training: Unstable (adversarial)
- Likelihood: Cannot compute p_G(x)

**VAEs:**
- Maximize: ELBO â‰¤ log p(x)
- Sample quality: Lower (blurry images)
- Sample diversity: Good (covers all modes)
- Training: Stable (direct optimization)
- Likelihood: Can compute lower bound

**Formal difference:**
- GAN optimizes f-divergence (focuses on high-probability regions)
- VAE optimizes likelihood (penalizes all errors equally)

**Use Cases:**
```
Original GAN:
âœ— Hard to train, unstable

Non-saturating GAN:
âœ“ Better gradients, still can be unstable

WGAN/WGAN-GP:
âœ“ Image generation (best quality)
âœ“ More stable training
âœ“ Meaningful loss values (W_1 distance)
âœ“ State-of-the-art for GANs
âœ“ When: Sample quality > sample diversity

VAE:
âœ“ When: Need stable training
âœ“ When: Need likelihood estimates
âœ“ When: Sample diversity critical
```

---

## Convergence Properties Summary

### Convexity and Convergence Rates

```
Loss Type        | Convexity | GD Rate      | SGD Rate     | Notes
-----------------|-----------|--------------|--------------|------------------
MSE (Linear)     | Convex    | O(exp(-k))   | O(1/k)       | Strongly convex
MAE              | Convex    | O(1/âˆšk)      | O(1/âˆšk)      | Non-smooth
Huber            | Convex    | O(exp(-k))   | O(1/k)       | Smooth, robust
Cross-Entropy    | Convex*   | O(1/k)       | O(1/âˆšk)      | *if separable
Hinge            | Convex    | O(1/âˆšk)      | O(1/âˆšk)      | Non-smooth
Triplet          | Non-conv  | -            | O(1/âˆšk)      | Mining crucial
ELBO             | Non-conv  | -            | Local        | VAE
GAN              | Non-conv  | -            | Oscillates   | Nash equilibrium

where k = iteration number
```

### Lipschitz Constants

```
Loss                     | Lipschitz Constant L
-------------------------|--------------------
MSE                      | Î»_max(X^T X)
Cross-entropy            | 1/4 (for logistic)
Huber(Î´)                | 1 (in linear region)
Hinge                    | 1

For Gradient Descent: Î± â‰¤ 1/L for convergence
```

### Practical Convergence Tips

```
1. **MSE**: Fast convergence, watch for outliers
2. **Cross-Entropy**: Standard for classification, use with softmax
3. **Huber**: Best of both worlds for regression
4. **Focal**: Need more iterations than cross-entropy
5. **Triplet**: Mining strategy critical, start easy
6. **ELBO**: KL annealing often helps
7. **GAN**: Use WGAN-GP, multiple critic steps
```

---

## Loss Function Selection Guide

### Decision Tree

```
Task: Regression?
â”œâ”€ Yes: Outliers present?
â”‚  â”œâ”€ Yes: Use Huber or MAE
â”‚  â””â”€ No: Use MSE
â”‚
â””â”€ No (Classification)
   â”œâ”€ Binary or Multi-class?
   â”‚  â”œâ”€ Binary: Use Binary Cross-Entropy
   â”‚  â””â”€ Multi-class:
   â”‚     â”œâ”€ Exclusive classes? Use Categorical Cross-Entropy
   â”‚     â””â”€ Non-exclusive? Use Binary Cross-Entropy per class
   â”‚
   â”œâ”€ Class imbalance?
   â”‚  â”œâ”€ Severe (>100:1): Use Focal Loss
   â”‚  â””â”€ Moderate: Use weighted Cross-Entropy
   â”‚
   â”œâ”€ Need embeddings?
   â”‚  â””â”€ Use Triplet or Contrastive Loss
   â”‚
   â””â”€ Generative model?
      â”œâ”€ VAE: Use ELBO
      â”œâ”€ GAN: Use WGAN-GP
      â””â”€ Autoregressive: Use Cross-Entropy
```

### Quick Reference

```
Scenario                           | Recommended Loss
-----------------------------------|------------------
Image classification               | Cross-Entropy + Label Smoothing
Object detection                   | Focal Loss (RetinaNet) or Smooth L1 (Faster R-CNN)
Semantic segmentation              | Cross-Entropy or Dice Loss
Image regression                   | MSE or Huber
Face recognition                   | Triplet or ArcFace
Self-supervised learning           | Contrastive (SimCLR)
Speech recognition                 | CTC
Machine translation                | Cross-Entropy (teacher forcing)
VAE                               | ELBO (reconstruction + KL)
GAN                               | WGAN-GP
Reinforcement learning             | Policy Gradient + Value Loss (A2C/PPO)
Ranking/Retrieval                  | Triplet or Listwise
Anomaly detection                  | Reconstruction error (MSE/MAE)
Time series forecasting            | MSE, MAE, or Quantile Loss
Ordinal regression                 | Ordinal Cross-Entropy
Multi-task learning                | Weighted sum + uncertainty weighting
```

---

## Practical Implementations of Theory

### Computing Proper Scoring Rules

**Implementation of proper scoring rules and calibration metrics:**

```python
import numpy as np
from scipy.stats import chi2
from scipy.special import softmax

def log_score(y_true, y_pred_proba):
    """
    Logarithmic scoring rule (negative log-likelihood).
    Strictly proper scoring rule.

    Args:
        y_true: Binary labels (0 or 1), shape (n,)
        y_pred_proba: Predicted probabilities for class 1, shape (n,)

    Returns:
        score: Average log score (lower is better)
    """
    epsilon = 1e-15  # Avoid log(0)
    y_pred_proba = np.clip(y_pred_proba, epsilon, 1 - epsilon)

    scores = -np.where(y_true == 1,
                       np.log(y_pred_proba),
                       np.log(1 - y_pred_proba))

    return np.mean(scores)

def brier_score(y_true, y_pred_proba):
    """
    Brier score (mean squared error of probabilities).
    Strictly proper scoring rule.

    Args:
        y_true: Binary labels (0 or 1), shape (n,)
        y_pred_proba: Predicted probabilities for class 1, shape (n,)

    Returns:
        score: Brier score (lower is better)
    """
    return np.mean((y_pred_proba - y_true) ** 2)

def spherical_score(y_true, y_pred_proba):
    """
    Spherical scoring rule.
    Proper scoring rule that normalizes predictions.

    Args:
        y_true: Binary labels (0 or 1), shape (n,)
        y_pred_proba: Predicted probabilities for class 1, shape (n,)

    Returns:
        score: Negative spherical score (lower is better)
    """
    # For binary: p' = [1-p, p], y' = [1-y, y]
    p0 = 1 - y_pred_proba
    p1 = y_pred_proba
    norm = np.sqrt(p0**2 + p1**2)

    scores = np.where(y_true == 1, p1 / norm, p0 / norm)

    return -np.mean(scores)  # Negative because higher is better

def expected_calibration_error(y_true, y_pred_proba, n_bins=10):
    """
    Expected Calibration Error (ECE).
    Measures calibration by binning predictions.

    Args:
        y_true: True labels, shape (n,)
        y_pred_proba: Predicted probabilities, shape (n,)
        n_bins: Number of bins for calibration

    Returns:
        ece: Expected calibration error
        bin_stats: Dictionary with per-bin statistics
    """
    # Create bins
    bins = np.linspace(0, 1, n_bins + 1)
    bin_indices = np.digitize(y_pred_proba, bins) - 1
    bin_indices = np.clip(bin_indices, 0, n_bins - 1)

    ece = 0.0
    bin_stats = []

    for i in range(n_bins):
        mask = bin_indices == i

        if np.sum(mask) == 0:
            continue

        bin_size = np.sum(mask)
        bin_confidence = np.mean(y_pred_proba[mask])
        bin_accuracy = np.mean(y_true[mask])

        bin_error = np.abs(bin_confidence - bin_accuracy)
        ece += (bin_size / len(y_true)) * bin_error

        bin_stats.append({
            'bin': i,
            'count': bin_size,
            'confidence': bin_confidence,
            'accuracy': bin_accuracy,
            'error': bin_error
        })

    return ece, bin_stats

def maximum_calibration_error(y_true, y_pred_proba, n_bins=10):
    """
    Maximum Calibration Error (MCE).
    Worst-case calibration across bins.

    Args:
        y_true: True labels, shape (n,)
        y_pred_proba: Predicted probabilities, shape (n,)
        n_bins: Number of bins

    Returns:
        mce: Maximum calibration error
    """
    _, bin_stats = expected_calibration_error(y_true, y_pred_proba, n_bins)

    if len(bin_stats) == 0:
        return 0.0

    return max(stat['error'] for stat in bin_stats)

def temperature_scaling(logits, y_true, T_init=1.0, lr=0.01, max_iter=100):
    """
    Temperature scaling for model calibration.
    Optimizes temperature T to minimize NLL on validation set.

    Args:
        logits: Model logits before softmax, shape (n, n_classes)
        y_true: True class labels, shape (n,)
        T_init: Initial temperature
        lr: Learning rate
        max_iter: Maximum iterations

    Returns:
        T_optimal: Optimal temperature
        nll_history: NLL at each iteration
    """
    T = T_init
    nll_history = []

    for iteration in range(max_iter):
        # Compute probabilities with current temperature
        scaled_logits = logits / T
        probs = softmax(scaled_logits, axis=1)

        # Compute negative log-likelihood
        epsilon = 1e-15
        probs = np.clip(probs, epsilon, 1 - epsilon)
        nll = -np.mean(np.log(probs[np.arange(len(y_true)), y_true]))
        nll_history.append(nll)

        # Gradient of NLL w.r.t. T
        # âˆ‚NLL/âˆ‚T = (1/TÂ²) Î£_i (z_i - z_{y_i})
        one_hot = np.zeros_like(probs)
        one_hot[np.arange(len(y_true)), y_true] = 1

        grad_T = np.mean(np.sum((probs - one_hot) * logits, axis=1)) / (T ** 2)

        # Update temperature
        T -= lr * grad_T
        T = max(T, 0.01)  # Ensure T > 0

        # Check convergence
        if iteration > 0 and abs(nll_history[-1] - nll_history[-2]) < 1e-6:
            break

    return T, nll_history

# Example usage
if __name__ == "__main__":
    # Generate example data
    np.random.seed(42)
    n_samples = 1000

    # Ground truth labels
    y_true = np.random.binomial(1, 0.3, n_samples)

    # Overconfident predictions (miscalibrated)
    y_pred_overconfident = np.where(y_true == 1,
                                     np.random.beta(8, 2, n_samples),
                                     np.random.beta(2, 8, n_samples))

    # Well-calibrated predictions
    y_pred_calibrated = np.where(y_true == 1,
                                  np.random.beta(3, 2, n_samples),
                                  np.random.beta(2, 3, n_samples))

    print("=== Proper Scoring Rules ===")
    print(f"\nOverconfident model:")
    print(f"  Log score: {log_score(y_true, y_pred_overconfident):.4f}")
    print(f"  Brier score: {brier_score(y_true, y_pred_overconfident):.4f}")
    print(f"  Spherical score: {spherical_score(y_true, y_pred_overconfident):.4f}")

    print(f"\nCalibrated model:")
    print(f"  Log score: {log_score(y_true, y_pred_calibrated):.4f}")
    print(f"  Brier score: {brier_score(y_true, y_pred_calibrated):.4f}")
    print(f"  Spherical score: {spherical_score(y_true, y_pred_calibrated):.4f}")

    print("\n=== Calibration Metrics ===")
    ece_over, _ = expected_calibration_error(y_true, y_pred_overconfident)
    mce_over = maximum_calibration_error(y_true, y_pred_overconfident)

    ece_cal, _ = expected_calibration_error(y_true, y_pred_calibrated)
    mce_cal = maximum_calibration_error(y_true, y_pred_calibrated)

    print(f"\nOverconfident model:")
    print(f"  ECE: {ece_over:.4f}")
    print(f"  MCE: {mce_over:.4f}")

    print(f"\nCalibrated model:")
    print(f"  ECE: {ece_cal:.4f}")
    print(f"  MCE: {mce_cal:.4f}")
```

### Computing f-Divergences

**Implementation of various f-divergences:**

```python
import numpy as np
from scipy.special import kl_div, rel_entr

def compute_kl_divergence(P, Q, epsilon=1e-10):
    """
    Kullback-Leibler divergence KL(P || Q).

    Args:
        P: True distribution, shape (n,)
        Q: Approximate distribution, shape (n,)
        epsilon: Small value to avoid log(0)

    Returns:
        kl: KL divergence (nats)
    """
    P = np.asarray(P) + epsilon
    Q = np.asarray(Q) + epsilon

    # Normalize
    P = P / np.sum(P)
    Q = Q / np.sum(Q)

    return np.sum(rel_entr(P, Q))

def compute_reverse_kl(P, Q, epsilon=1e-10):
    """
    Reverse KL divergence KL(Q || P).

    Args:
        P: First distribution, shape (n,)
        Q: Second distribution, shape (n,)
        epsilon: Small value to avoid log(0)

    Returns:
        rkl: Reverse KL divergence (nats)
    """
    return compute_kl_divergence(Q, P, epsilon)

def compute_js_divergence(P, Q, epsilon=1e-10):
    """
    Jensen-Shannon divergence JS(P || Q).
    Symmetric and bounded version of KL divergence.

    Args:
        P: First distribution, shape (n,)
        Q: Second distribution, shape (n,)
        epsilon: Small value to avoid log(0)

    Returns:
        js: Jensen-Shannon divergence (nats)
    """
    P = np.asarray(P) + epsilon
    Q = np.asarray(Q) + epsilon

    # Normalize
    P = P / np.sum(P)
    Q = Q / np.sum(Q)

    # Mixture distribution
    M = 0.5 * (P + Q)

    return 0.5 * compute_kl_divergence(P, M, 0) + 0.5 * compute_kl_divergence(Q, M, 0)

def compute_hellinger_distance(P, Q, epsilon=1e-10):
    """
    Hellinger distance H(P, Q).
    Metric derived from squared Hellinger divergence.

    Args:
        P: First distribution, shape (n,)
        Q: Second distribution, shape (n,)
        epsilon: Small value for numerical stability

    Returns:
        hellinger: Hellinger distance [0, 1]
    """
    P = np.asarray(P) + epsilon
    Q = np.asarray(Q) + epsilon

    # Normalize
    P = P / np.sum(P)
    Q = Q / np.sum(Q)

    # Hellinger distance: (1/âˆš2)||âˆšP - âˆšQ||
    return np.sqrt(np.sum((np.sqrt(P) - np.sqrt(Q)) ** 2)) / np.sqrt(2)

def compute_total_variation(P, Q, epsilon=1e-10):
    """
    Total Variation distance TV(P, Q).

    Args:
        P: First distribution, shape (n,)
        Q: Second distribution, shape (n,)
        epsilon: Small value for numerical stability

    Returns:
        tv: Total variation distance [0, 1]
    """
    P = np.asarray(P) + epsilon
    Q = np.asarray(Q) + epsilon

    # Normalize
    P = P / np.sum(P)
    Q = Q / np.sum(Q)

    return 0.5 * np.sum(np.abs(P - Q))

def compute_chi_squared(P, Q, epsilon=1e-10):
    """
    Chi-squared divergence Ï‡Â²(P || Q).

    Args:
        P: True distribution, shape (n,)
        Q: Approximate distribution, shape (n,)
        epsilon: Small value to avoid division by zero

    Returns:
        chi_sq: Chi-squared divergence
    """
    P = np.asarray(P) + epsilon
    Q = np.asarray(Q) + epsilon

    # Normalize
    P = P / np.sum(P)
    Q = Q / np.sum(Q)

    return np.sum((P - Q) ** 2 / Q)

def compute_f_divergence(P, Q, f_func, epsilon=1e-10):
    """
    General f-divergence computation.

    Args:
        P: True distribution, shape (n,)
        Q: Approximate distribution, shape (n,)
        f_func: Convex function f(t) with f(1) = 0
        epsilon: Small value for numerical stability

    Returns:
        div: f-divergence
    """
    P = np.asarray(P) + epsilon
    Q = np.asarray(Q) + epsilon

    # Normalize
    P = P / np.sum(P)
    Q = Q / np.sum(Q)

    # Compute t = P(x)/Q(x) for each x
    t = P / Q

    # D_f(P || Q) = Î£ Q(x) f(P(x)/Q(x))
    return np.sum(Q * f_func(t))

# Example usage
if __name__ == "__main__":
    # Example distributions
    P = np.array([0.4, 0.3, 0.2, 0.1])
    Q = np.array([0.3, 0.3, 0.25, 0.15])

    print("=== f-Divergences between P and Q ===")
    print(f"P = {P}")
    print(f"Q = {Q}")
    print()

    print(f"KL(P || Q):         {compute_kl_divergence(P, Q):.6f} nats")
    print(f"KL(Q || P):         {compute_reverse_kl(P, Q):.6f} nats")
    print(f"JS(P || Q):         {compute_js_divergence(P, Q):.6f} nats")
    print(f"Hellinger(P, Q):    {compute_hellinger_distance(P, Q):.6f}")
    print(f"TV(P, Q):           {compute_total_variation(P, Q):.6f}")
    print(f"Ï‡Â²(P || Q):         {compute_chi_squared(P, Q):.6f}")

    print("\n=== Custom f-divergences ===")

    # KL: f(t) = t log t
    kl_custom = compute_f_divergence(P, Q, lambda t: t * np.log(t))
    print(f"KL (via f-div):     {kl_custom:.6f} nats")

    # Reverse KL: f(t) = -log t
    rkl_custom = compute_f_divergence(P, Q, lambda t: -np.log(t))
    print(f"Reverse KL (via f): {rkl_custom:.6f} nats")

    # Chi-squared: f(t) = (t - 1)Â²
    chi_custom = compute_f_divergence(P, Q, lambda t: (t - 1) ** 2)
    print(f"Ï‡Â² (via f-div):     {chi_custom:.6f}")
```

### Visualizing Surrogate Losses

**Compare surrogate losses and their relationship to 0-1 loss:**

```python
import numpy as np
import matplotlib.pyplot as plt

def zero_one_loss(margin):
    """0-1 loss: ğŸ™[margin â‰¤ 0]"""
    return (margin <= 0).astype(float)

def hinge_loss(margin):
    """Hinge loss: max(0, 1 - margin)"""
    return np.maximum(0, 1 - margin)

def logistic_loss(margin):
    """Logistic loss: log(1 + exp(-margin))"""
    return np.log(1 + np.exp(-margin))

def exponential_loss(margin):
    """Exponential loss: exp(-margin)"""
    return np.exp(-margin)

def squared_loss(margin):
    """Squared loss: (1 - margin)Â²"""
    return (1 - margin) ** 2

# Plot comparison
margin = np.linspace(-2, 3, 500)

plt.figure(figsize=(12, 7))

plt.plot(margin, zero_one_loss(margin), 'k-', linewidth=2, label='0-1 loss')
plt.plot(margin, hinge_loss(margin), 'r-', linewidth=2, label='Hinge')
plt.plot(margin, logistic_loss(margin), 'b-', linewidth=2, label='Logistic')
plt.plot(margin, exponential_loss(margin), 'g-', linewidth=2, label='Exponential')
plt.plot(margin, squared_loss(margin), 'm-', linewidth=2, label='Squared')

plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5)
plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)
plt.axvline(x=1, color='gray', linestyle=':', alpha=0.3, label='Margin = 1')

plt.xlabel('Margin (yÂ·f(x))', fontsize=12)
plt.ylabel('Loss', fontsize=12)
plt.title('Surrogate Losses for Binary Classification', fontsize=14)
plt.legend(fontsize=10)
plt.grid(True, alpha=0.3)
plt.ylim(-0.1, 3)
plt.xlim(-2, 3)

plt.tight_layout()
plt.savefig('surrogate_losses.png', dpi=300, bbox_inches='tight')
print("Saved surrogate_losses.png")

print("\n=== Loss Values at Key Margins ===")
margins_test = [-1, 0, 0.5, 1, 2]

for m in margins_test:
    print(f"\nMargin = {m}:")
    print(f"  0-1:         {zero_one_loss(np.array([m]))[0]:.4f}")
    print(f"  Hinge:       {hinge_loss(np.array([m]))[0]:.4f}")
    print(f"  Logistic:    {logistic_loss(np.array([m]))[0]:.4f}")
    print(f"  Exponential: {exponential_loss(np.array([m]))[0]:.4f}")
    print(f"  Squared:     {squared_loss(np.array([m]))[0]:.4f}")
```

## Custom Loss Functions

### Implementing Custom Losses

**PyTorch Template:**
```python
import torch
import torch.nn as nn

class CustomLoss(nn.Module):
    """Template for custom loss function"""

    def __init__(self, param1=1.0, param2=0.5):
        super().__init__()
        self.param1 = param1
        self.param2 = param2

    def forward(self, y_pred, y_true):
        """
        Args:
            y_pred: Model predictions (batch_size, ...)
            y_true: Ground truth (batch_size, ...)

        Returns:
            loss: Scalar loss value
        """
        # Implement loss computation
        loss = self.compute_loss(y_pred, y_true)

        return loss

    def compute_loss(self, y_pred, y_true):
        """Core loss computation"""
        # Example: Weighted MSE + MAE
        mse = torch.mean((y_pred - y_true) ** 2)
        mae = torch.mean(torch.abs(y_pred - y_true))

        loss = self.param1 * mse + self.param2 * mae
        return loss

# Usage
criterion = CustomLoss(param1=0.7, param2=0.3)
loss = criterion(predictions, targets)
loss.backward()
```

**TensorFlow/Keras Template:**
```python
import tensorflow as tf

def custom_loss(param1=1.0, param2=0.5):
    """Factory function for custom loss"""

    def loss_fn(y_true, y_pred):
        """
        Args:
            y_true: Ground truth tensor
            y_pred: Prediction tensor

        Returns:
            loss: Scalar tensor
        """
        # Ensure same shape
        y_true = tf.cast(y_true, y_pred.dtype)

        # Compute loss components
        mse = tf.reduce_mean(tf.square(y_pred - y_true))
        mae = tf.reduce_mean(tf.abs(y_pred - y_true))

        # Combine
        loss = param1 * mse + param2 * mae

        return loss

    return loss_fn

# Usage
model.compile(
    optimizer='adam',
    loss=custom_loss(param1=0.7, param2=0.3)
)
```

### Advanced: Composite Losses

**Example: Perceptual Loss (for Image Generation)**
```python
class PerceptualLoss(nn.Module):
    """Combines pixel-level and perceptual losses"""

    def __init__(self, feature_extractor, layer_weights=None):
        super().__init__()
        self.feature_extractor = feature_extractor
        self.layer_weights = layer_weights or [1.0, 1.0, 1.0]

        # Freeze feature extractor
        for param in self.feature_extractor.parameters():
            param.requires_grad = False

    def forward(self, generated, target):
        # Pixel loss (L1)
        pixel_loss = torch.mean(torch.abs(generated - target))

        # Feature loss
        gen_features = self.feature_extractor(generated)
        target_features = self.feature_extractor(target)

        feature_loss = 0
        for i, (gen_feat, target_feat) in enumerate(zip(gen_features, target_features)):
            feature_loss += self.layer_weights[i] * torch.mean(
                torch.abs(gen_feat - target_feat)
            )

        # Total loss
        total_loss = pixel_loss + 0.1 * feature_loss

        return total_loss, pixel_loss, feature_loss
```

### Validation and Testing

**Loss Sanity Checks:**
```python
def test_loss_function(loss_fn):
    """Test properties of custom loss"""

    # 1. Test gradient flow
    y_pred = torch.randn(10, 5, requires_grad=True)
    y_true = torch.randn(10, 5)

    loss = loss_fn(y_pred, y_true)
    loss.backward()

    assert y_pred.grad is not None, "No gradient!"
    print(f"âœ“ Gradient flows: {y_pred.grad.shape}")

    # 2. Test perfect prediction
    y_pred = y_true.clone().detach().requires_grad_(True)
    loss = loss_fn(y_pred, y_true)

    assert loss.item() < 1e-6, f"Loss should be ~0 for perfect pred: {loss.item()}"
    print(f"âœ“ Perfect prediction loss: {loss.item():.6f}")

    # 3. Test batch invariance
    single_loss = loss_fn(y_pred[:1], y_true[:1])
    batch_loss = loss_fn(y_pred, y_true)

    # Should be similar (unless using batch statistics)
    print(f"âœ“ Single vs batch loss: {single_loss.item():.4f} vs {batch_loss.item():.4f}")

    # 4. Test range
    random_pred = torch.randn_like(y_true)
    random_loss = loss_fn(random_pred, y_true)

    assert random_loss.item() > 0, "Loss should be positive"
    print(f"âœ“ Random prediction loss: {random_loss.item():.4f}")

# Test
test_loss_function(nn.MSELoss())
```

---

## References

### Regression Losses

1. **Huber, P. J.** (1964). "Robust estimation of a location parameter." *The Annals of Mathematical Statistics*, 35(1), 73-101.
   - Huber loss derivation

2. **Koenker, R., & Bassett Jr, G.** (1978). "Regression quantiles." *Econometrica*, 46(1), 33-50.
   - Quantile regression and pinball loss

### Classification Losses

3. **Bishop, C. M.** (2006). *Pattern recognition and machine learning*. Springer.
   - Cross-entropy and probabilistic losses

4. **Lin, T. Y., Goyal, P., Girshick, R., He, K., & DollÃ¡r, P.** (2017). "Focal loss for dense object detection." *ICCV*.
   - Focal loss for class imbalance

5. **MÃ¼ller, R., Kornblith, S., & Hinton, G. E.** (2019). "When does label smoothing help?" *NeurIPS*.
   - Label smoothing analysis

### Metric Learning

6. **Schroff, F., Kalinichenko, D., & Philbin, J.** (2015). "FaceNet: A unified embedding for face recognition and clustering." *CVPR*.
   - Triplet loss for face recognition

7. **Chen, T., Kornblith, S., Norouzi, M., & Hinton, G.** (2020). "A simple framework for contrastive learning of visual representations." *ICML*.
   - SimCLR contrastive loss

### Generative Models

8. **Kingma, D. P., & Welling, M.** (2014). "Auto-encoding variational bayes." *ICLR*.
   - ELBO and VAE

9. **Arjovsky, M., Chintala, S., & Bottou, L.** (2017). "Wasserstein generative adversarial networks." *ICML*.
   - Wasserstein loss for GANs

10. **Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., & Courville, A. C.** (2017). "Improved training of wasserstein gans." *NeurIPS*.
    - WGAN with gradient penalty

### Theoretical Foundations

11. **Goodfellow, I., Bengio, Y., & Courville, A.** (2016). *Deep learning*. MIT Press.
    - Comprehensive deep learning theory

12. **Bottou, L., Curtis, F. E., & Nocedal, J.** (2018). "Optimization methods for large-scale machine learning." *SIAM Review*, 60(2), 223-311.
    - Optimization and convergence analysis

---

*This comprehensive guide covers the mathematical foundations, convergence properties, and practical considerations for loss functions across all major machine learning tasks.*
