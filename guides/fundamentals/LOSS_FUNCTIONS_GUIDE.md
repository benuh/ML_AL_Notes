# Loss Functions: Complete Mathematical Guide

## Table of Contents
1. [Introduction to Loss Functions](#introduction)
2. [Regression Loss Functions](#regression)
3. [Classification Loss Functions](#classification)
4. [Probabilistic Loss Functions](#probabilistic)
5. [Ranking and Metric Learning](#ranking)
6. [Sequence-to-Sequence Losses](#sequence)
7. [Generative Model Losses](#generative)
8. [Convergence Properties](#convergence)
9. [Loss Function Selection Guide](#selection)
10. [Custom Loss Functions](#custom)

---

## Introduction to Loss Functions

### Mathematical Framework

**Loss Function Definition:**
```
Loss function L: Y √ó Y ‚Üí ‚Ñù‚Å∫

L(y_true, y_pred): Measures discrepancy between prediction and truth

Properties:
1. Non-negativity: L(y, ≈∑) ‚â• 0
2. Identity: L(y, y) = 0
3. Often (not always) symmetric or convex
```

**Risk Minimization:**
```
Empirical Risk: RÃÇ(f) = (1/n) Œ£·µ¢ L(y·µ¢, f(x·µ¢))
True Risk: R(f) = E_{(x,y)~P}[L(y, f(x))]

Goal: min_{f‚ààF} R(f)
Practice: min_{f‚ààF} RÃÇ(f) + Œª¬∑Œ©(f)

where Œ©(f) is regularization
```

**Gradient-Based Optimization:**
```
For parameter Œ∏:

‚àÇL/‚àÇŒ∏: Gradient of loss w.r.t. parameters

Update: Œ∏ ‚Üê Œ∏ - Œ±¬∑‚àÇL/‚àÇŒ∏

Convergence depends on:
- Convexity of L
- Lipschitz properties
- Smoothness
```

### Statistical Decision Theory

**Bayes Risk and Optimal Decision Rules**

The foundation of loss functions lies in statistical decision theory.

**Definition 1 (Decision Function):**
A decision function Œ¥: X ‚Üí A maps inputs to actions.
- For regression: A = ‚Ñù (predict real values)
- For binary classification: A = {0, 1} or A = [0, 1] (probabilities)
- For multi-class: A = Œî_K (probability simplex)

**Definition 2 (Risk):**
The risk of decision function Œ¥ under loss L:

R(Œ¥) = E_{(X,Y)~P}[L(Y, Œ¥(X))]

**Theorem 1 (Bayes Optimal Decision Rule):**
The Bayes optimal decision rule minimizes expected loss:

Œ¥*(x) = argmin_{a‚ààA} E_{Y|X=x}[L(Y, a)]

This is the best possible decision rule given the true distribution P(Y|X).

**Proof:**
By iterated expectation:

R(Œ¥) = E_X[E_{Y|X}[L(Y, Œ¥(X))]]

For each x, E_{Y|X=x}[L(Y, Œ¥(x))] depends only on Œ¥(x).
Minimizing R(Œ¥) requires minimizing the inner expectation for each x:

Œ¥*(x) = argmin_{a‚ààA} E_{Y|X=x}[L(Y, a)]

Any other choice Œ¥(x) ‚â† Œ¥*(x) increases R(Œ¥). ‚àé

**Corollary 1.1 (Bayes Risk):**
The Bayes risk R* is the minimum achievable risk:

R* = E_X[min_{a‚ààA} E_{Y|X}[L(Y, a)]]

No decision rule can achieve R(Œ¥) < R*.

**Excess Risk Decomposition:**
For any decision rule Œ¥:

R(Œ¥) - R* = E_X[E_{Y|X}[L(Y, Œ¥(X))] - min_a E_{Y|X}[L(Y, a)]]
          ‚â• 0

The excess risk measures sub-optimality of Œ¥.

**Example 1 (Squared Loss):**
For L(y, ≈∑) = (y - ≈∑)¬≤:

Œ¥*(x) = argmin_a E_{Y|X=x}[(Y - a)¬≤]
      = argmin_a E[(Y - E[Y|X=x] + E[Y|X=x] - a)¬≤]
      = argmin_a {Var[Y|X=x] + (E[Y|X=x] - a)¬≤}
      = E[Y|X=x]

Therefore: **MSE loss leads to predicting the conditional mean**.

**Example 2 (Absolute Loss):**
For L(y, ≈∑) = |y - ≈∑|:

Œ¥*(x) = argmin_a E_{Y|X=x}[|Y - a|]
      = median(Y|X=x)

Therefore: **MAE loss leads to predicting the conditional median**.

**Example 3 (0-1 Loss for Classification):**
For L(y, ≈∑) = ùüô[y ‚â† ≈∑]:

Œ¥*(x) = argmin_{c‚àà{1,...,K}} P(Y ‚â† c | X=x)
      = argmin_c (1 - P(Y = c | X=x))
      = argmax_c P(Y = c | X=x)

Therefore: **0-1 loss leads to predicting the most probable class**.

### Convexity Theory for Loss Functions

**Definition 3 (Convex Function):**
A function f: ‚Ñù‚Åø ‚Üí ‚Ñù is convex if for all x, y ‚àà ‚Ñù‚Åø and Œª ‚àà [0, 1]:

f(Œªx + (1-Œª)y) ‚â§ Œªf(x) + (1-Œª)f(y)

f is strictly convex if inequality is strict for Œª ‚àà (0, 1) and x ‚â† y.

**Definition 4 (Strong Convexity):**
f is Œº-strongly convex if for all x, y:

f(y) ‚â• f(x) + ‚àáf(x)·µÄ(y - x) + (Œº/2)||y - x||¬≤

Strong convexity implies unique global minimum.

**Theorem 2 (Convexity of Common Losses):**

(a) **MSE is convex:**
For L(w) = (1/n)Œ£·µ¢(w·µÄx·µ¢ - y·µ¢)¬≤, L is convex in w.

Proof: The Hessian is:
‚àá¬≤L(w) = (2/n)X·µÄX

This is positive semidefinite since for any v:
v·µÄ‚àá¬≤L(w)v = (2/n)v·µÄX·µÄXv = (2/n)||Xv||¬≤ ‚â• 0

If X has full column rank: ‚àá¬≤L = (2/n)X·µÄX ‚âª 0, so L is Œº-strongly convex
with Œº = (2/n)Œª_min(X·µÄX). ‚àé

(b) **Cross-entropy is convex:**
For logistic regression with L(w) = -Œ£·µ¢[y·µ¢log œÉ(w·µÄx·µ¢) + (1-y·µ¢)log(1-œÉ(w·µÄx·µ¢))],
where œÉ(z) = 1/(1+e^(-z)):

Proof: The Hessian is:
‚àá¬≤L(w) = Œ£·µ¢ œÉ(w·µÄx·µ¢)(1-œÉ(w·µÄx·µ¢)) x·µ¢x·µ¢·µÄ

Since œÉ(z)(1-œÉ(z)) > 0 for all z, this is a sum of positive semidefinite matrices,
hence positive semidefinite. Therefore L is convex. ‚àé

(c) **MAE is convex:**
For L(w) = Œ£·µ¢|w·µÄx·µ¢ - y·µ¢|, L is convex (as sum of convex functions |¬∑|).

(d) **Hinge loss is convex:**
For L(w) = Œ£·µ¢max(0, 1 - y·µ¢w·µÄx·µ¢), L is convex (max of affine functions).

**Theorem 3 (Smoothness and Lipschitz Continuity):**

(a) **L-smooth:** A differentiable function f is L-smooth if:
||‚àáf(x) - ‚àáf(y)|| ‚â§ L||x - y|| for all x, y

Equivalently: ‚àá¬≤f(x) ‚™Ø LI (all eigenvalues ‚â§ L)

(b) **For MSE:** If L(w) = (1/n)||Xw - y||¬≤, then:
‚àá¬≤L = (2/n)X·µÄX

L-smooth with L = (2/n)Œª_max(X·µÄX).

(c) **For logistic regression:** L-smooth with L = (1/4n)||X||¬≤_F
because max_z œÉ(z)(1-œÉ(z)) = 1/4.

**Theorem 4 (Convergence Rates):**

For Œº-strongly convex and L-smooth function, gradient descent with step size Œ± = 1/L:

||w_k - w*||¬≤ ‚â§ (1 - Œº/L)^k ||w_0 - w*||¬≤

where Œ∫ = L/Œº is the condition number.

Proof: By L-smoothness and Œº-strong convexity:

f(w_k) - f(w*) ‚â§ (1 - 1/Œ∫)(f(w_{k-1}) - f(w*))

This is linear (exponential) convergence with rate depending on Œ∫. ‚àé

**Practical Implications:**
- Well-conditioned problems (Œ∫ ‚âà 1): fast convergence
- Ill-conditioned problems (Œ∫ >> 1): slow convergence
- For linear regression: Œ∫ = Œª_max(X·µÄX)/Œª_min(X·µÄX)
- Regularization improves conditioning: Œ∫ decreases with Œª in L + Œª||w||¬≤

### Bregman Divergences

Many loss functions are Bregman divergences, providing a unified framework.

**Definition 5 (Bregman Divergence):**
For strictly convex, differentiable function œÜ:

D_œÜ(y || ≈∑) = œÜ(y) - œÜ(≈∑) - ‚ü®‚àáœÜ(≈∑), y - ≈∑‚ü©

This measures the "error" of approximating œÜ(y) by its first-order Taylor expansion at ≈∑.

**Properties:**
1. D_œÜ(y || ≈∑) ‚â• 0 with equality iff y = ≈∑
2. Generally not symmetric: D_œÜ(y || ≈∑) ‚â† D_œÜ(≈∑ || y)
3. Convex in second argument

**Theorem 5 (Common Losses as Bregman Divergences):**

(a) **Squared loss:** œÜ(y) = (1/2)y¬≤
D_œÜ(y || ≈∑) = (1/2)y¬≤ - (1/2)≈∑¬≤ - ≈∑(y - ≈∑)
            = (1/2)(y - ≈∑)¬≤

(b) **Generalized KL divergence:** œÜ(y) = y log y - y
D_œÜ(y || ≈∑) = y log(y/≈∑) - (y - ≈∑)  (for y, ≈∑ > 0)

(c) **Itakura-Saito divergence:** œÜ(y) = -log y
D_œÜ(y || ≈∑) = y/≈∑ - log(y/≈∑) - 1

**Theorem 6 (Bregman Projection):**
The Bregman projection onto convex set C:

P_C(y) = argmin_{≈∑‚ààC} D_œÜ(y || ≈∑)

satisfies the generalized Pythagorean theorem:

D_œÜ(y || z) = D_œÜ(y || P_C(y)) + D_œÜ(P_C(y) || z)  for all z ‚àà C

This generalizes ordinary Euclidean projection (when œÜ(y) = (1/2)||y||¬≤).

---

## Regression Loss Functions

### Mean Squared Error (MSE) / L2 Loss

**Mathematical Form:**
```
L(y, ≈∑) = (1/n) Œ£·µ¢ (y·µ¢ - ≈∑·µ¢)¬≤

Alternative forms:
- Sum of squared errors: Œ£·µ¢ (y·µ¢ - ≈∑·µ¢)¬≤
- Root MSE: ‚àö[(1/n) Œ£·µ¢ (y·µ¢ - ≈∑·µ¢)¬≤]
```

**Gradient:**
```
‚àÇL/‚àÇ≈∑·µ¢ = 2(≈∑·µ¢ - y·µ¢)

Properties:
- Linear gradient in error
- Differentiable everywhere
- Convex
```

**Statistical Interpretation:**
```
Maximum Likelihood Estimation under Gaussian noise:

Assume: y = f(x) + Œµ, where Œµ ~ N(0, œÉ¬≤)

Likelihood: P(y|x) = (1/‚àö(2œÄœÉ¬≤)) exp(-(y - f(x))¬≤/(2œÉ¬≤))

Log-likelihood: log P(y|x) = -(y - f(x))¬≤/(2œÉ¬≤) + const

Maximizing log-likelihood ‚ü∫ Minimizing MSE!
```

**Convergence Properties:**
```
For linear regression y = Xw:

Loss: L(w) = ||Xw - y||¬≤

Properties:
- Strongly convex (for X with full column rank)
- Unique global minimum: w* = (X^T X)^(-1) X^T y
- Gradient Lipschitz with constant L = Œª_max(X^T X)

Convergence rate (Gradient Descent):
- With step size Œ± = 1/L:
  ||w_k - w*||¬≤ ‚â§ (1 - Œº/L)^k ||w_0 - w*||¬≤

  where Œº = Œª_min(X^T X) (strong convexity parameter)

- Condition number: Œ∫ = L/Œº = Œª_max/Œª_min
- Linear convergence: O((1 - 1/Œ∫)^k)
```

**Robustness:**
```
Sensitivity to outliers: HIGH

Example:
True values: [1, 2, 3, 4, 5]
Predictions: [1.1, 2.1, 3.1, 4.1, 5.1]
MSE = 0.01 (good)

With one outlier:
True values: [1, 2, 3, 4, 100]
Predictions: [1.1, 2.1, 3.1, 4.1, 5.1]
MSE = 1801.01 (terrible!)

Outlier dominates loss due to squaring
```

**Use Cases:**
```
‚úì When: Errors are normally distributed
‚úì When: Outliers are rare and should be heavily penalized
‚úì When: Need smooth gradients
‚úó When: Data has outliers (use Huber or MAE instead)
```

**MSE vs MAE: Decision guide**

**Choose MSE when:**
1. **Gaussian noise assumption holds:** Real-world data often has Gaussian errors (measurement noise, sensor noise)
2. **Large errors are qualitatively different:** Predicting $100K vs $50K for house price is worse than $51K vs $50K
3. **Want smooth optimization:** MSE gradients are continuous and smooth everywhere
4. **Training stability matters:** Constant-magnitude gradients of MAE can cause issues with learning rate

**Choose MAE when:**
1. **Outliers exist and are NOT errors:** Legitimate extreme values that shouldn't dominate loss
2. **Want median prediction:** MSE ‚Üí mean, MAE ‚Üí median (median more robust)
3. **All errors equally bad:** Missing by $10K or $100K both count as one failed prediction
4. **Interpretable loss:** MAE in same units as target (average error)

**Practical example (house prices):**
- Dataset: 100 houses, 99 priced $200K-$300K, 1 mansion at $5M
- Model predicting $250K for everything:
  - MSE: Dominated by $5M outlier, loss ‚âà 22M¬≤, pushes model to overpredict
  - MAE: All errors weighted equally, loss ‚âà $50K average, robust to outlier
- If mansion is legitimate (not error): Use MAE
- If mansion is data entry error: Use MSE or remove outlier

### Mean Absolute Error (MAE) / L1 Loss

**Mathematical Form:**
```
L(y, ≈∑) = (1/n) Œ£·µ¢ |y·µ¢ - ≈∑·µ¢|
```

**Gradient:**
```
‚àÇL/‚àÇ≈∑·µ¢ = sign(≈∑·µ¢ - y·µ¢)

Properties:
- Constant gradient magnitude
- Not differentiable at ≈∑·µ¢ = y·µ¢ (use subgradient)
- Convex
```

**Statistical Interpretation:**
```
MLE under Laplace distribution:

Assume: y = f(x) + Œµ, where Œµ ~ Laplace(0, b)

P(Œµ) = (1/2b) exp(-|Œµ|/b)

Log-likelihood: -|y - f(x)|/b + const

Maximizing ‚ü∫ Minimizing MAE
```

**Convergence Properties:**
```
For linear model:

L(w) = Œ£·µ¢ |x·µ¢^T w - y·µ¢|

Properties:
- Convex but not differentiable at solution points
- Subgradient method required
- No closed-form solution (unlike MSE)

Convergence rate (Subgradient Descent):
- Best iterate after k steps:
  f(w_best) - f(w*) ‚â§ O(1/‚àök)

- Slower than smooth losses
- Step size: Œ±_k = Œ±_0/‚àök (diminishing)

Coordinate descent:
- Often more efficient for L1
- Soft-thresholding operator
```

**Robustness:**
```
Sensitivity to outliers: LOW

Example with outlier:
True values: [1, 2, 3, 4, 100]
Predictions: [1.1, 2.1, 3.1, 4.1, 5.1]

MAE = (0.1 + 0.1 + 0.1 + 0.1 + 94.9)/5 = 19.06

vs MSE = 1801.01

MAE less affected by outlier (no squaring)
```

**Use Cases:**
```
‚úì When: Outliers present in data
‚úì When: Median regression desired (MAE ‚Üí median, MSE ‚Üí mean)
‚úì When: All errors equally important
‚úó When: Need smooth gradients (use Huber instead)
‚úó When: Want to heavily penalize large errors
```

### Huber Loss

**Mathematical Form:**
```
L_Œ¥(y, ≈∑) = {
  ¬Ω(y - ≈∑)¬≤               if |y - ≈∑| ‚â§ Œ¥
  Œ¥|y - ≈∑| - ¬ΩŒ¥¬≤          if |y - ≈∑| > Œ¥
}

Combines MSE (for small errors) and MAE (for large errors)
```

**Gradient:**
```
‚àÇL_Œ¥/‚àÇ≈∑ = {
  (≈∑ - y)           if |y - ≈∑| ‚â§ Œ¥
  Œ¥¬∑sign(≈∑ - y)     if |y - ≈∑| > Œ¥
}

Properties:
- Differentiable everywhere (unlike MAE)
- Linear gradient for large errors (unlike MSE)
- Convex
- Œ¥: transition point (hyperparameter)
```

**Convergence Properties:**
```
Properties:
- Strongly convex in neighborhood of minimum
- Lipschitz continuous gradient
- Smooth everywhere

Convergence rate:
- In convex region: Linear convergence (like MSE)
- Overall: Better than MAE, similar to MSE
- Less sensitive to Œ¥ choice than might be expected

Optimal Œ¥:
- Œ¥ ‚âà 1.345œÉ where œÉ is noise standard deviation
- Makes Huber ~95% as efficient as MSE under Gaussian noise
- Much more robust to outliers
```

**Robustness:**
```
Sensitivity: MEDIUM (between MSE and MAE)

Œ¥ = 1.0 example:
Small errors (|e| < 1): L = 0.5e¬≤ (smooth, like MSE)
Large errors (|e| > 1): L = |e| - 0.5 (robust, like MAE)

Best of both worlds:
- Smooth gradients near optimum
- Robust to outliers
```

**Use Cases:**
```
‚úì When: Data has some outliers but want smooth training
‚úì When: Need balance between MSE and MAE
‚úì When: Regression with heteroscedastic noise
‚úì Most versatile regression loss
```

### Quantile Loss / Pinball Loss

**Mathematical Form:**
```
L_œÑ(y, ≈∑) = {
  œÑ(y - ≈∑)        if y ‚â• ≈∑
  (1-œÑ)(≈∑ - y)    if y < ≈∑
}

where œÑ ‚àà (0, 1) is the quantile

Special cases:
- œÑ = 0.5: MAE (median)
- œÑ = 0.9: 90th percentile
```

**Gradient:**
```
‚àÇL_œÑ/‚àÇ≈∑ = {
  -œÑ        if y ‚â• ≈∑
  (1-œÑ)     if y < ≈∑
}

Asymmetric penalties:
- Underestimation (y > ≈∑): weighted by œÑ
- Overestimation (y < ≈∑): weighted by (1-œÑ)
```

**Statistical Interpretation:**
```
Minimizing L_œÑ(y, ≈∑) gives œÑ-th quantile of P(y|x)

Example (œÑ = 0.9):
- Prediction is 90th percentile
- 90% of observed values below prediction
- Useful for risk assessment, inventory planning
```

**Use Cases:**
```
‚úì Quantile regression
‚úì Confidence interval estimation
‚úì Risk-sensitive prediction
‚úì Asymmetric cost functions
```

### Log-Cosh Loss

**Mathematical Form:**
```
L(y, ≈∑) = Œ£·µ¢ log(cosh(≈∑·µ¢ - y·µ¢))

where cosh(x) = (e^x + e^(-x))/2
```

**Gradient:**
```
‚àÇL/‚àÇ≈∑·µ¢ = tanh(≈∑·µ¢ - y·µ¢)

Properties:
- Smooth everywhere (unlike Huber)
- Approximately MSE for small errors
- Approximately MAE for large errors
- No hyperparameter (unlike Huber's Œ¥)
```

**Approximations:**
```
For small |x|: log(cosh(x)) ‚âà x¬≤/2 (like MSE)
For large |x|: log(cosh(x)) ‚âà |x| - log 2 (like MAE)

Automatic transition between quadratic and linear
```

**Use Cases:**
```
‚úì When: Want Huber-like behavior without tuning Œ¥
‚úì When: Need smoothness everywhere
‚úì When: Outlier robustness with simple implementation
```

---

## Classification Loss Functions

### Proper Scoring Rules and Calibration

Before discussing specific classification losses, we establish the theory of proper scoring rules.

**Definition 6 (Scoring Rule):**
A scoring rule S(P, y) measures the quality of probabilistic forecast P when outcome is y.
- P: predicted probability distribution over outcomes
- y: realized outcome
- Lower score = better prediction

**Definition 7 (Proper Scoring Rule):**
A scoring rule S is **proper** if the expected score is minimized by predicting the true distribution:

argmin_Q E_{Y~P}[S(Q, Y)] = P

It is **strictly proper** if P is the unique minimizer.

**Theorem 7 (Cross-Entropy is Proper):**
The logarithmic scoring rule S(P, y) = -log P(y) is strictly proper.

Proof:
Let P be the true distribution. For any other distribution Q:

E_{Y~P}[S(Q, Y)] = E_{Y~P}[-log Q(Y)]
                  = -Œ£_y P(y) log Q(y)
                  = H(P, Q)  (cross-entropy)

We want to show: H(P, Q) ‚â• H(P, P) with equality iff Q = P.

H(P, Q) - H(P, P) = -Œ£_y P(y) log Q(y) + Œ£_y P(y) log P(y)
                  = Œ£_y P(y) log(P(y)/Q(y))
                  = KL(P || Q)
                  ‚â• 0

with equality iff P = Q (by non-negativity of KL divergence). ‚àé

**Corollary 7.1:**
Minimizing cross-entropy loss encourages calibrated probability predictions.

**Theorem 8 (Brier Score is Proper):**
The Brier score S(p, y) = (p - y)¬≤ for binary outcomes y ‚àà {0, 1} is strictly proper.

Proof:
For true probability P(Y=1) = œÄ, and predicted probability p:

E_Y[S(p, Y)] = œÄ(p - 1)¬≤ + (1-œÄ)(p - 0)¬≤
              = œÄ(p¬≤ - 2p + 1) + (1-œÄ)p¬≤
              = p¬≤(œÄ + 1 - œÄ) - 2œÄp + œÄ
              = p¬≤ - 2œÄp + œÄ

Taking derivative w.r.t. p:
‚àÇ/‚àÇp E[S(p, Y)] = 2p - 2œÄ

Setting to zero: p = œÄ (the true probability).

Second derivative: ‚àÇ¬≤/‚àÇp¬≤ E[S(p, Y)] = 2 > 0, confirming minimum. ‚àé

**Theorem 9 (Characterization of Proper Scoring Rules):**
A scoring rule S(p, y) for binary outcomes is proper if and only if it can be written as:

S(p, 1) = -G(p)
S(p, 0) = -G(1-p) - p¬∑G'(p)

where G is a strictly convex function with G' being its derivative.

This gives a general family of proper scoring rules based on choice of G.

**Examples:**
- Log score: G(p) = log p (strictly convex for p > 0)
- Brier score: G(p) = 2p - p¬≤ (strictly convex)
- Spherical score: G(p) = 1/‚àö(p¬≤ + (1-p)¬≤)

### Calibration Theory

**Definition 8 (Calibration):**
A classifier with predicted probabilities p is **calibrated** if:

P(Y = 1 | p(X) = q) = q  for all q ‚àà [0, 1]

In words: among all predictions with confidence q, exactly fraction q should be correct.

**Example (Well-calibrated):**
- 100 predictions with p = 0.7
- Exactly 70 should have Y = 1
- If 70/100 are correct ‚Üí well calibrated
- If 50/100 are correct ‚Üí underconfident (miscalibrated)
- If 90/100 are correct ‚Üí overconfident (miscalibrated)

**Theorem 10 (Expected Calibration Error):**
The Expected Calibration Error (ECE) measures calibration:

ECE = Œ£_{m=1}^M (n_m/n)|acc(m) - conf(m)|

where:
- Predictions binned into M bins by confidence
- n_m: number of predictions in bin m
- acc(m): accuracy in bin m
- conf(m): average confidence in bin m

Perfect calibration: ECE = 0.

**Theorem 11 (Temperature Scaling for Calibration):**
Post-hoc calibration via temperature scaling:

p_calibrated = softmax(z/T)

where z are logits and T > 0 is temperature.

Finding optimal T:
T* = argmin_T NLL(softmax(z/T), y) on validation set

This preserves model accuracy while improving calibration.

**Proof of effectiveness:**
Temperature scaling is a monotonic transformation of probabilities:
- Preserves argmax ‚Üí same predicted classes
- T > 1: reduces confidence (spreads probability mass)
- T < 1: increases confidence (concentrates probability mass)
- Optimizing T via NLL finds best calibration on validation data. ‚àé

### Surrogate Loss Bounds

The 0-1 loss L_{0-1}(y, f(x)) = ùüô[y ‚â† sign(f(x))] is non-convex and discontinuous.
Classification algorithms use **surrogate losses** that are convex and differentiable.

**Definition 9 (œÜ-surrogate Loss):**
A surrogate loss œÜ: ‚Ñù ‚Üí ‚Ñù‚Çä replaces 0-1 loss:

L_œÜ(y, f(x)) = œÜ(y¬∑f(x))

where y ‚àà {-1, +1} and f(x) ‚àà ‚Ñù is the margin.

**Common Surrogates:**
- 0-1 loss: œÜ(z) = ùüô[z ‚â§ 0]
- Hinge: œÜ(z) = max(0, 1 - z)
- Logistic: œÜ(z) = log(1 + e^(-z))
- Exponential: œÜ(z) = e^(-z)
- Squared: œÜ(z) = (1 - z)¬≤

**Theorem 12 (Classification Calibration):**
A surrogate œÜ is **classification-calibrated** if minimizing œÜ-risk implies minimizing 0-1 risk:

R_œÜ(f_n) ‚Üí inf_f R_œÜ(f)  ‚üπ  R_{0-1}(f_n) ‚Üí inf_f R_{0-1}(f)

**Sufficient condition:** œÜ is differentiable, convex, and œÜ'(0) < 0.

All common surrogates (hinge, logistic, exponential) satisfy this.

**Theorem 13 (Bartlett-Jordan-McAuliffe Bound):**
For margin-based surrogates with œÜ convex, decreasing, and œÜ(0) = 1:

R_{0-1}(f) - R*_{0-1} ‚â§ œà_œÜ^(-1)(R_œÜ(f) - R*_œÜ)

where œà_œÜ is the calibration function measuring how well œÜ bounds 0-1 loss.

**Explicit bounds:**

(a) **Hinge loss:** œÜ(z) = max(0, 1-z)
R_{0-1}(f) - R* ‚â§ R_œÜ(f) - R*_œÜ

(b) **Logistic loss:** œÜ(z) = log(1 + e^(-z))
R_{0-1}(f) - R* ‚â§ ‚àö(2(R_œÜ(f) - R*_œÜ))

(c) **Exponential loss:** œÜ(z) = e^(-z)
R_{0-1}(f) - R* ‚â§ ‚àö(2(R_œÜ(f) - R*_œÜ))

**Interpretation:**
- Minimizing surrogate œÜ-risk yields low 0-1 risk
- Logistic and exponential: square root relationship (faster than linear)
- Hinge: linear relationship (tightest bound)

**Theorem 14 (H-Consistency Bounds):**
For hypothesis class H, a loss is **H-consistent** if minimizing empirical œÜ-risk over H
yields optimal 0-1 risk over H.

For finite VC dimension d and n samples:

R_{0-1}(fÃÇ) ‚â§ R*_{0-1} + O(‚àö(d/n)) + œà_œÜ^(-1)(RÃÇ_œÜ(fÃÇ) - R_œÜ(f*) + O(‚àö(d/n)))

This combines:
- Approximation error: R_œÜ(f*) - R*_œÜ
- Estimation error: RÃÇ_œÜ(fÃÇ) - R_œÜ(fÃÇ)
- Calibration gap: œà_œÜ^(-1)

### Cross-Entropy Loss (Log Loss)

**Binary Classification:**
```
L(y, ≈∑) = -[y log(≈∑) + (1-y) log(1-≈∑)]

where:
- y ‚àà {0, 1}: true label
- ≈∑ ‚àà [0, 1]: predicted probability
```

**Multi-Class (Categorical Cross-Entropy):**
```
L(y, ≈∑) = -Œ£_c y_c log(≈∑_c)

where:
- y: one-hot encoded true label [0,0,1,0,...,0]
- ≈∑: predicted probabilities [0.1, 0.2, 0.6, 0.1, ...]
- Œ£_c y_c = 1, Œ£_c ≈∑_c = 1
```

**Gradient (with Softmax):**
```
Combined softmax + cross-entropy:

Softmax: ≈∑_i = exp(z_i) / Œ£_j exp(z_j)

Gradient: ‚àÇL/‚àÇz_i = ≈∑_i - y_i

Remarkably simple! This is why softmax + cross-entropy is standard.

Derivation:
‚àÇL/‚àÇz_i = Œ£_c (‚àÇL/‚àÇ≈∑_c)(‚àÇ≈∑_c/‚àÇz_i)
        = -Œ£_c (y_c/≈∑_c)(‚àÇ≈∑_c/‚àÇz_i)

For softmax:
‚àÇ≈∑_j/‚àÇz_i = ≈∑_j(Œ¥_ij - ≈∑_i)

Combining:
‚àÇL/‚àÇz_i = -Œ£_c (y_c/≈∑_c)¬∑≈∑_c¬∑(Œ¥_ic - ≈∑_i)
        = -y_i + ≈∑_i¬∑Œ£_c y_c
        = ≈∑_i - y_i  (since Œ£_c y_c = 1)
```

**Why this gradient is elegant and perfect:**

1. **Intuitive form:** Gradient = prediction error
   - If ≈∑_i = 0.9 and y_i = 1 (correct class): gradient = -0.1 (small correction)
   - If ≈∑_i = 0.1 and y_i = 1: gradient = -0.9 (large correction)
   - Automatic adjustment based on confidence error

2. **No vanishing gradient problem:**
   - Gradient magnitude = |≈∑_i - y_i|
   - Even when completely wrong (≈∑_i ‚Üí 0 for true class), gradient stays bounded
   - Compare to squared loss: ‚àÇ(≈∑ - y)¬≤/‚àÇz = 2(≈∑ - y)¬∑≈∑(1-≈∑) ‚Üí vanishes when ≈∑ ‚Üí 0 or 1
   - Cross-entropy: Always provides learning signal when wrong

3. **Natural cancellation of complex terms:**
   - Softmax derivative involves all classes (Jacobian)
   - Cross-entropy derivative has division by prediction
   - Together they magically simplify to ≈∑_i - y_i
   - No exponentials, no divisions in final gradient!

4. **Probabilistic interpretation:**
   - Gradient points in direction of steepest KL divergence reduction
   - Matches Fisher information matrix for efficient learning
   - Statistically optimal for categorical distributions

**Example showing non-saturation:**
```
True class: i=2, y = [0, 0, 1, 0]
Logits: z = [-5, -5, -5, 1] (very wrong! predicting class 3)

Softmax: ≈∑ ‚âà [0.002, 0.002, 0.002, 0.994]

Gradient w.r.t. z‚ÇÇ (true class): 0.002 - 1 = -0.998
Large negative gradient ‚Üí increase z‚ÇÇ strongly ‚úì

Compare if we used squared loss (≈∑ - y)¬≤:
‚àÇ/‚àÇz‚ÇÇ = 2(≈∑‚ÇÇ - 1)¬∑≈∑‚ÇÇ¬∑(1-≈∑‚ÇÇ) ‚âà 2(-0.998)¬∑0.002¬∑0.998 ‚âà -0.004
Tiny gradient due to ≈∑‚ÇÇ ‚âà 0 ‚Üí training stalls! ‚úó
```

**Statistical Interpretation:**
```
Maximum Likelihood under Categorical distribution:

P(y = c | x) = ≈∑_c

Likelihood: L = Œ†_i P(y_i | x_i)
Log-likelihood: Œ£_i log P(y_i | x_i) = Œ£_i log ≈∑_{y_i}

Negative log-likelihood = Cross-entropy

Minimizing cross-entropy ‚ü∫ Maximum likelihood estimation
```

**Convergence Properties:**
```
For logistic regression (binary):

z = w^T x + b
≈∑ = œÉ(z) = 1/(1 + e^(-z))
L(w) = -Œ£_i [y_i log œÉ(w^T x_i) + (1-y_i) log(1-œÉ(w^T x_i))]

Properties:
- Convex in w (assuming linearly separable or regularized)
- Gradient: ‚àáL(w) = Œ£_i (≈∑_i - y_i) x_i
- Smooth (unlike hinge loss)

Convergence rate (GD):
- Convex case: O(1/k) for Œµ-accuracy
- Locally strongly convex: O(log(1/Œµ))

**Numerical stability:**
Critical issue: Direct computation causes overflow/underflow.

**Problem:**
```
Softmax: ≈∑_i = exp(z_i) / Œ£_j exp(z_j)

For z = [1000, 1001, 1002]:
exp(1000) ‚âà 10^434 ‚Üí overflow!
exp(-1000) ‚âà 10^-434 ‚Üí underflow!
```

**Solution - Log-Sum-Exp trick:**
```
log(Œ£ exp(z_i)) = c + log(Œ£ exp(z_i - c))

where c = max_i(z_i)

Implementation:
1. Compute c = max(z)
2. Compute z' = z - c  (now max(z') = 0)
3. Compute log(Œ£ exp(z'_i))
4. Result = c + log(Œ£ exp(z'_i))

For z = [1000, 1001, 1002]:
z' = [0, 1, 2]  ‚úì (stable!)
exp(z') = [1, 2.718, 7.389]  ‚úì
```

**Softmax stability:**
```
Numerically stable implementation:
z' = z - max(z)
≈∑ = exp(z') / sum(exp(z'))

This ensures:
- Largest logit becomes 0 ‚Üí exp(0) = 1 (stable)
- All other logits are negative ‚Üí exp(negative) ‚àà (0,1) (stable)
- Denominator ‚â• 1 (no division by tiny numbers)
```

**Cross-entropy stability:**
```
Avoid: L = -log(softmax(z)_y)

Use: L = -z_y + log_sum_exp(z)

This combines operations for better numerical precision.
```
```

**Behavior Analysis:**
```
For binary case:

When y = 1:
- ≈∑ ‚Üí 1: L ‚Üí 0 (correct, confident)
- ≈∑ ‚Üí 0.5: L = -log(0.5) = 0.69 (uncertain)
- ≈∑ ‚Üí 0: L ‚Üí ‚àû (wrong, confident - heavily penalized!)

Gradient magnitude:
|‚àÇL/‚àÇz| = |≈∑ - y|

Properties:
- Large gradient when very wrong (fast correction)
- Small gradient when correct (stability)
- Never saturates when wrong (unlike squared error)
```

**Use Cases:**
```
‚úì Multi-class classification (standard choice)
‚úì Binary classification
‚úì Probability estimation tasks
‚úì When: Labels are mutually exclusive
‚úì When: Need calibrated probabilities
‚úó When: Labels are not exclusive (use BCE with logits)
```

### Hinge Loss (SVM Loss)

**Mathematical Form:**
```
Binary classification (y ‚àà {-1, +1}):

L(y, z) = max(0, 1 - y¬∑z)

where z = w^T x + b (decision function, not probability)

Multi-class (SVM):
L(y, z) = Œ£_{j‚â†y} max(0, z_j - z_y + Œî)

where Œî is margin (typically Œî = 1)
```

**Gradient:**
```
‚àÇL/‚àÇz = {
  -y    if y¬∑z < 1
  0     if y¬∑z ‚â• 1
}

Properties:
- Non-differentiable at y¬∑z = 1 (use subgradient)
- Zero gradient in margin region
- Constant gradient outside margin
```

**Geometric Interpretation:**
```
Decision boundary: w^T x + b = 0

Margin region: |w^T x + b| ‚â§ 1

Hinge loss:
- Zero for points correctly classified with margin > 1
- Linear penalty for points in margin or misclassified

Goal: Maximize margin while allowing few violations
```

**Convergence Properties:**
```
SVM optimization:

min_{w,b} Œª||w||¬≤ + (1/n)Œ£·µ¢ max(0, 1 - y·µ¢(w^T x·µ¢ + b))

Properties:
- Convex
- Non-smooth (has corners)
- Regularization Œª||w||¬≤ makes it strongly convex

**Convergence (Subgradient Descent):**
- Rate: O(1/‚àök) for non-smooth convex functions
- After k iterations: f(wÃÑ_k) - f(w*) ‚â§ R¬∑G/‚àök
  - R = ||w_0 - w*|| (initial distance)
  - G = bound on subgradient norm
  - wÃÑ_k = (1/k)Œ£ w_i (average iterate)
- Slower than smooth losses (O(1/k) or O((1-1/Œ∫)^k))
- Requires diminishing step size: Œ±_k = Œ±_0/‚àök

**Coordinate descent / SMO (Sequential Minimal Optimization):**
- More efficient for SVMs than subgradient methods
- Exploits structure: updates 2 variables at a time
- Convergence rate:
  - Worst-case: O(1/Œµ¬≤) iterations
  - Typical practice: O(1/Œµ) or better
  - Each iteration: O(n) for linear kernel
- Advantages:
  - No learning rate tuning needed
  - Exact line search per coordinate
  - Handles constraints naturally

**Dual formulation:**
```
Primal: min_{w,b} ¬Ω||w||¬≤ + C¬∑Œ£_i max(0, 1 - y_i(w^T x_i + b))

Dual: max_Œ± Œ£_i Œ±_i - ¬ΩŒ£_{i,j} Œ±_i Œ±_j y_i y_j k(x_i, x_j)
      s.t. 0 ‚â§ Œ±_i ‚â§ C, Œ£_i Œ±_i y_i = 0

- Kernel trick: k(x_i, x_j) = œÜ(x_i)^T œÜ(x_j)
- QP solvers: Polynomial time O(n¬≤) to O(n¬≥)
- Sparse solution: Most Œ±_i = 0 (support vectors)
```
```

**Comparison with Cross-Entropy:**
```
Property          | Hinge Loss        | Cross-Entropy
------------------|-------------------|------------------
Output            | Margin z          | Probability ≈∑
Range             | z ‚àà ‚Ñù             | ≈∑ ‚àà [0,1]
Differentiable    | No (at margin)    | Yes
Far from boundary | 0 gradient        | Small gradient
At boundary       | Constant grad     | Depends on ≈∑
Outlier effect    | Linear            | Logarithmic
Calibration       | No                | Yes (probabilities)

Hinge: Focuses on decision boundary
Cross-Entropy: Optimizes probability estimates
```

**Use Cases:**
```
‚úì Support Vector Machines
‚úì When: Only care about classification, not probabilities
‚úì When: Want sparse solutions (many support vectors)
‚úó When: Need probability estimates
‚úó When: Using neural networks (use cross-entropy instead)
```

### Focal Loss

**Mathematical Form:**
```
FL(p_t) = -Œ±_t(1 - p_t)^Œ≥ log(p_t)

where:
p_t = {
  p      if y = 1
  1-p    if y = 0
}

Parameters:
- Œ≥ ‚â• 0: focusing parameter (typically Œ≥ = 2)
- Œ±_t: class balancing weight

Standard cross-entropy: Œ≥ = 0, Œ±_t = 1
```

**Motivation:**
```
Problem: Class imbalance in object detection
- Background: 99% of examples
- Objects: 1% of examples

Cross-entropy:
- Easy negatives dominate loss
- Model doesn't learn hard examples well

Focal loss solution:
- Down-weight easy examples: (1 - p_t)^Œ≥
- Focus on hard examples
```

**Behavior:**
```
For Œ≥ = 2:

Example classified correctly with p = 0.9:
- CE: -log(0.9) = 0.105
- FL: -(1-0.9)¬≤ log(0.9) = -0.01¬∑0.105 = 0.00105
- 100√ó reduction!

Example misclassified with p = 0.1 (y=1):
- CE: -log(0.1) = 2.303
- FL: -(1-0.1)¬≤ log(0.1) = -0.81¬∑2.303 = 1.865
- Only 20% reduction

Gradient for hard examples >> easy examples
```

**Gradient:**
```
‚àÇFL/‚àÇz = Œ±_t y(1-p_t)^Œ≥[Œ≥p_t log(p_t) + p_t - 1]

Properties:
- Complicated but can be efficiently computed
- Automatic hard example mining
- No need for manual sampling
```

**Convergence Properties:**
```
Properties:
- Non-convex (due to modulating factor)
- Still works well in practice
- Similar convergence to cross-entropy
- May need more iterations

**Hyperparameter sensitivity:**
- Œ≥: Controls focusing strength
  - Typical range: [0, 5]
  - Robust choice: Œ≥ = 2 (from RetinaNet paper)
  - Higher Œ≥ = more aggressive down-weighting
- Œ±_t: Balances class importance
  - Formula for positive class: Œ± = n_neg/(n_pos + n_neg)
  - Typical range: [0.25, 0.75]
  - Start with 0.25, tune on validation set
- Grid search recommended: Œ≥ ‚àà {0.5, 1, 2, 3}, Œ± ‚àà {0.25, 0.5, 0.75}
- Less sensitive than expected: [1.5, 2.5] √ó [0.2, 0.3] usually works

**Computational overhead:**
- Extra operations: (1-p_t)^Œ≥ and Œ≥¬∑p_t¬∑log(p_t) terms
- Overhead: ~15-25% vs standard cross-entropy
- Memory: Same as cross-entropy (no extra storage)
- Backward pass: More complex gradient but well-optimized in modern frameworks
```

**Hyperparameter selection guide:**

**Œ≥ (focusing parameter):**
- Œ≥ = 0: Standard cross-entropy (no focusing)
- Œ≥ = 1: Moderate down-weighting of easy examples
- Œ≥ = 2: Standard choice (aggressive down-weighting)
- Œ≥ = 5: Very aggressive (may ignore too many examples)

**Effect of Œ≥ on loss reduction:**
```
Well-classified example (p_t = 0.9):
Œ≥=0: weight = 1.0     (CE baseline)
Œ≥=1: weight = 0.1     (10√ó reduction)
Œ≥=2: weight = 0.01    (100√ó reduction)
Œ≥=5: weight = 0.00001 (100,000√ó reduction!)

Misclassified example (p_t = 0.1):
Œ≥=0: weight = 1.0
Œ≥=1: weight = 0.9     (10% reduction)
Œ≥=2: weight = 0.81    (19% reduction)
Œ≥=5: weight = 0.59    (41% reduction)
```

**Œ±_t (class balancing):**
- Purpose: Balance positive/negative class importance
- Formula: Œ±_t = n_neg / (n_pos + n_neg) for positive class
- Example with 1:99 imbalance: Œ±_t = 0.99 for positive class
- Typical range: 0.25-0.75
- Interacts with Œ≥: start with Œ±_t = 0.25, adjust based on validation

**When focal loss helps vs doesn't:**

**Helps when:**
- Extreme imbalance (>100:1)
- Many easy negatives (background in object detection)
- Few hard positives that matter
- One-stage detectors (RetinaNet, FCOS)

**Doesn't help / may hurt when:**
- Balanced dataset (use standard CE)
- Easy positives and hard negatives (focal loss would focus on wrong examples)
- Two-stage methods already handling imbalance via sampling
- Very small datasets (<1K samples) - may overfit to hard examples

**Practical tip:**
Start with CE, switch to focal loss if:
1. Model predicts majority class with high confidence
2. Minority class recall is low despite high precision
3. Training loss plateaus early

**Use Cases:**
```
‚úì Object detection (RetinaNet)
‚úì Extreme class imbalance
‚úì When: Hard examples are important
‚úì When: Easy negatives dominate
‚úó When: Classes are balanced (use cross-entropy)
```

### Label Smoothing

**Mathematical Form:**
```
Standard one-hot: y_i = Œ¥_ij (1 if i=j, 0 otherwise)

Label smoothing:
y'_i = (1 - Œµ)Œ¥_ij + Œµ/K

where:
- Œµ: smoothing parameter (typically 0.1)
- K: number of classes

Loss: L = -Œ£_i y'_i log(≈∑_i)
```

**Effect:**
```
Instead of [0, 0, 1, 0, 0] (K=5)
Use: [0.02, 0.02, 0.92, 0.02, 0.02] with Œµ=0.1

Prevents overfitting to training labels
Encourages model not to be overconfident
```

**Gradient:**
```
‚àÇL/‚àÇz_i = ≈∑_i - y'_i
        = ≈∑_i - [(1-Œµ)Œ¥_ij + Œµ/K]

For true class j:
‚àÇL/‚àÇz_j = ≈∑_j - (1-Œµ) - Œµ/K
        = ≈∑_j - 1 + Œµ(1 - 1/K)

For other classes:
‚àÇL/‚àÇz_i = ≈∑_i - Œµ/K

Model can never drive ≈∑_j ‚Üí 1 (always has small error Œµ)
```

**Regularization Effect:**
```
Theorem (M√ºller et al., 2019):
Label smoothing is equivalent to adding KL divergence to uniform:

L_LS = (1-Œµ)L_CE + Œµ¬∑KL(u || ≈∑)

where u is uniform distribution

Effect:
- Prevents largest logit from becoming much larger than others
- Improves generalization
- Better calibration
```

**Why label smoothing works:**

1. **Prevents overconfidence:**
   - Hard labels encourage model to make predictions arbitrarily confident
   - Softmax with z_correct >> z_wrong ‚Üí ≈∑_correct ‚âà 1
   - Model achieves this by increasing logit magnitudes: ||z|| ‚Üí ‚àû
   - Label smoothing caps maximum achievable probability: ≈∑_max = 1 - Œµ + Œµ/K

2. **Implicit regularization on logits:**
   - Without smoothing: logits can grow unbounded
   - With smoothing: bounded logits (can't make perfect prediction)
   - Example with K=10, Œµ=0.1:
     - Target: [0.01, 0.01, 0.91, ..., 0.01]
     - Model can't achieve this by making z_2 ‚Üí ‚àû
     - Forces model to learn reasonable feature magnitudes

3. **Better calibration:**
   - Overconfident models: predicted probability doesn't match true probability
   - Example: Model predicts 99% confidence but is only 90% accurate
   - Label smoothing reduces confidence ‚Üí better calibrated probabilities
   - Calibration: P(correct | confidence=p) ‚âà p

4. **Robustness to label noise:**
   - Real labels may have ~5-10% error rate (human annotation mistakes)
   - Hard labels assume 100% certainty ‚Üí model fits noise
   - Soft labels admit uncertainty ‚Üí model less affected by noise

**Optimal Œµ selection:**
```
Theory suggests: Œµ ‚âà estimated label noise rate

Practical recommendations:
- Œµ = 0.1: Standard choice for ImageNet (works for most cases)
- Œµ = 0.05: Conservative, when labels are high quality
- Œµ = 0.2: Aggressive, when labels are noisy or dataset is small
- Œµ = 0.3: Very aggressive, rarely used (may hurt)

Rule of thumb: Start with Œµ = 0.1, increase if overfitting, decrease if underfitting
```

**Use Cases:**
```
‚úì Image classification (standard practice now)
‚úì When: Want better generalization
‚úì When: Training labels may be noisy
‚úì When: Model tends to overfit
‚úì When: Need calibrated probabilities for decision-making
‚úó When: Dataset is very small (<1K samples)
‚úó When: Need perfect accuracy on training set (e.g., memorization tasks)
‚úó When: Labels are verified perfect (rare in practice)
```

---

## Probabilistic Loss Functions

### f-Divergences: A Unified Framework

Many divergences between probability distributions belong to the family of f-divergences.

**Definition 10 (f-Divergence):**
For convex function f: ‚Ñù‚Çä ‚Üí ‚Ñù with f(1) = 0:

D_f(P || Q) = E_Q[f(dP/dQ)] = Œ£_x Q(x)¬∑f(P(x)/Q(x))

where dP/dQ is the Radon-Nikodym derivative (density ratio).

**Properties:**
1. Non-negativity: D_f(P || Q) ‚â• f(1) = 0 (by Jensen's inequality)
2. Identity: D_f(P || Q) = 0 ‚ü∫ P = Q
3. Convexity: D_f(P || Q) is convex in the pair (P, Q)

**Proof of non-negativity:**
By Jensen's inequality (f is convex):

D_f(P || Q) = E_Q[f(dP/dQ)]
            ‚â• f(E_Q[dP/dQ])
            = f(Œ£_x Q(x)¬∑(P(x)/Q(x)))
            = f(Œ£_x P(x))
            = f(1)
            = 0 ‚àé

**Theorem 15 (Common Divergences as f-Divergences):**

(a) **KL Divergence:** f(t) = t log t
D_f(P || Q) = KL(P || Q) = Œ£_x P(x) log(P(x)/Q(x))

(b) **Reverse KL:** f(t) = -log t
D_f(P || Q) = KL(Q || P) = Œ£_x Q(x) log(Q(x)/P(x))

(c) **Total Variation:** f(t) = (1/2)|t - 1|
D_f(P || Q) = (1/2)Œ£_x |P(x) - Q(x)| = TV(P, Q)

(d) **Squared Hellinger:** f(t) = (‚àöt - 1)¬≤
D_f(P || Q) = Œ£_x Q(x)(‚àö(P(x)/Q(x)) - 1)¬≤
            = Œ£_x (‚àöP(x) - ‚àöQ(x))¬≤

(e) **Chi-squared:** f(t) = (t - 1)¬≤
D_f(P || Q) = Œ£_x Q(x)((P(x)/Q(x)) - 1)¬≤
            = Œ£_x (P(x) - Q(x))¬≤/Q(x)
            = œá¬≤(P || Q)

(f) **Jensen-Shannon:** Symmetrized KL
JS(P || Q) = (1/2)KL(P || M) + (1/2)KL(Q || M)
where M = (1/2)(P + Q)

Properties:
- Symmetric: JS(P || Q) = JS(Q || P)
- Bounded: 0 ‚â§ JS(P || Q) ‚â§ log 2
- Square root is a metric: ‚àöJS is a proper distance

**Theorem 16 (Variational Representation of f-Divergences):**
For any f-divergence:

D_f(P || Q) = sup_{T: X‚Üí‚Ñù} {E_P[T(X)] - E_Q[f*(T(X))]}

where f* is the convex conjugate of f:
f*(y) = sup_t {ty - f(t)}

This variational form is the basis for adversarial training (GANs).

**Example (KL Divergence):**
For f(t) = t log t:
f*(y) = e^(y-1)

Thus:
KL(P || Q) = sup_T {E_P[T] - E_Q[e^(T-1)]}

**Theorem 17 (Data Processing Inequality):**
For any f-divergence and Markov chain X ‚Üí Y ‚Üí Z:

D_f(P_X || Q_X) ‚â• D_f(P_Y || Q_Y) ‚â• D_f(P_Z || Q_Z)

In words: processing data through any channel cannot increase divergence.

Proof: Uses Jensen's inequality and the Markov property. ‚àé

**Practical implication:**
- Feature extraction reduces divergence between distributions
- Information is lost, never gained, through transformations
- Applies to all f-divergences simultaneously

### Kullback-Leibler (KL) Divergence

**Mathematical Form:**
```
KL(P || Q) = Œ£_x P(x) log(P(x)/Q(x))
           = E_P[log P(x)] - E_P[log Q(x)]

Continuous case:
KL(P || Q) = ‚à´ p(x) log(p(x)/q(x)) dx
```

**Properties:**
```
1. Non-negativity: KL(P || Q) ‚â• 0
2. Identity: KL(P || Q) = 0 ‚ü∫ P = Q (almost everywhere)
3. Asymmetry: KL(P || Q) ‚â† KL(Q || P) in general
4. Not a metric (no triangle inequality)

Proof of non-negativity (Jensen's inequality):
KL(P || Q) = -E_P[log(Q/P)]
           ‚â• -log E_P[Q/P]
           = -log Œ£_x P(x)¬∑(Q(x)/P(x))
           = -log Œ£_x Q(x)
           = -log 1 = 0
```

**Relationship to Cross-Entropy:**
```
H(P, Q) = -E_P[log Q(x)]         (cross-entropy)
H(P) = -E_P[log P(x)]            (entropy)

KL(P || Q) = H(P, Q) - H(P)

In supervised learning:
- P: true distribution (one-hot)
- Q: predicted distribution
- H(P) = 0 for one-hot labels

Therefore: KL(P || Q) = H(P, Q)

Minimizing KL ‚ü∫ Minimizing cross-entropy
```

**Forward vs Reverse KL:**
```
Forward KL: KL(P || Q)
- Minimizing Q w.r.t. P
- Mean-seeking: Q covers all modes of P
- Used in: Maximum likelihood

Reverse KL: KL(Q || P)
- Mode-seeking: Q focuses on one mode of P
- Used in: Variational inference

Behavior difference:
P has two modes, Q is unimodal
- Forward KL: Q spreads between modes (bad)
- Reverse KL: Q picks one mode (better for VI)
```

**Intuitive explanation of forward vs reverse:**

**Forward KL: KL(P || Q) = E_P[log P - log Q]**

Behavior:
- Expectation over P: Must have high probability under Q wherever P has probability
- If P(x) > 0 but Q(x) ‚âà 0: log Q(x) ‚Üí -‚àû, KL ‚Üí ‚àû (infinite penalty!)
- Forces Q to cover all of P (Q spreads out to avoid zero probability under P)

Example (mixture of two Gaussians):
```
P = 0.5¬∑N(-3, 1) + 0.5¬∑N(+3, 1)  (two modes at -3 and +3)
Q = N(Œº, œÉ¬≤)                      (single Gaussian)

Forward KL minimization:
- Q must have P(x)>0 ‚Üí Q(x)>0 everywhere P has mass
- Result: Q = N(0, 4) (wide Gaussian covering both modes)
- Mean Œº=0 (average of -3 and +3)
- Large variance to cover both modes
```

**Reverse KL: KL(Q || P) = E_Q[log Q - log P]**

Behavior:
- Expectation over Q: Only cares about where Q has probability
- If Q(x) > 0 but P(x) ‚âà 0: log P(x) ‚Üí -‚àû, KL ‚Üí ‚àû
- Forces Q to only put mass where P has high probability (Q is zero-avoiding under P)
- Q ignores low-probability regions of P

Example (same mixture):
```
P = 0.5¬∑N(-3, 1) + 0.5¬∑N(+3, 1)
Q = N(Œº, œÉ¬≤)

Reverse KL minimization:
- Q can put mass anywhere, but gets penalized if P is low there
- Result: Q = N(-3, 1) OR N(+3, 1) (picks one mode!)
- Which mode depends on initialization
- Tight fit to chosen mode
```

**Practical consequences:**

**Use Forward KL (Maximum Likelihood) when:**
- Have samples from P (data distribution)
- Want Q to explain all data variations
- Can't afford to miss any data modes
- Example: Generative modeling (GAN, diffusion)

**Use Reverse KL (Variational Inference) when:**
- Need to sample from Q and evaluate P
- Want tractable approximate posterior
- Okay with approximating one mode well
- Example: VAE, variational Bayes

**Visual intuition:**
```
P: ‚ö´  ‚ö´    (two modes)

Forward KL ‚Üí Q: ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ (covers both, wide)
Reverse KL ‚Üí Q:  ‚ö´    (picks one, tight)
```

**Mathematical reason:**
```
Forward: ‚à´ p(x) log[p(x)/q(x)] dx
- Integral weighted by p(x)
- High penalty where p(x) is large but q(x) is small
- q must spread to cover all of p

Reverse: ‚à´ q(x) log[q(x)/p(x)] dx
- Integral weighted by q(x)
- Only penalty where q(x) is large
- q can ignore regions where p is small
```

**Gradient (for Variational Inference):**
```
For parameterized Q_Œ∏:

‚àá_Œ∏ KL(Q_Œ∏ || P) = ‚àá_Œ∏ E_Q[log Q_Œ∏(z) - log P(z)]

Using reparameterization trick:
z = g_Œ∏(Œµ, x) where Œµ ~ p(Œµ)

‚àá_Œ∏ KL = E_Œµ[‚àá_Œ∏(log Q_Œ∏(g_Œ∏(Œµ,x)) - log P(g_Œ∏(Œµ,x)))]

Enables backpropagation through sampling
```

**Use Cases:**
```
‚úì Variational inference (VAE, VIB)
‚úì Knowledge distillation
‚úì Distribution matching
‚úì Policy optimization (RL)
‚úó When: Need symmetric divergence (use JS divergence)
```

### Wasserstein Distance (Earth Mover's Distance)

**Mathematical Form:**
```
W_p(P, Q) = (inf_{Œ≥‚ààŒì(P,Q)} E_{(x,y)~Œ≥}[||x-y||^p])^{1/p}

where Œì(P,Q) is set of joint distributions with marginals P and Q

For p=1: W_1(P, Q) = inf_{Œ≥} E_{(x,y)~Œ≥}[||x-y||]

Interpretation: Minimum cost to transport mass from P to Q
```

**Properties:**
```
1. Metric: Satisfies triangle inequality
2. Weaker topology than KL divergence
3. Doesn't suffer from mode collapse
4. Well-defined even when P and Q don't overlap

Advantage over KL:
- KL(P || Q) = ‚àû if supports don't overlap
- W(P, Q) is always finite
```

**Kantorovich-Rubinstein Duality:**
```
W_1(P, Q) = sup_{||f||_L‚â§1} [E_P[f(x)] - E_Q[f(x)]]

where ||f||_L ‚â§ 1 means f is 1-Lipschitz

This is computable! (used in WGANs)
```

**Gradient (WGAN):**
```
Discriminator (critic) f must be 1-Lipschitz:

Methods:
1. Weight clipping: clip weights to [-c, c]
2. Gradient penalty: ||‚àá_x f(x)||‚ÇÇ ‚âà 1

WGAN-GP loss:
L_D = E_Q[f(x)] - E_P[f(x)] + Œª¬∑E_xÃÇ[(||‚àá_xÃÇ f(xÃÇ)||‚ÇÇ - 1)¬≤]

where xÃÇ = Œ±x_real + (1-Œ±)x_fake, Œ± ~ U[0,1]
```

**Convergence Properties:**
```
WGAN theoretical guarantees:

Theorem: Optimizing W_1 distance:
- Generator converges to data distribution
- Even when distributions have disjoint supports

Gradient behavior:
- Non-saturating (unlike original GAN)
- Meaningful loss values (correlate with quality)
- Stable training

Practical convergence:
- Requires many critic iterations per generator step
- GP version more stable than weight clipping
```

**Use Cases:**
```
‚úì GANs (WGAN, WGAN-GP)
‚úì When: Distributions may not overlap
‚úì When: Need meaningful loss values
‚úì Optimal transport problems
‚úó When: Computational cost is critical (expensive)
```

---

## Ranking and Metric Learning

### Triplet Loss

**Mathematical Form:**
```
L = max(0, d(a, p) - d(a, n) + margin)

where:
- a: anchor sample
- p: positive sample (same class as anchor)
- n: negative sample (different class)
- d(¬∑,¬∑): distance metric (usually L2)
- margin: hyperparameter (typically 0.2-1.0)

Goal: d(a, p) + margin < d(a, n)
```

**Gradient:**
```
When loss > 0:

‚àÇL/‚àÇa = 2(a - p) - 2(a - n) = 2(n - p)
‚àÇL/‚àÇp = 2(p - a)
‚àÇL/‚àÇn = 2(n - a)

Properties:
- Pulls anchor to positive
- Pushes anchor from negative
- Zero gradient when margin satisfied
```

**Mining Strategies:**
```
Triplet selection critical!

1. Hard negative mining:
   n = argmax_n d(a, n) s.t. d(a,p) > d(a,n)
   Most violating negative

2. Semi-hard negative:
   d(a, p) < d(a, n) < d(a, p) + margin
   Violates margin but not order

3. Easy negative:
   d(a, n) > d(a, p) + margin
   Don't contribute to loss (skip)

4. Batch-all mining:
   Use all valid triplets in batch
   Can be many: O(batch_size¬≥)
```

**Convergence Properties:**
```
Challenges:
- Non-smooth (max function)
- Many triplets satisfy margin (zero gradient)
- Requires good mining strategy

Convergence:
- Depends heavily on mining
- Hard mining: faster but unstable
- Semi-hard: slower but more stable

Practical tips:
- Start with easy/random triplets
- Gradually increase difficulty
- Use online mining within batches
```

**Use Cases:**
```
‚úì Face recognition (FaceNet)
‚úì Metric learning
‚úì Image retrieval
‚úì When: Need embedding space with metric properties
‚úó When: Number of classes is small (use softmax)
```

### Contrastive Loss

**Mathematical Form:**
```
L = (1-y)¬∑¬Ωd¬≤ + y¬∑¬Ωmax(0, margin - d)¬≤

where:
- y = 1 if same class, 0 if different
- d = ||f(x‚ÇÅ) - f(x‚ÇÇ)||‚ÇÇ

Alternative (SimCLR style):
L = -log(exp(sim(z_i, z_j)/œÑ) / Œ£_k exp(sim(z_i, z_k)/œÑ))

where sim(u,v) = u¬∑v / (||u||¬∑||v||) (cosine similarity)
```

**Gradient:**
```
For positive pair (y=1):
‚àÇL/‚àÇz‚ÇÅ = (z‚ÇÅ - z‚ÇÇ)¬∑max(0, margin - d)/d

For negative pair (y=0):
‚àÇL/‚àÇz‚ÇÅ = (z‚ÇÅ - z‚ÇÇ)¬∑d

Behavior:
- Positive: push together until margin
- Negative: push apart always (no limit)
```

**InfoNCE (Contrastive Predictive Coding):**
```
L = -E[log(f(x,c_pos) / (f(x,c_pos) + Œ£_i f(x,c_neg,i)))]

where f(x,c) = exp(x^T c / œÑ)

Properties:
- Lower bound on mutual information
- As negatives ‚Üí ‚àû: approaches MI
- Temperature œÑ controls difficulty
```

**Use Cases:**
```
‚úì Self-supervised learning (SimCLR, MoCo)
‚úì Representation learning
‚úì When: Have pairs of similar/dissimilar samples
‚úì When: Unsupervised or semi-supervised setting
```

---

## Sequence-to-Sequence Losses

### Teacher Forcing with Cross-Entropy

**Mathematical Form:**
```
For sequence y = [y‚ÇÅ, y‚ÇÇ, ..., y_T]:

L = -(1/T) Œ£_t log P(y_t | y‚ÇÅ, ..., y_{t-1}, x)

Teacher forcing: Use ground truth y_{t-1} during training

Problems:
- Exposure bias: Model sees gold labels in training
- Test time: Uses own predictions ‚Üí distribution mismatch
```

**Scheduled Sampling:**
```
Mix gold and predicted tokens during training:

With probability p: use y_{t-1} (gold)
With probability 1-p: use ≈∑_{t-1} (predicted)

Schedule: p decreases over training (e.g., p = 1/(1+k/k‚ÇÄ))

Reduces exposure bias but makes training non-differentiable
```

### Connectionist Temporal Classification (CTC)

**Mathematical Form:**
```
For input X of length T, output Y of length U ‚â§ T:

P(Y | X) = Œ£_{œÄ‚ààAlignments(Y)} Œ†_t P(œÄ_t | X)

where Alignments(Y) are all valid alignments

Example:
Y = "cat"
Alignments: [c,c,a,t], [c,a,a,t], [c,a,t,t], [-,c,a,t], etc.

CTC loss: L = -log P(Y | X)
```

**Forward-Backward Algorithm:**
```
Efficiently compute sum over exponential alignments:

Forward: Œ±(t,s) = P(labels[1:s] | input[1:t])
Backward: Œ≤(t,s) = P(labels[s:] | input[t:])

Combined: P(Y|X) = Œ±(T,|Y|)

Complexity: O(T ¬∑ U) instead of O(T^U)
```

**Gradient:**
```
‚àÇL/‚àÇy_{t,k} = p_{t,k} - (1/P(Y|X)) Œ£_{s:œÄ_s=k} Œ±(t,s)Œ≤(t,s)

where p_{t,k} = P(label k at time t)

Computed efficiently with forward-backward
```

**Use Cases:**
```
‚úì Speech recognition
‚úì OCR (handwriting recognition)
‚úì When: Input and output lengths differ
‚úì When: Alignment is unknown
‚úó When: Need attention mechanism (use seq2seq instead)
```

---

## Generative Model Losses

### Variational Lower Bound (ELBO)

**Mathematical Form:**
```
For VAE with encoder q_œÜ(z|x) and decoder p_Œ∏(x|z):

ELBO(Œ∏,œÜ) = E_q[log p_Œ∏(x|z)] - KL(q_œÜ(z|x) || p(z))

Equivalent forms:
= log p(x) - KL(q_œÜ(z|x) || p(z|x))
= E_q[log p_Œ∏(x,z) - log q_œÜ(z|x)]

Loss: L = -ELBO (we minimize negative ELBO)
```

**Derivation:**
```
log p(x) = log‚à´ p(x,z) dz
         = log‚à´ p(x,z)¬∑(q(z|x)/q(z|x)) dz
         = log E_q[p(x,z)/q(z|x)]
         ‚â• E_q[log p(x,z)/q(z|x)]    (Jensen's inequality)
         = E_q[log p(x|z)] - KL(q(z|x) || p(z))
         = ELBO

Gap: KL(q(z|x) || p(z|x))
Tight when: q(z|x) = p(z|x) (intractable)
```

**Gradient Estimation:**
```
Problem: Can't backprop through z ~ q_œÜ(z|x)

Reparameterization trick:
z = Œº_œÜ(x) + œÉ_œÜ(x) ‚äô Œµ, where Œµ ~ N(0,I)

Now:
‚àá_œÜ E_q[f(z)] = ‚àá_œÜ E_Œµ[f(Œº_œÜ(x) + œÉ_œÜ(x) ‚äô Œµ)]
              = E_Œµ[‚àá_œÜ f(Œº_œÜ(x) + œÉ_œÜ(x) ‚äô Œµ)]

Low variance, unbiased gradient!
```

**Œ≤-VAE:**
```
L = E_q[log p_Œ∏(x|z)] - Œ≤¬∑KL(q_œÜ(z|x) || p(z))

Œ≤ > 1: Stronger regularization
- More disentangled representations
- Worse reconstruction

Œ≤ < 1: Weaker regularization
- Better reconstruction
- Less disentangled

Trade-off between reconstruction and regularization
```

**Convergence Properties:**
```
ELBO optimization:

Properties:
- ELBO is lower bound on log p(x)
- Maximizing ELBO ‚üπ good generative model
- Non-convex (neural networks)

Convergence:
- Alternating optimization (not simultaneous)
- Can use Adam/SGD
- Often reaches good local optimum

Challenges:
- KL vanishing: KL ‚Üí 0, model ignores z
- Solutions: KL annealing, free bits
```

**Use Cases:**
```
‚úì Variational Autoencoders
‚úì Bayesian inference approximation
‚úì Latent variable models
‚úì Semi-supervised learning
```

### GAN Losses

**Original GAN (Minimax):**
```
L_D = -E_real[log D(x)] - E_fake[log(1 - D(G(z)))]
L_G = -E_fake[log D(G(z))]

Minimax formulation:
min_G max_D V(D,G) = E_real[log D(x)] + E_fake[log(1-D(G(z)))]

Problems:
- Vanishing gradients for generator
- When D is optimal: log(1-D(G(z))) ‚âà 0
```

**Non-Saturating GAN:**
```
L_G = -E_fake[log D(G(z))]

Instead of minimizing log(1-D(G(z))), maximize log D(G(z))

Gradient:
- Original: -D'(G(z)) when D(G(z)) small (weak signal)
- Non-saturating: D'(G(z))/D(G(z)) (stronger)

Still minimax, just different G objective
```

**Wasserstein GAN (WGAN):**
```
L_D = -E_real[D(x)] + E_fake[D(G(z))]
L_G = -E_fake[D(G(z))]

where D is 1-Lipschitz

Advantages:
- Meaningful loss (correlates with quality)
- No saturation
- More stable training

Enforce Lipschitz:
- Weight clipping: clip Œ∏ to [-c, c]
- Gradient penalty (WGAN-GP): better
```

**WGAN-GP (Gradient Penalty):**
```
L_D = -E_real[D(x)] + E_fake[D(G(z))]
      + Œª¬∑E_xÃÇ[(||‚àá_xÃÇ D(xÃÇ)||‚ÇÇ - 1)¬≤]

where:
xÃÇ = Œµ¬∑x_real + (1-Œµ)¬∑x_fake, Œµ ~ U[0,1]
Œª = 10 typically

Gradient penalty enforces 1-Lipschitz constraint softly
```

**Convergence Analysis:**
```
Original GAN:
- Theoretically: Reaches Nash equilibrium (D*,G*)
- Practically: Oscillates, mode collapse, instability

WGAN:
- Theoretical guarantee: G ‚Üí data distribution
- Practically: More stable, slower

WGAN-GP:
- Best practical performance
- Lipschitz constraint well-enforced
- Typical: 5 critic steps per 1 generator step
```

**Use Cases:**
```
Original GAN:
‚úó Hard to train, unstable

Non-saturating GAN:
‚úì Better gradients, still can be unstable

WGAN/WGAN-GP:
‚úì Image generation
‚úì More stable training
‚úì Meaningful loss values
‚úì State-of-the-art for GANs
```

---

## Convergence Properties Summary

### Convexity and Convergence Rates

```
Loss Type        | Convexity | GD Rate      | SGD Rate     | Notes
-----------------|-----------|--------------|--------------|------------------
MSE (Linear)     | Convex    | O(exp(-k))   | O(1/k)       | Strongly convex
MAE              | Convex    | O(1/‚àök)      | O(1/‚àök)      | Non-smooth
Huber            | Convex    | O(exp(-k))   | O(1/k)       | Smooth, robust
Cross-Entropy    | Convex*   | O(1/k)       | O(1/‚àök)      | *if separable
Hinge            | Convex    | O(1/‚àök)      | O(1/‚àök)      | Non-smooth
Triplet          | Non-conv  | -            | O(1/‚àök)      | Mining crucial
ELBO             | Non-conv  | -            | Local        | VAE
GAN              | Non-conv  | -            | Oscillates   | Nash equilibrium

where k = iteration number
```

### Lipschitz Constants

```
Loss                     | Lipschitz Constant L
-------------------------|--------------------
MSE                      | Œª_max(X^T X)
Cross-entropy            | 1/4 (for logistic)
Huber(Œ¥)                | 1 (in linear region)
Hinge                    | 1

For Gradient Descent: Œ± ‚â§ 1/L for convergence
```

### Practical Convergence Tips

```
1. **MSE**: Fast convergence, watch for outliers
2. **Cross-Entropy**: Standard for classification, use with softmax
3. **Huber**: Best of both worlds for regression
4. **Focal**: Need more iterations than cross-entropy
5. **Triplet**: Mining strategy critical, start easy
6. **ELBO**: KL annealing often helps
7. **GAN**: Use WGAN-GP, multiple critic steps
```

---

## Loss Function Selection Guide

### Decision Tree

```
Task: Regression?
‚îú‚îÄ Yes: Outliers present?
‚îÇ  ‚îú‚îÄ Yes: Use Huber or MAE
‚îÇ  ‚îî‚îÄ No: Use MSE
‚îÇ
‚îî‚îÄ No (Classification)
   ‚îú‚îÄ Binary or Multi-class?
   ‚îÇ  ‚îú‚îÄ Binary: Use Binary Cross-Entropy
   ‚îÇ  ‚îî‚îÄ Multi-class:
   ‚îÇ     ‚îú‚îÄ Exclusive classes? Use Categorical Cross-Entropy
   ‚îÇ     ‚îî‚îÄ Non-exclusive? Use Binary Cross-Entropy per class
   ‚îÇ
   ‚îú‚îÄ Class imbalance?
   ‚îÇ  ‚îú‚îÄ Severe (>100:1): Use Focal Loss
   ‚îÇ  ‚îî‚îÄ Moderate: Use weighted Cross-Entropy
   ‚îÇ
   ‚îú‚îÄ Need embeddings?
   ‚îÇ  ‚îî‚îÄ Use Triplet or Contrastive Loss
   ‚îÇ
   ‚îî‚îÄ Generative model?
      ‚îú‚îÄ VAE: Use ELBO
      ‚îú‚îÄ GAN: Use WGAN-GP
      ‚îî‚îÄ Autoregressive: Use Cross-Entropy
```

### Quick Reference

```
Scenario                           | Recommended Loss
-----------------------------------|------------------
Image classification               | Cross-Entropy + Label Smoothing
Object detection                   | Focal Loss (RetinaNet) or Smooth L1 (Faster R-CNN)
Semantic segmentation              | Cross-Entropy or Dice Loss
Image regression                   | MSE or Huber
Face recognition                   | Triplet or ArcFace
Self-supervised learning           | Contrastive (SimCLR)
Speech recognition                 | CTC
Machine translation                | Cross-Entropy (teacher forcing)
VAE                               | ELBO (reconstruction + KL)
GAN                               | WGAN-GP
Reinforcement learning             | Policy Gradient + Value Loss (A2C/PPO)
Ranking/Retrieval                  | Triplet or Listwise
Anomaly detection                  | Reconstruction error (MSE/MAE)
Time series forecasting            | MSE, MAE, or Quantile Loss
Ordinal regression                 | Ordinal Cross-Entropy
Multi-task learning                | Weighted sum + uncertainty weighting
```

---

## Practical Implementations of Theory

### Computing Proper Scoring Rules

**Implementation of proper scoring rules and calibration metrics:**

```python
import numpy as np
from scipy.stats import chi2
from scipy.special import softmax

def log_score(y_true, y_pred_proba):
    """
    Logarithmic scoring rule (negative log-likelihood).
    Strictly proper scoring rule.

    Args:
        y_true: Binary labels (0 or 1), shape (n,)
        y_pred_proba: Predicted probabilities for class 1, shape (n,)

    Returns:
        score: Average log score (lower is better)
    """
    epsilon = 1e-15  # Avoid log(0)
    y_pred_proba = np.clip(y_pred_proba, epsilon, 1 - epsilon)

    scores = -np.where(y_true == 1,
                       np.log(y_pred_proba),
                       np.log(1 - y_pred_proba))

    return np.mean(scores)

def brier_score(y_true, y_pred_proba):
    """
    Brier score (mean squared error of probabilities).
    Strictly proper scoring rule.

    Args:
        y_true: Binary labels (0 or 1), shape (n,)
        y_pred_proba: Predicted probabilities for class 1, shape (n,)

    Returns:
        score: Brier score (lower is better)
    """
    return np.mean((y_pred_proba - y_true) ** 2)

def spherical_score(y_true, y_pred_proba):
    """
    Spherical scoring rule.
    Proper scoring rule that normalizes predictions.

    Args:
        y_true: Binary labels (0 or 1), shape (n,)
        y_pred_proba: Predicted probabilities for class 1, shape (n,)

    Returns:
        score: Negative spherical score (lower is better)
    """
    # For binary: p' = [1-p, p], y' = [1-y, y]
    p0 = 1 - y_pred_proba
    p1 = y_pred_proba
    norm = np.sqrt(p0**2 + p1**2)

    scores = np.where(y_true == 1, p1 / norm, p0 / norm)

    return -np.mean(scores)  # Negative because higher is better

def expected_calibration_error(y_true, y_pred_proba, n_bins=10):
    """
    Expected Calibration Error (ECE).
    Measures calibration by binning predictions.

    Args:
        y_true: True labels, shape (n,)
        y_pred_proba: Predicted probabilities, shape (n,)
        n_bins: Number of bins for calibration

    Returns:
        ece: Expected calibration error
        bin_stats: Dictionary with per-bin statistics
    """
    # Create bins
    bins = np.linspace(0, 1, n_bins + 1)
    bin_indices = np.digitize(y_pred_proba, bins) - 1
    bin_indices = np.clip(bin_indices, 0, n_bins - 1)

    ece = 0.0
    bin_stats = []

    for i in range(n_bins):
        mask = bin_indices == i

        if np.sum(mask) == 0:
            continue

        bin_size = np.sum(mask)
        bin_confidence = np.mean(y_pred_proba[mask])
        bin_accuracy = np.mean(y_true[mask])

        bin_error = np.abs(bin_confidence - bin_accuracy)
        ece += (bin_size / len(y_true)) * bin_error

        bin_stats.append({
            'bin': i,
            'count': bin_size,
            'confidence': bin_confidence,
            'accuracy': bin_accuracy,
            'error': bin_error
        })

    return ece, bin_stats

def maximum_calibration_error(y_true, y_pred_proba, n_bins=10):
    """
    Maximum Calibration Error (MCE).
    Worst-case calibration across bins.

    Args:
        y_true: True labels, shape (n,)
        y_pred_proba: Predicted probabilities, shape (n,)
        n_bins: Number of bins

    Returns:
        mce: Maximum calibration error
    """
    _, bin_stats = expected_calibration_error(y_true, y_pred_proba, n_bins)

    if len(bin_stats) == 0:
        return 0.0

    return max(stat['error'] for stat in bin_stats)

def temperature_scaling(logits, y_true, T_init=1.0, lr=0.01, max_iter=100):
    """
    Temperature scaling for model calibration.
    Optimizes temperature T to minimize NLL on validation set.

    Args:
        logits: Model logits before softmax, shape (n, n_classes)
        y_true: True class labels, shape (n,)
        T_init: Initial temperature
        lr: Learning rate
        max_iter: Maximum iterations

    Returns:
        T_optimal: Optimal temperature
        nll_history: NLL at each iteration
    """
    T = T_init
    nll_history = []

    for iteration in range(max_iter):
        # Compute probabilities with current temperature
        scaled_logits = logits / T
        probs = softmax(scaled_logits, axis=1)

        # Compute negative log-likelihood
        epsilon = 1e-15
        probs = np.clip(probs, epsilon, 1 - epsilon)
        nll = -np.mean(np.log(probs[np.arange(len(y_true)), y_true]))
        nll_history.append(nll)

        # Gradient of NLL w.r.t. T
        # ‚àÇNLL/‚àÇT = (1/T¬≤) Œ£_i (z_i - z_{y_i})
        one_hot = np.zeros_like(probs)
        one_hot[np.arange(len(y_true)), y_true] = 1

        grad_T = np.mean(np.sum((probs - one_hot) * logits, axis=1)) / (T ** 2)

        # Update temperature
        T -= lr * grad_T
        T = max(T, 0.01)  # Ensure T > 0

        # Check convergence
        if iteration > 0 and abs(nll_history[-1] - nll_history[-2]) < 1e-6:
            break

    return T, nll_history

# Example usage
if __name__ == "__main__":
    # Generate example data
    np.random.seed(42)
    n_samples = 1000

    # Ground truth labels
    y_true = np.random.binomial(1, 0.3, n_samples)

    # Overconfident predictions (miscalibrated)
    y_pred_overconfident = np.where(y_true == 1,
                                     np.random.beta(8, 2, n_samples),
                                     np.random.beta(2, 8, n_samples))

    # Well-calibrated predictions
    y_pred_calibrated = np.where(y_true == 1,
                                  np.random.beta(3, 2, n_samples),
                                  np.random.beta(2, 3, n_samples))

    print("=== Proper Scoring Rules ===")
    print(f"\nOverconfident model:")
    print(f"  Log score: {log_score(y_true, y_pred_overconfident):.4f}")
    print(f"  Brier score: {brier_score(y_true, y_pred_overconfident):.4f}")
    print(f"  Spherical score: {spherical_score(y_true, y_pred_overconfident):.4f}")

    print(f"\nCalibrated model:")
    print(f"  Log score: {log_score(y_true, y_pred_calibrated):.4f}")
    print(f"  Brier score: {brier_score(y_true, y_pred_calibrated):.4f}")
    print(f"  Spherical score: {spherical_score(y_true, y_pred_calibrated):.4f}")

    print("\n=== Calibration Metrics ===")
    ece_over, _ = expected_calibration_error(y_true, y_pred_overconfident)
    mce_over = maximum_calibration_error(y_true, y_pred_overconfident)

    ece_cal, _ = expected_calibration_error(y_true, y_pred_calibrated)
    mce_cal = maximum_calibration_error(y_true, y_pred_calibrated)

    print(f"\nOverconfident model:")
    print(f"  ECE: {ece_over:.4f}")
    print(f"  MCE: {mce_over:.4f}")

    print(f"\nCalibrated model:")
    print(f"  ECE: {ece_cal:.4f}")
    print(f"  MCE: {mce_cal:.4f}")
```

### Computing f-Divergences

**Implementation of various f-divergences:**

```python
import numpy as np
from scipy.special import kl_div, rel_entr

def compute_kl_divergence(P, Q, epsilon=1e-10):
    """
    Kullback-Leibler divergence KL(P || Q).

    Args:
        P: True distribution, shape (n,)
        Q: Approximate distribution, shape (n,)
        epsilon: Small value to avoid log(0)

    Returns:
        kl: KL divergence (nats)
    """
    P = np.asarray(P) + epsilon
    Q = np.asarray(Q) + epsilon

    # Normalize
    P = P / np.sum(P)
    Q = Q / np.sum(Q)

    return np.sum(rel_entr(P, Q))

def compute_reverse_kl(P, Q, epsilon=1e-10):
    """
    Reverse KL divergence KL(Q || P).

    Args:
        P: First distribution, shape (n,)
        Q: Second distribution, shape (n,)
        epsilon: Small value to avoid log(0)

    Returns:
        rkl: Reverse KL divergence (nats)
    """
    return compute_kl_divergence(Q, P, epsilon)

def compute_js_divergence(P, Q, epsilon=1e-10):
    """
    Jensen-Shannon divergence JS(P || Q).
    Symmetric and bounded version of KL divergence.

    Args:
        P: First distribution, shape (n,)
        Q: Second distribution, shape (n,)
        epsilon: Small value to avoid log(0)

    Returns:
        js: Jensen-Shannon divergence (nats)
    """
    P = np.asarray(P) + epsilon
    Q = np.asarray(Q) + epsilon

    # Normalize
    P = P / np.sum(P)
    Q = Q / np.sum(Q)

    # Mixture distribution
    M = 0.5 * (P + Q)

    return 0.5 * compute_kl_divergence(P, M, 0) + 0.5 * compute_kl_divergence(Q, M, 0)

def compute_hellinger_distance(P, Q, epsilon=1e-10):
    """
    Hellinger distance H(P, Q).
    Metric derived from squared Hellinger divergence.

    Args:
        P: First distribution, shape (n,)
        Q: Second distribution, shape (n,)
        epsilon: Small value for numerical stability

    Returns:
        hellinger: Hellinger distance [0, 1]
    """
    P = np.asarray(P) + epsilon
    Q = np.asarray(Q) + epsilon

    # Normalize
    P = P / np.sum(P)
    Q = Q / np.sum(Q)

    # Hellinger distance: (1/‚àö2)||‚àöP - ‚àöQ||
    return np.sqrt(np.sum((np.sqrt(P) - np.sqrt(Q)) ** 2)) / np.sqrt(2)

def compute_total_variation(P, Q, epsilon=1e-10):
    """
    Total Variation distance TV(P, Q).

    Args:
        P: First distribution, shape (n,)
        Q: Second distribution, shape (n,)
        epsilon: Small value for numerical stability

    Returns:
        tv: Total variation distance [0, 1]
    """
    P = np.asarray(P) + epsilon
    Q = np.asarray(Q) + epsilon

    # Normalize
    P = P / np.sum(P)
    Q = Q / np.sum(Q)

    return 0.5 * np.sum(np.abs(P - Q))

def compute_chi_squared(P, Q, epsilon=1e-10):
    """
    Chi-squared divergence œá¬≤(P || Q).

    Args:
        P: True distribution, shape (n,)
        Q: Approximate distribution, shape (n,)
        epsilon: Small value to avoid division by zero

    Returns:
        chi_sq: Chi-squared divergence
    """
    P = np.asarray(P) + epsilon
    Q = np.asarray(Q) + epsilon

    # Normalize
    P = P / np.sum(P)
    Q = Q / np.sum(Q)

    return np.sum((P - Q) ** 2 / Q)

def compute_f_divergence(P, Q, f_func, epsilon=1e-10):
    """
    General f-divergence computation.

    Args:
        P: True distribution, shape (n,)
        Q: Approximate distribution, shape (n,)
        f_func: Convex function f(t) with f(1) = 0
        epsilon: Small value for numerical stability

    Returns:
        div: f-divergence
    """
    P = np.asarray(P) + epsilon
    Q = np.asarray(Q) + epsilon

    # Normalize
    P = P / np.sum(P)
    Q = Q / np.sum(Q)

    # Compute t = P(x)/Q(x) for each x
    t = P / Q

    # D_f(P || Q) = Œ£ Q(x) f(P(x)/Q(x))
    return np.sum(Q * f_func(t))

# Example usage
if __name__ == "__main__":
    # Example distributions
    P = np.array([0.4, 0.3, 0.2, 0.1])
    Q = np.array([0.3, 0.3, 0.25, 0.15])

    print("=== f-Divergences between P and Q ===")
    print(f"P = {P}")
    print(f"Q = {Q}")
    print()

    print(f"KL(P || Q):         {compute_kl_divergence(P, Q):.6f} nats")
    print(f"KL(Q || P):         {compute_reverse_kl(P, Q):.6f} nats")
    print(f"JS(P || Q):         {compute_js_divergence(P, Q):.6f} nats")
    print(f"Hellinger(P, Q):    {compute_hellinger_distance(P, Q):.6f}")
    print(f"TV(P, Q):           {compute_total_variation(P, Q):.6f}")
    print(f"œá¬≤(P || Q):         {compute_chi_squared(P, Q):.6f}")

    print("\n=== Custom f-divergences ===")

    # KL: f(t) = t log t
    kl_custom = compute_f_divergence(P, Q, lambda t: t * np.log(t))
    print(f"KL (via f-div):     {kl_custom:.6f} nats")

    # Reverse KL: f(t) = -log t
    rkl_custom = compute_f_divergence(P, Q, lambda t: -np.log(t))
    print(f"Reverse KL (via f): {rkl_custom:.6f} nats")

    # Chi-squared: f(t) = (t - 1)¬≤
    chi_custom = compute_f_divergence(P, Q, lambda t: (t - 1) ** 2)
    print(f"œá¬≤ (via f-div):     {chi_custom:.6f}")
```

### Visualizing Surrogate Losses

**Compare surrogate losses and their relationship to 0-1 loss:**

```python
import numpy as np
import matplotlib.pyplot as plt

def zero_one_loss(margin):
    """0-1 loss: ùüô[margin ‚â§ 0]"""
    return (margin <= 0).astype(float)

def hinge_loss(margin):
    """Hinge loss: max(0, 1 - margin)"""
    return np.maximum(0, 1 - margin)

def logistic_loss(margin):
    """Logistic loss: log(1 + exp(-margin))"""
    return np.log(1 + np.exp(-margin))

def exponential_loss(margin):
    """Exponential loss: exp(-margin)"""
    return np.exp(-margin)

def squared_loss(margin):
    """Squared loss: (1 - margin)¬≤"""
    return (1 - margin) ** 2

# Plot comparison
margin = np.linspace(-2, 3, 500)

plt.figure(figsize=(12, 7))

plt.plot(margin, zero_one_loss(margin), 'k-', linewidth=2, label='0-1 loss')
plt.plot(margin, hinge_loss(margin), 'r-', linewidth=2, label='Hinge')
plt.plot(margin, logistic_loss(margin), 'b-', linewidth=2, label='Logistic')
plt.plot(margin, exponential_loss(margin), 'g-', linewidth=2, label='Exponential')
plt.plot(margin, squared_loss(margin), 'm-', linewidth=2, label='Squared')

plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5)
plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)
plt.axvline(x=1, color='gray', linestyle=':', alpha=0.3, label='Margin = 1')

plt.xlabel('Margin (y¬∑f(x))', fontsize=12)
plt.ylabel('Loss', fontsize=12)
plt.title('Surrogate Losses for Binary Classification', fontsize=14)
plt.legend(fontsize=10)
plt.grid(True, alpha=0.3)
plt.ylim(-0.1, 3)
plt.xlim(-2, 3)

plt.tight_layout()
plt.savefig('surrogate_losses.png', dpi=300, bbox_inches='tight')
print("Saved surrogate_losses.png")

print("\n=== Loss Values at Key Margins ===")
margins_test = [-1, 0, 0.5, 1, 2]

for m in margins_test:
    print(f"\nMargin = {m}:")
    print(f"  0-1:         {zero_one_loss(np.array([m]))[0]:.4f}")
    print(f"  Hinge:       {hinge_loss(np.array([m]))[0]:.4f}")
    print(f"  Logistic:    {logistic_loss(np.array([m]))[0]:.4f}")
    print(f"  Exponential: {exponential_loss(np.array([m]))[0]:.4f}")
    print(f"  Squared:     {squared_loss(np.array([m]))[0]:.4f}")
```

## Custom Loss Functions

### Implementing Custom Losses

**PyTorch Template:**
```python
import torch
import torch.nn as nn

class CustomLoss(nn.Module):
    """Template for custom loss function"""

    def __init__(self, param1=1.0, param2=0.5):
        super().__init__()
        self.param1 = param1
        self.param2 = param2

    def forward(self, y_pred, y_true):
        """
        Args:
            y_pred: Model predictions (batch_size, ...)
            y_true: Ground truth (batch_size, ...)

        Returns:
            loss: Scalar loss value
        """
        # Implement loss computation
        loss = self.compute_loss(y_pred, y_true)

        return loss

    def compute_loss(self, y_pred, y_true):
        """Core loss computation"""
        # Example: Weighted MSE + MAE
        mse = torch.mean((y_pred - y_true) ** 2)
        mae = torch.mean(torch.abs(y_pred - y_true))

        loss = self.param1 * mse + self.param2 * mae
        return loss

# Usage
criterion = CustomLoss(param1=0.7, param2=0.3)
loss = criterion(predictions, targets)
loss.backward()
```

**TensorFlow/Keras Template:**
```python
import tensorflow as tf

def custom_loss(param1=1.0, param2=0.5):
    """Factory function for custom loss"""

    def loss_fn(y_true, y_pred):
        """
        Args:
            y_true: Ground truth tensor
            y_pred: Prediction tensor

        Returns:
            loss: Scalar tensor
        """
        # Ensure same shape
        y_true = tf.cast(y_true, y_pred.dtype)

        # Compute loss components
        mse = tf.reduce_mean(tf.square(y_pred - y_true))
        mae = tf.reduce_mean(tf.abs(y_pred - y_true))

        # Combine
        loss = param1 * mse + param2 * mae

        return loss

    return loss_fn

# Usage
model.compile(
    optimizer='adam',
    loss=custom_loss(param1=0.7, param2=0.3)
)
```

### Advanced: Composite Losses

**Example: Perceptual Loss (for Image Generation)**
```python
class PerceptualLoss(nn.Module):
    """Combines pixel-level and perceptual losses"""

    def __init__(self, feature_extractor, layer_weights=None):
        super().__init__()
        self.feature_extractor = feature_extractor
        self.layer_weights = layer_weights or [1.0, 1.0, 1.0]

        # Freeze feature extractor
        for param in self.feature_extractor.parameters():
            param.requires_grad = False

    def forward(self, generated, target):
        # Pixel loss (L1)
        pixel_loss = torch.mean(torch.abs(generated - target))

        # Feature loss
        gen_features = self.feature_extractor(generated)
        target_features = self.feature_extractor(target)

        feature_loss = 0
        for i, (gen_feat, target_feat) in enumerate(zip(gen_features, target_features)):
            feature_loss += self.layer_weights[i] * torch.mean(
                torch.abs(gen_feat - target_feat)
            )

        # Total loss
        total_loss = pixel_loss + 0.1 * feature_loss

        return total_loss, pixel_loss, feature_loss
```

### Validation and Testing

**Loss Sanity Checks:**
```python
def test_loss_function(loss_fn):
    """Test properties of custom loss"""

    # 1. Test gradient flow
    y_pred = torch.randn(10, 5, requires_grad=True)
    y_true = torch.randn(10, 5)

    loss = loss_fn(y_pred, y_true)
    loss.backward()

    assert y_pred.grad is not None, "No gradient!"
    print(f"‚úì Gradient flows: {y_pred.grad.shape}")

    # 2. Test perfect prediction
    y_pred = y_true.clone().detach().requires_grad_(True)
    loss = loss_fn(y_pred, y_true)

    assert loss.item() < 1e-6, f"Loss should be ~0 for perfect pred: {loss.item()}"
    print(f"‚úì Perfect prediction loss: {loss.item():.6f}")

    # 3. Test batch invariance
    single_loss = loss_fn(y_pred[:1], y_true[:1])
    batch_loss = loss_fn(y_pred, y_true)

    # Should be similar (unless using batch statistics)
    print(f"‚úì Single vs batch loss: {single_loss.item():.4f} vs {batch_loss.item():.4f}")

    # 4. Test range
    random_pred = torch.randn_like(y_true)
    random_loss = loss_fn(random_pred, y_true)

    assert random_loss.item() > 0, "Loss should be positive"
    print(f"‚úì Random prediction loss: {random_loss.item():.4f}")

# Test
test_loss_function(nn.MSELoss())
```

---

## References

### Regression Losses

1. **Huber, P. J.** (1964). "Robust estimation of a location parameter." *The Annals of Mathematical Statistics*, 35(1), 73-101.
   - Huber loss derivation

2. **Koenker, R., & Bassett Jr, G.** (1978). "Regression quantiles." *Econometrica*, 46(1), 33-50.
   - Quantile regression and pinball loss

### Classification Losses

3. **Bishop, C. M.** (2006). *Pattern recognition and machine learning*. Springer.
   - Cross-entropy and probabilistic losses

4. **Lin, T. Y., Goyal, P., Girshick, R., He, K., & Doll√°r, P.** (2017). "Focal loss for dense object detection." *ICCV*.
   - Focal loss for class imbalance

5. **M√ºller, R., Kornblith, S., & Hinton, G. E.** (2019). "When does label smoothing help?" *NeurIPS*.
   - Label smoothing analysis

### Metric Learning

6. **Schroff, F., Kalinichenko, D., & Philbin, J.** (2015). "FaceNet: A unified embedding for face recognition and clustering." *CVPR*.
   - Triplet loss for face recognition

7. **Chen, T., Kornblith, S., Norouzi, M., & Hinton, G.** (2020). "A simple framework for contrastive learning of visual representations." *ICML*.
   - SimCLR contrastive loss

### Generative Models

8. **Kingma, D. P., & Welling, M.** (2014). "Auto-encoding variational bayes." *ICLR*.
   - ELBO and VAE

9. **Arjovsky, M., Chintala, S., & Bottou, L.** (2017). "Wasserstein generative adversarial networks." *ICML*.
   - Wasserstein loss for GANs

10. **Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., & Courville, A. C.** (2017). "Improved training of wasserstein gans." *NeurIPS*.
    - WGAN with gradient penalty

### Theoretical Foundations

11. **Goodfellow, I., Bengio, Y., & Courville, A.** (2016). *Deep learning*. MIT Press.
    - Comprehensive deep learning theory

12. **Bottou, L., Curtis, F. E., & Nocedal, J.** (2018). "Optimization methods for large-scale machine learning." *SIAM Review*, 60(2), 223-311.
    - Optimization and convergence analysis

---

*This comprehensive guide covers the mathematical foundations, convergence properties, and practical considerations for loss functions across all major machine learning tasks.*
