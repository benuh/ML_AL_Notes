# AutoML and Neural Architecture Search

Automated Machine Learning and Neural Architecture Search: Let algorithms design better algorithms.

## Table of Contents
1. [Introduction to AutoML](#introduction-to-automl)
2. [AutoML Frameworks](#automl-frameworks)
3. [Neural Architecture Search (NAS)](#neural-architecture-search)
4. [Advanced NAS Methods](#advanced-nas-methods)
5. [Efficient NAS](#efficient-nas)
6. [AutoML for Specific Domains](#automl-for-specific-domains)
7. [Best Practices](#best-practices)

---

## Introduction to AutoML

### What is AutoML?

**AutoML** automates the entire ML pipeline:
1. **Data preprocessing** - Feature engineering, cleaning, encoding
2. **Model selection** - Choose algorithm family
3. **Hyperparameter optimization** - Find best hyperparameters
4. **Architecture search** - Design neural network architectures
5. **Ensemble construction** - Combine multiple models

### Why AutoML?

**Benefits:**
- **Democratizes ML** - Non-experts can build models
- **Saves time** - Automates tedious tasks
- **Often better results** - Explores larger search spaces
- **Reproducible** - Systematic approach

**When to Use:**
- Limited ML expertise
- Quick prototyping
- Baseline model
- Large search space

**When NOT to Use:**
- Need full control and interpretability
- Domain-specific constraints
- Real-time requirements

---

## AutoML Frameworks

### 1. Auto-sklearn

**Best for:** Scikit-learn users, tabular data, quick results.

```python
import autosklearn.classification
import sklearn.metrics

# Basic usage
automl = autosklearn.classification.AutoSklearnClassifier(
    time_left_for_this_task=3600,  # 1 hour
    per_run_time_limit=300,  # 5 min per model
    n_jobs=-1,
    memory_limit=8192  # MB
)

automl.fit(X_train, y_train)

# Predictions
y_pred = automl.predict(X_test)
accuracy = sklearn.metrics.accuracy_score(y_test, y_pred)

print(f"Accuracy: {accuracy:.4f}")

# See models tried
print(automl.leaderboard())

# Get ensemble
print(automl.show_models())
```

**Advanced Configuration:**
```python
from autosklearn.classification import AutoSklearnClassifier
from autosklearn.metrics import accuracy

automl = AutoSklearnClassifier(
    time_left_for_this_task=7200,  # 2 hours
    per_run_time_limit=300,

    # Model constraints
    include={
        'classifier': ['random_forest', 'gradient_boosting', 'xgboost'],
        'preprocessor': ['no_preprocessing', 'standardize']
    },

    # Ensemble
    ensemble_size=50,
    ensemble_nbest=200,

    # Resampling
    resampling_strategy='cv',
    resampling_strategy_arguments={'folds': 5},

    # Metrics
    metric=accuracy,

    # Parallelization
    n_jobs=-1,
)

automl.fit(
    X_train, y_train,
    dataset_name='my_dataset',
    feat_type=['Numerical'] * X_train.shape[1]  # Feature types
)

# Get statistics
print(automl.sprint_statistics())

# Get best model
print(f"Best model: {automl.show_models()}")

# Feature importance (if available)
if hasattr(automl, 'feature_importances_'):
    importances = automl.feature_importances_
    for i, imp in enumerate(importances):
        print(f"Feature {i}: {imp:.4f}")
```

---

### 2. TPOT (Tree-based Pipeline Optimization Tool)

**Best for:** Feature engineering, pipeline optimization.

```python
from tpot import TPOTClassifier
from sklearn.model_selection import train_test_split

# TPOT with genetic programming
tpot = TPOTClassifier(
    generations=5,  # Number of iterations
    population_size=50,  # Population size
    offspring_size=100,  # Offspring per generation
    mutation_rate=0.9,
    crossover_rate=0.1,
    scoring='accuracy',
    cv=5,
    random_state=42,
    verbosity=2,
    n_jobs=-1,
    max_time_mins=60,  # Time limit
    max_eval_time_mins=5  # Per pipeline
)

tpot.fit(X_train, y_train)

# Evaluate
score = tpot.score(X_test, y_test)
print(f"Test accuracy: {score:.4f}")

# Export best pipeline as Python code
tpot.export('best_pipeline.py')
```

**Generated Pipeline Example:**
```python
# best_pipeline.py (generated by TPOT)
import numpy as np
import pandas as pd
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from tpot.builtins import ZeroCount

# Create pipeline
exported_pipeline = make_pipeline(
    ZeroCount(),
    StandardScaler(),
    GradientBoostingClassifier(
        learning_rate=0.1,
        max_depth=3,
        max_features=0.8,
        min_samples_leaf=5,
        min_samples_split=10,
        n_estimators=100
    )
)

exported_pipeline.fit(X_train, y_train)
results = exported_pipeline.predict(X_test)
```

**Custom Configuration:**
```python
tpot_config = {
    'sklearn.ensemble.RandomForestClassifier': {
        'n_estimators': [100, 200],
        'max_depth': [10, 20, None],
        'min_samples_split': [2, 5, 10],
    },
    'sklearn.ensemble.GradientBoostingClassifier': {
        'n_estimators': [100, 200],
        'learning_rate': [0.01, 0.1, 1.0],
        'max_depth': [3, 5, 7],
    },
    'sklearn.preprocessing.StandardScaler': {},
    'sklearn.preprocessing.RobustScaler': {},
}

tpot = TPOTClassifier(
    config_dict=tpot_config,
    generations=10,
    population_size=50,
    cv=5,
    random_state=42,
    verbosity=2
)

tpot.fit(X_train, y_train)
```

---

### 3. H2O AutoML

**Best for:** Large datasets, distributed computing, production.

```python
import h2o
from h2o.automl import H2OAutoML

# Initialize H2O
h2o.init()

# Convert to H2O frame
train_h2o = h2o.H2OFrame(pd.concat([X_train, y_train], axis=1))
test_h2o = h2o.H2OFrame(pd.concat([X_test, y_test], axis=1))

# Identify predictors and response
x = X_train.columns.tolist()
y = 'target'

# AutoML
aml = H2OAutoML(
    max_runtime_secs=3600,  # 1 hour
    max_models=20,
    seed=42,
    balance_classes=True,  # For imbalanced data
    sort_metric='AUC'
)

aml.train(x=x, y=y, training_frame=train_h2o)

# Leaderboard
lb = aml.leaderboard
print(lb.head(rows=lb.nrows))

# Best model
best_model = aml.leader
print(best_model)

# Predictions
preds = best_model.predict(test_h2o)
```

**Explain Best Model:**
```python
# Model explanation
best_model.explain(test_h2o)

# Feature importance
varimp = best_model.varimp(use_pandas=True)
print(varimp)

# SHAP summary
best_model.shap_summary_plot(test_h2o)

# Partial dependence plots
best_model.pd_plot(test_h2o, column='feature_name')
```

---

### 4. AutoGluon

**Best for:** State-of-the-art accuracy, multi-modal data, deep learning.

```python
from autogluon.tabular import TabularPredictor

# Basic usage
predictor = TabularPredictor(
    label='target',
    eval_metric='accuracy',
    problem_type='binary'  # or 'multiclass', 'regression'
)

predictor.fit(
    train_data=train_df,
    time_limit=3600,  # 1 hour
    presets='best_quality',  # or 'medium_quality', 'good_quality'
)

# Predictions
predictions = predictor.predict(test_df)
probabilities = predictor.predict_proba(test_df)

# Evaluation
performance = predictor.evaluate(test_df)
print(performance)

# Leaderboard
leaderboard = predictor.leaderboard(test_df, silent=True)
print(leaderboard)
```

**Advanced Configuration:**
```python
from autogluon.tabular import TabularPredictor

predictor = TabularPredictor(
    label='target',
    eval_metric='roc_auc',
    path='./autogluon_models/'
)

# Hyperparameter tuning
hyperparameters = {
    'GBM': [
        {'num_boost_round': 100, 'learning_rate': 0.1},
        {'num_boost_round': 200, 'learning_rate': 0.05},
    ],
    'CAT': {},
    'XGB': {},
    'RF': [
        {'n_estimators': 100, 'max_depth': 10},
        {'n_estimators': 200, 'max_depth': 15},
    ],
    'NN_TORCH': [
        {'num_epochs': 10, 'learning_rate': 0.001},
    ],
}

predictor.fit(
    train_data=train_df,
    time_limit=7200,
    hyperparameters=hyperparameters,
    num_bag_folds=5,  # Bagging
    num_bag_sets=1,
    num_stack_levels=1,  # Stacking
)

# Feature importance
importance = predictor.feature_importance(test_df)
print(importance)
```

---

### 5. PyCaret

**Best for:** Quick prototyping, comparison, deployment.

```python
from pycaret.classification import *

# Initialize setup
clf_setup = setup(
    data=train_df,
    target='target',
    session_id=42,
    normalize=True,
    transformation=True,
    ignore_low_variance=True,
    remove_multicollinearity=True,
    multicollinearity_threshold=0.9,
    train_size=0.8
)

# Compare all models
best_model = compare_models(
    n_select=1,
    sort='Accuracy',
    turbo=True
)

# Tune best model
tuned_model = tune_model(
    best_model,
    optimize='Accuracy',
    n_iter=50,
    search_library='optuna',
    search_algorithm='tpe'
)

# Create ensemble
bagged_model = ensemble_model(tuned_model, method='Bagging', n_estimators=10)
boosted_model = ensemble_model(tuned_model, method='Boosting', n_estimators=10)

# Blend top models
blend_models = blend_models(
    estimator_list=compare_models(n_select=3),
    method='soft'
)

# Stack models
stacked_model = stack_models(
    estimator_list=compare_models(n_select=5),
    meta_model=create_model('lr')
)

# Evaluate
evaluate_model(stacked_model)

# Predict
predictions = predict_model(stacked_model, data=test_df)

# Save model
save_model(stacked_model, 'final_model')
```

---

## Neural Architecture Search (NAS)

### What is NAS?

**NAS** automatically designs neural network architectures.

**Search Space Components:**
1. **Macro-architecture** - Number of layers, skip connections
2. **Micro-architecture** - Cell design, operations
3. **Hyperparameters** - Learning rate, optimizer

### Reinforcement Learning-based NAS

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

class NASController(nn.Module):
    """RNN controller for architecture search"""

    def __init__(self, num_layers=3, hidden_size=128):
        super().__init__()
        self.num_layers = num_layers

        # RNN controller
        self.lstm = nn.LSTM(
            input_size=32,
            hidden_size=hidden_size,
            num_layers=1,
            batch_first=True
        )

        # Output heads for architecture decisions
        self.layer_type = nn.Linear(hidden_size, 3)  # conv, pool, fc
        self.kernel_size = nn.Linear(hidden_size, 3)  # 3, 5, 7
        self.num_filters = nn.Linear(hidden_size, 4)  # 32, 64, 128, 256

    def forward(self, batch_size=1):
        """Generate architecture"""
        # Start token
        input_embed = torch.randn(batch_size, 1, 32)

        architecture = []
        hidden = None

        for i in range(self.num_layers):
            # LSTM step
            output, hidden = self.lstm(input_embed, hidden)

            # Sample architecture decisions
            layer_logits = self.layer_type(output.squeeze(1))
            kernel_logits = self.kernel_size(output.squeeze(1))
            filters_logits = self.num_filters(output.squeeze(1))

            # Sample from categorical distribution
            layer_type = torch.multinomial(torch.softmax(layer_logits, dim=1), 1)
            kernel_size = torch.multinomial(torch.softmax(kernel_logits, dim=1), 1)
            num_filters = torch.multinomial(torch.softmax(filters_logits, dim=1), 1)

            architecture.append({
                'layer_type': layer_type.item(),
                'kernel_size': [3, 5, 7][kernel_size.item()],
                'num_filters': [32, 64, 128, 256][num_filters.item()]
            })

            # Next input is current output
            input_embed = output

        return architecture, (layer_logits, kernel_logits, filters_logits)

class NASSearcher:
    """NAS with REINFORCE"""

    def __init__(self, controller, train_loader, val_loader):
        self.controller = controller
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.controller_optimizer = optim.Adam(controller.parameters(), lr=0.001)

    def build_model(self, architecture):
        """Build CNN from architecture"""
        layers = []
        in_channels = 3

        for i, layer_config in enumerate(architecture):
            if layer_config['layer_type'] == 0:  # Conv
                layers.append(nn.Conv2d(
                    in_channels,
                    layer_config['num_filters'],
                    kernel_size=layer_config['kernel_size'],
                    padding=layer_config['kernel_size']//2
                ))
                layers.append(nn.ReLU())
                layers.append(nn.BatchNorm2d(layer_config['num_filters']))
                in_channels = layer_config['num_filters']

            elif layer_config['layer_type'] == 1:  # Pool
                layers.append(nn.MaxPool2d(2))

        layers.append(nn.AdaptiveAvgPool2d(1))
        layers.append(nn.Flatten())
        layers.append(nn.Linear(in_channels, 10))

        return nn.Sequential(*layers)

    def train_child_model(self, architecture, epochs=10):
        """Train and evaluate child model"""
        model = self.build_model(architecture)
        optimizer = optim.Adam(model.parameters(), lr=0.01)
        criterion = nn.CrossEntropyLoss()

        # Train
        for epoch in range(epochs):
            for X_batch, y_batch in self.train_loader:
                optimizer.zero_grad()
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)
                loss.backward()
                optimizer.step()

        # Validate
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for X_batch, y_batch in self.val_loader:
                outputs = model(X_batch)
                _, predicted = outputs.max(1)
                total += y_batch.size(0)
                correct += predicted.eq(y_batch).sum().item()

        accuracy = correct / total
        return accuracy

    def search(self, num_iterations=100):
        """Search for best architecture"""
        best_architecture = None
        best_accuracy = 0

        for iteration in range(num_iterations):
            # Sample architecture
            architecture, logits = self.controller(batch_size=1)

            # Train child model
            accuracy = self.train_child_model(architecture)

            # REINFORCE update
            reward = accuracy  # Use accuracy as reward

            # Calculate loss
            all_logits = torch.cat([l for l in logits])
            log_probs = torch.log_softmax(all_logits, dim=1)

            # Policy gradient loss
            loss = -log_probs.mean() * reward  # Maximize reward

            self.controller_optimizer.zero_grad()
            loss.backward()
            self.controller_optimizer.step()

            # Track best
            if accuracy > best_accuracy:
                best_accuracy = accuracy
                best_architecture = architecture

            print(f"Iteration {iteration+1}: Accuracy = {accuracy:.4f}")

        return best_architecture, best_accuracy

# Usage
controller = NASController(num_layers=5)
searcher = NASSearcher(controller, train_loader, val_loader)
best_arch, best_acc = searcher.search(num_iterations=50)

print(f"\nBest Architecture: {best_arch}")
print(f"Best Accuracy: {best_acc:.4f}")
```

---

### Evolutionary NAS

```python
import random
import copy

class EvolutionaryNAS:
    """NAS with evolutionary algorithm"""

    def __init__(self, population_size=20, num_generations=50):
        self.population_size = population_size
        self.num_generations = num_generations

    def random_architecture(self):
        """Generate random architecture"""
        num_layers = random.randint(3, 10)
        architecture = []

        for _ in range(num_layers):
            architecture.append({
                'type': random.choice(['conv', 'pool', 'residual']),
                'filters': random.choice([32, 64, 128, 256]),
                'kernel': random.choice([3, 5, 7]),
                'activation': random.choice(['relu', 'gelu', 'swish'])
            })

        return architecture

    def mutate(self, architecture):
        """Mutate architecture"""
        arch = copy.deepcopy(architecture)
        mutation_type = random.choice(['add', 'remove', 'modify'])

        if mutation_type == 'add' and len(arch) < 15:
            # Add random layer
            position = random.randint(0, len(arch))
            arch.insert(position, {
                'type': random.choice(['conv', 'pool', 'residual']),
                'filters': random.choice([32, 64, 128, 256]),
                'kernel': random.choice([3, 5, 7]),
                'activation': random.choice(['relu', 'gelu', 'swish'])
            })

        elif mutation_type == 'remove' and len(arch) > 3:
            # Remove random layer
            position = random.randint(0, len(arch)-1)
            arch.pop(position)

        else:  # modify
            # Modify random layer
            if len(arch) > 0:
                position = random.randint(0, len(arch)-1)
                arch[position]['filters'] = random.choice([32, 64, 128, 256])

        return arch

    def crossover(self, parent1, parent2):
        """Crossover two architectures"""
        # Single-point crossover
        point = min(len(parent1), len(parent2)) // 2
        child = parent1[:point] + parent2[point:]
        return child

    def evaluate(self, architecture):
        """Evaluate architecture (placeholder)"""
        # In practice, train model and return validation accuracy
        # Here we use a dummy score based on architecture
        score = random.random()  # Replace with actual training
        return score

    def search(self):
        """Evolutionary search"""
        # Initialize population
        population = [self.random_architecture()
                     for _ in range(self.population_size)]

        best_architecture = None
        best_fitness = 0

        for generation in range(self.num_generations):
            # Evaluate population
            fitness_scores = [(arch, self.evaluate(arch))
                            for arch in population]
            fitness_scores.sort(key=lambda x: x[1], reverse=True)

            # Track best
            if fitness_scores[0][1] > best_fitness:
                best_fitness = fitness_scores[0][1]
                best_architecture = fitness_scores[0][0]

            print(f"Generation {generation+1}: Best fitness = {best_fitness:.4f}")

            # Selection (keep top 50%)
            selected = [arch for arch, _ in fitness_scores[:self.population_size//2]]

            # Create next generation
            next_generation = selected.copy()

            # Crossover
            while len(next_generation) < self.population_size:
                parent1 = random.choice(selected)
                parent2 = random.choice(selected)
                child = self.crossover(parent1, parent2)

                # Mutation
                if random.random() < 0.3:  # 30% mutation rate
                    child = self.mutate(child)

                next_generation.append(child)

            population = next_generation

        return best_architecture, best_fitness

# Usage
evo_nas = EvolutionaryNAS(population_size=20, num_generations=30)
best_arch, best_fitness = evo_nas.search()
```

---

## Advanced NAS Methods

### DARTS (Differentiable Architecture Search)

**Key Innovation:** Make architecture search continuous and differentiable.

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MixedOp(nn.Module):
    """Mixed operation with architecture weights"""

    def __init__(self, in_channels, out_channels):
        super().__init__()

        # Candidate operations
        self.ops = nn.ModuleList([
            nn.Conv2d(in_channels, out_channels, 3, padding=1),  # 3x3 conv
            nn.Conv2d(in_channels, out_channels, 5, padding=2),  # 5x5 conv
            nn.MaxPool2d(3, stride=1, padding=1),  # max pool
            nn.Identity(),  # skip connection
        ])

    def forward(self, x, weights):
        """Forward with weighted sum of operations"""
        return sum(w * op(x) for w, op in zip(weights, self.ops))

class DARTSCell(nn.Module):
    """DARTS cell"""

    def __init__(self, in_channels, out_channels, num_nodes=4):
        super().__init__()
        self.num_nodes = num_nodes

        # Mixed operations for each edge
        self.ops = nn.ModuleList()
        for i in range(num_nodes):
            for j in range(i+1):  # i+1 predecessors
                self.ops.append(MixedOp(in_channels if j < 2 else out_channels,
                                       out_channels))

    def forward(self, x, arch_weights):
        """Forward pass"""
        states = [x, x]  # Two inputs
        offset = 0

        for i in range(self.num_nodes):
            # Aggregate from all predecessors
            s = sum(self.ops[offset+j](h, arch_weights[offset+j])
                   for j, h in enumerate(states))
            offset += len(states)
            states.append(s)

        # Concatenate intermediate nodes
        return torch.cat(states[-self.num_nodes:], dim=1)

class DARTS(nn.Module):
    """DARTS search model"""

    def __init__(self, num_cells=8, num_nodes=4, num_classes=10):
        super().__init__()

        # Stem
        self.stem = nn.Conv2d(3, 16, 3, padding=1)

        # Cells
        self.cells = nn.ModuleList([
            DARTSCell(16, 16, num_nodes) for _ in range(num_cells)
        ])

        # Classifier
        self.classifier = nn.Linear(16 * num_nodes, num_classes)

        # Architecture weights (learnable)
        num_ops = 4  # Number of operations
        num_edges = sum(i+1 for i in range(num_nodes))
        self.arch_weights = nn.Parameter(torch.randn(num_edges, num_ops))

    def forward(self, x):
        # Stem
        x = self.stem(x)

        # Cells
        arch_probs = F.softmax(self.arch_weights, dim=-1)
        for cell in self.cells:
            x = cell(x, arch_probs)

        # Classifier
        x = F.adaptive_avg_pool2d(x, 1).view(x.size(0), -1)
        x = self.classifier(x)

        return x

    def get_architecture(self):
        """Extract discrete architecture"""
        arch_probs = F.softmax(self.arch_weights, dim=-1)

        # Select operation with highest weight for each edge
        architecture = []
        for edge_probs in arch_probs:
            best_op = edge_probs.argmax().item()
            architecture.append(best_op)

        return architecture

# Training DARTS
def train_darts(model, train_loader, val_loader, epochs=50):
    # Two optimizers: weights and architecture
    weight_optimizer = optim.SGD(
        model.parameters(),
        lr=0.025,
        momentum=0.9,
        weight_decay=3e-4
    )

    arch_optimizer = optim.Adam(
        [model.arch_weights],
        lr=3e-4,
        weight_decay=1e-3
    )

    for epoch in range(epochs):
        # Alternate between weight and architecture updates
        for (X_train, y_train), (X_val, y_val) in zip(train_loader, val_loader):
            # Update architecture
            arch_optimizer.zero_grad()
            logits = model(X_val)
            loss = F.cross_entropy(logits, y_val)
            loss.backward()
            arch_optimizer.step()

            # Update weights
            weight_optimizer.zero_grad()
            logits = model(X_train)
            loss = F.cross_entropy(logits, y_train)
            loss.backward()
            weight_optimizer.step()

    # Extract final architecture
    final_arch = model.get_architecture()
    return final_arch
```

---

## Efficient NAS

### One-Shot NAS (Weight Sharing)

```python
class SuperNet(nn.Module):
    """SuperNet containing all possible architectures"""

    def __init__(self, num_layers=5):
        super().__init__()

        # Supernet with all possible operations
        self.layers = nn.ModuleList()
        for i in range(num_layers):
            self.layers.append(nn.ModuleDict({
                'conv3x3_32': nn.Conv2d(3 if i == 0 else 32, 32, 3, padding=1),
                'conv3x3_64': nn.Conv2d(3 if i == 0 else 64, 64, 3, padding=1),
                'conv5x5_32': nn.Conv2d(3 if i == 0 else 32, 32, 5, padding=2),
                'conv5x5_64': nn.Conv2d(3 if i == 0 else 64, 64, 5, padding=2),
                'pool': nn.MaxPool2d(3, stride=1, padding=1),
            }))

    def forward(self, x, architecture):
        """Forward with specific architecture path"""
        for i, layer_ops in enumerate(self.layers):
            op_name = architecture[i]
            x = layer_ops[op_name](x)
            x = F.relu(x)
        return x

def train_supernet(supernet, train_loader, epochs=50):
    """Train supernet with random path sampling"""
    optimizer = optim.SGD(supernet.parameters(), lr=0.01, momentum=0.9)

    for epoch in range(epochs):
        for X_batch, y_batch in train_loader:
            # Sample random architecture
            architecture = [
                random.choice(['conv3x3_32', 'conv3x3_64', 'conv5x5_32',
                              'conv5x5_64', 'pool'])
                for _ in range(len(supernet.layers))
            ]

            # Train sampled architecture
            optimizer.zero_grad()
            outputs = supernet(X_batch, architecture)
            loss = F.cross_entropy(outputs, y_batch)
            loss.backward()
            optimizer.step()

def search_supernet(supernet, val_loader, num_samples=100):
    """Search best architecture in trained supernet"""
    best_arch = None
    best_acc = 0

    for _ in range(num_samples):
        # Sample architecture
        architecture = [
            random.choice(['conv3x3_32', 'conv3x3_64', 'conv5x5_32',
                          'conv5x5_64', 'pool'])
            for _ in range(len(supernet.layers))
        ]

        # Evaluate
        supernet.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                outputs = supernet(X_batch, architecture)
                _, predicted = outputs.max(1)
                total += y_batch.size(0)
                correct += predicted.eq(y_batch).sum().item()

        acc = correct / total
        if acc > best_acc:
            best_acc = acc
            best_arch = architecture

    return best_arch, best_acc
```

---

## AutoML for Specific Domains

### AutoML for Computer Vision

```python
from autogluon.vision import ImagePredictor

# Image classification
predictor = ImagePredictor()

predictor.fit(
    train_data='./train/',
    time_limit=3600,
    presets='medium_quality_faster_train'
)

# Predictions
predictions = predictor.predict('./test/')

# Best model
print(predictor.fit_summary())
```

### AutoML for NLP

```python
from autogluon.text import TextPredictor

# Text classification
predictor = TextPredictor(
    label='sentiment',
    eval_metric='acc'
)

predictor.fit(
    train_data=train_df,
    time_limit=3600,
    presets='medium_quality'
)

# Predictions
predictions = predictor.predict(test_df)

# Leaderboard
print(predictor.leaderboard())
```

### AutoML for Time Series

```python
from autogluon.timeseries import TimeSeriesPredictor

# Forecasting
predictor = TimeSeriesPredictor(
    target='sales',
    prediction_length=24,  # Forecast 24 steps ahead
    eval_metric='MASE'
)

predictor.fit(
    train_data=train_df,
    time_limit=3600,
    presets='best_quality'
)

# Forecast
predictions = predictor.predict(test_df)
```

---

## Best Practices

### 1. Define Budget and Constraints

```python
# Time-based budget
automl = AutoML(
    time_budget=3600,  # 1 hour
    early_stop=True
)

# Resource-based budget
automl = AutoML(
    max_models=50,
    memory_limit=8192,  # 8GB
    n_jobs=-1
)
```

### 2. Warm Start from Previous Search

```python
# Save search history
automl.fit(X_train, y_train)
automl.save('automl_state.pkl')

# Resume search
automl_resumed = AutoML.load('automl_state.pkl')
automl_resumed.fit(X_train_new, y_train_new, time_budget=1800)
```

### 3. Transfer Learning for NAS

```python
class TransferNAS:
    """Transfer architecture from similar task"""

    def __init__(self):
        self.architecture_db = {}

    def save_architecture(self, task_name, architecture, performance):
        self.architecture_db[task_name] = {
            'arch': architecture,
            'perf': performance
        }

    def get_similar_architecture(self, new_task, similarity_fn):
        # Find most similar task
        similarities = {
            task: similarity_fn(new_task, task)
            for task in self.architecture_db.keys()
        }

        most_similar = max(similarities, key=similarities.get)
        return self.architecture_db[most_similar]['arch']
```

### 4. Multi-Objective NAS

```python
from pymoo.algorithms.moo.nsga2 import NSGA2
from pymoo.optimize import minimize

class MultiObjectiveNAS:
    """NAS optimizing accuracy and model size"""

    def evaluate(self, architecture):
        # Train model
        accuracy = train_and_evaluate(architecture)

        # Compute model size
        model = build_model(architecture)
        num_params = sum(p.numel() for p in model.parameters())

        # Return objectives (minimize both)
        return -accuracy, num_params  # Negative accuracy to minimize

    def search(self, population_size=20, generations=50):
        # Define problem
        from pymoo.core.problem import Problem

        class NASProblem(Problem):
            def _evaluate(self, architectures, out, *args, **kwargs):
                results = [self.evaluate(arch) for arch in architectures]
                out["F"] = np.array(results)

        problem = NASProblem()
        algorithm = NSGA2(pop_size=population_size)

        # Optimize
        res = minimize(
            problem,
            algorithm,
            ('n_gen', generations),
            verbose=True
        )

        # Return Pareto front
        return res.X, res.F
```

---

## Summary

| Method | Best For | Speed | Accuracy |
|--------|----------|-------|----------|
| **Auto-sklearn** | Tabular data, scikit-learn users | Fast | Good |
| **TPOT** | Feature engineering, pipelines | Medium | Good |
| **H2O AutoML** | Large data, distributed | Fast | Good |
| **AutoGluon** | State-of-the-art, multi-modal | Slow | Excellent |
| **PyCaret** | Quick prototyping | Fast | Good |
| **RL-NAS** | Novel architectures | Very Slow | Excellent |
| **DARTS** | Efficient search | Fast | Good |
| **One-Shot NAS** | Very efficient | Very Fast | Good |

---

## Key Takeaways

1. **Start with AutoML frameworks** before custom NAS
2. **Define budget clearly** - time, compute, models
3. **AutoGluon for accuracy**, **Auto-sklearn for speed**
4. **DARTS for efficient NAS**, **RL-NAS for best architectures**
5. **Use transfer learning** from similar tasks
6. **Multi-objective optimization** for production constraints
7. **Validate thoroughly** - AutoML can overfit to validation set

**Next Steps:**
- Try AutoGluon on your dataset
- Implement DARTS for architecture search
- Build AutoML pipeline for your domain
- Contribute to open-source AutoML projects
