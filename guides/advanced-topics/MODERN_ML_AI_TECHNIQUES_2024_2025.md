# üöÄ Modern ML/AI Techniques (2024-2025)

**A Comprehensive Guide to State-of-the-Art Machine Learning and AI**

> Last Updated: November 2025
> This guide covers cutting-edge techniques, architectures, and methodologies that define the current state of AI.
> **Note:** Model specifications, especially for proprietary models, are based on publicly available information and may not reflect actual implementations.

---

## üìã Table of Contents

1. [Large Language Models (LLMs)](#large-language-models)
2. [Diffusion Models](#diffusion-models)
3. [Vision Transformers](#vision-transformers)
4. [Retrieval-Augmented Generation (RAG)](#retrieval-augmented-generation)
5. [Mixture of Experts (MoE)](#mixture-of-experts)
6. [Constitutional AI & RLHF](#constitutional-ai-and-rlhf)
7. [Model Quantization](#model-quantization)
8. [Multimodal Models](#multimodal-models)
9. [Efficient Training Techniques](#efficient-training-techniques)
10. [Emerging Architectures](#emerging-architectures)

---

## ü§ñ Large Language Models (LLMs)

### Overview
Large Language Models have revolutionized NLP and are now being applied across domains. Models with billions to trillions of parameters can understand and generate human-like text.

### Key Models (2024-2025)

#### **GPT-4 and GPT-4 Turbo** (OpenAI)
- **Architecture:** Decoder-only transformer
  - *Exact parameters undisclosed; speculation suggests MoE architecture with ~1T+ total parameters*
  - *Note: OpenAI has not confirmed architectural details publicly*
- **Key Features:**
  - 128K context window
  - Multimodal (text + vision)
  - Improved reasoning and factuality
- **Applications:** Code generation, analysis, creative writing, complex reasoning

#### **Claude (Sonnet 4.5, Opus, Sonnet 3.5, Haiku)** (Anthropic)
- **Architecture:** Constitutional AI with RLHF
- **Key Features:**
  - 200K context window
  - Strong safety alignment
  - Excellent instruction following
  - Extended thinking capabilities (Sonnet 4+)
- **Applications:** Long-document analysis, safe AI assistants, code review, complex reasoning

#### **Llama 3** (Meta)
- **Parameters:** 8B, 70B, 405B variants
- **Key Features:**
  - Open source and permissive license
  - Strong performance on benchmarks
  - Efficient fine-tuning
- **Applications:** Research, custom deployments, on-premise solutions

#### **Gemini Ultra/Pro/Nano** (Google DeepMind)
- **Architecture:** Multimodal from the ground up
- **Key Features:**
  - Native multimodal understanding
  - Efficient Nano variant for mobile
  - Strong mathematical reasoning
- **Applications:** Multimodal tasks, mobile deployment, scientific computing

### Training Techniques

#### **1. Chinchilla Scaling Laws**
Optimal model training requires balancing model size and training tokens for compute-optimal performance:
```
Optimal tokens ‚âà 20 √ó parameters (for a given compute budget)
```
**More precisely:** For compute-optimal training, the number of training tokens should scale proportionally with the number of parameters. The Chinchilla paper found that many large models (like GPT-3) were significantly undertrained relative to their size.

**Example:**
- 70B parameter model ‚Üí ~1.4T training tokens (compute-optimal)
- GPT-3 (175B params) was trained on 300B tokens ‚Üí undertrained by ~6x

**Key Insight:** Smaller models trained on more data can outperform larger undertrained models, especially when compute budget is fixed. This suggests investing more in high-quality training data rather than just increasing model size.

**Reference:** *Training Compute-Optimal Large Language Models* - Hoffmann et al. (2022)

#### **2. Instruction Tuning**
Fine-tuning pretrained LLMs on instruction-following datasets:
```python
# Example instruction format
{
    "instruction": "Summarize the following text",
    "input": "Long text here...",
    "output": "Summary here..."
}
```

**Popular Datasets:**
- FLAN (Fine-tuned Language Net)
- Alpaca (52K instructions)
- Dolly (15K human-generated)
- OpenOrca (synthetic instructions)

**Reference:** *Scaling Instruction-Finetuned Language Models* - Chung et al. (2022)

#### **3. Parameter-Efficient Fine-Tuning (PEFT)**

**LoRA (Low-Rank Adaptation):**
Instead of updating all weights, inject trainable low-rank matrices:
```
W = W‚ÇÄ + BA
```
Where:
- W‚ÇÄ: frozen pretrained weights (d √ó k)
- B: trainable matrix (d √ó r)
- A: trainable matrix (r √ó k)
- r << min(d, k) (typically r = 8-64)

**Memory Savings:** Only train ~0.1-1% of parameters!

```python
# Example with HuggingFace PEFT
from peft import LoraConfig, get_peft_model

config = LoraConfig(
    r=16,                      # Rank
    lora_alpha=32,             # Scaling factor
    target_modules=["q", "v"], # Which layers to adapt
    lora_dropout=0.1,
    bias="none"
)

model = get_peft_model(base_model, config)
# Train only ~0.5% of parameters!
```

**QLoRA (Quantized LoRA):**
Combines LoRA with 4-bit quantization:
- Base model in 4-bit (NormalFloat4)
- LoRA adapters in 16-bit
- **Enables fine-tuning 65B models on a single 48GB GPU!**

**Reference:** *LoRA: Low-Rank Adaptation of Large Language Models* - Hu et al. (2021)
**Reference:** *QLoRA: Efficient Finetuning of Quantized LLMs* - Dettmers et al. (2023)

### Prompt Engineering Best Practices

#### **Chain-of-Thought (CoT) Prompting**
```python
# Zero-shot CoT
prompt = """
Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls.
Each can has 3 tennis balls. How many tennis balls does he have now?

Let's think step by step:
"""
# Model breaks down reasoning automatically
```

#### **Few-Shot Prompting**
```python
prompt = """
Sentiment classification:

Text: "The movie was fantastic!"
Sentiment: Positive

Text: "Terrible experience, very disappointed."
Sentiment: Negative

Text: "The product works as expected."
Sentiment: {model completes}
"""
```

#### **Tree-of-Thoughts (ToT)**
Explore multiple reasoning paths:
```
Problem
‚îú‚îÄ‚îÄ Approach 1
‚îÇ   ‚îú‚îÄ‚îÄ Step 1a ‚Üí Step 2a ‚Üí Solution A
‚îÇ   ‚îî‚îÄ‚îÄ Step 1b ‚Üí Step 2b ‚Üí Solution B
‚îî‚îÄ‚îÄ Approach 2
    ‚îî‚îÄ‚îÄ Step 1c ‚Üí Step 2c ‚Üí Solution C
```

**Reference:** *Tree of Thoughts: Deliberate Problem Solving with LLMs* - Yao et al. (2023)

---

## üé® Diffusion Models

### Overview
Diffusion models generate high-quality images by learning to reverse a noise-adding process.

### Mathematical Foundation

**Rigorous Theory of Diffusion Models:**

```
Problem: Generate samples from data distribution p_data(x) without explicit density modeling

Key Insight (Sohl-Dickstein et al., 2015):
Reverse a gradual noising process by learning score function ‚àá_x log p(x)
```

**Theorem 1 (Forward Diffusion Process - DDPM):**

```
Forward process (fixed, no learning):
q(x‚Çú | x‚Çú‚Çã‚ÇÅ) = N(x‚Çú; ‚àö(1-Œ≤‚Çú)¬∑x‚Çú‚Çã‚ÇÅ, Œ≤‚ÇúI)

where Œ≤‚ÇÅ, ..., Œ≤‚Çú is variance schedule with 0 < Œ≤‚Çú < 1

Reparameterization:
x‚Çú = ‚àö(1-Œ≤‚Çú)¬∑x‚Çú‚Çã‚ÇÅ + ‚àöŒ≤‚Çú¬∑Œµ‚Çú‚Çã‚ÇÅ,  Œµ‚Çú‚Çã‚ÇÅ ~ N(0,I)

Closed-form (key property):
Let Œ±‚Çú = 1 - Œ≤‚Çú, ·æ±‚Çú = ‚àè‚Çõ‚Çå‚ÇÅ·µó Œ±‚Çõ

Then:
q(x‚Çú | x‚ÇÄ) = N(x‚Çú; ‚àö·æ±‚Çú¬∑x‚ÇÄ, (1-·æ±‚Çú)I)

Proof (by induction):
x‚Çú = ‚àöŒ±‚Çú¬∑x‚Çú‚Çã‚ÇÅ + ‚àö(1-Œ±‚Çú)¬∑Œµ
   = ‚àöŒ±‚Çú(‚àö·æ±‚Çú‚Çã‚ÇÅ¬∑x‚ÇÄ + ‚àö(1-·æ±‚Çú‚Çã‚ÇÅ)¬∑Œµ') + ‚àö(1-Œ±‚Çú)¬∑Œµ
   = ‚àö(Œ±‚Çú·æ±‚Çú‚Çã‚ÇÅ)¬∑x‚ÇÄ + ‚àö(Œ±‚Çú(1-·æ±‚Çú‚Çã‚ÇÅ) + (1-Œ±‚Çú))¬∑Œµ''  (by Gaussian identity)
   = ‚àö·æ±‚Çú¬∑x‚ÇÄ + ‚àö(1-·æ±‚Çú)¬∑Œµ'' ‚àé

This allows sampling x‚Çú directly from x‚ÇÄ without iterating through all intermediate steps!

Reparameterized sampling:
x‚Çú = ‚àö·æ±‚Çú¬∑x‚ÇÄ + ‚àö(1-·æ±‚Çú)¬∑Œµ,  Œµ ~ N(0,I)

As t ‚Üí T: ·æ±‚Çú ‚Üí 0, x‚Çú ‚Üí N(0,I) (pure noise)
```

**Theorem 2 (Reverse Process and Score Matching):**

```
Reverse process (learned):
pŒ∏(x‚Çú‚Çã‚ÇÅ | x‚Çú) = N(x‚Çú‚Çã‚ÇÅ; ŒºŒ∏(x‚Çú,t), Œ£Œ∏(x‚Çú,t))

Bayes rule (true posterior):
q(x‚Çú‚Çã‚ÇÅ | x‚Çú, x‚ÇÄ) = N(x‚Çú‚Çã‚ÇÅ; ŒºÃÉ‚Çú(x‚Çú,x‚ÇÄ), œÉÃÉ‚Çú¬≤I)

where:
ŒºÃÉ‚Çú(x‚Çú,x‚ÇÄ) = (‚àö·æ±‚Çú‚Çã‚ÇÅ¬∑Œ≤‚Çú)/(1-·æ±‚Çú)¬∑x‚ÇÄ + (‚àöŒ±‚Çú¬∑(1-·æ±‚Çú‚Çã‚ÇÅ))/(1-·æ±‚Çú)¬∑x‚Çú
œÉÃÉ‚Çú¬≤ = ((1-·æ±‚Çú‚Çã‚ÇÅ)/(1-·æ±‚Çú))¬∑Œ≤‚Çú

Rewrite ŒºÃÉ‚Çú in terms of Œµ (noise):
Since x‚Çú = ‚àö·æ±‚Çú¬∑x‚ÇÄ + ‚àö(1-·æ±‚Çú)¬∑Œµ:
x‚ÇÄ = (x‚Çú - ‚àö(1-·æ±‚Çú)¬∑Œµ)/‚àö·æ±‚Çú

Substituting:
ŒºÃÉ‚Çú(x‚Çú,Œµ) = (1/‚àöŒ±‚Çú)¬∑(x‚Çú - (Œ≤‚Çú/‚àö(1-·æ±‚Çú))¬∑Œµ)

Parameterization: Learn ŒµŒ∏(x‚Çú,t) to predict noise
ŒºŒ∏(x‚Çú,t) = (1/‚àöŒ±‚Çú)¬∑(x‚Çú - (Œ≤‚Çú/‚àö(1-·æ±‚Çú))¬∑ŒµŒ∏(x‚Çú,t))

Training objective (DDPM):
L_simple(Œ∏) = E‚Çú,x‚ÇÄ,Œµ[||Œµ - ŒµŒ∏(‚àö·æ±‚Çú¬∑x‚ÇÄ + ‚àö(1-·æ±‚Çú)¬∑Œµ, t)||¬≤]

This is equivalent to score matching:
‚àá_x log p(x‚Çú) = -Œµ/‚àö(1-·æ±‚Çú)
```

**Theorem 3 (ELBO Derivation - Ho et al., 2020):**

```
Full training objective (variational lower bound):
L_VLB = E‚Çì‚ÇÄ[-log pŒ∏(x‚ÇÄ)]
      ‚â§ E‚Çì‚ÇÄ,q[KL(q(x‚Çú|x‚ÇÄ) || pŒ∏(x‚Çú)) + Œ£‚Çú KL(q(x‚Çú‚Çã‚ÇÅ|x‚Çú,x‚ÇÄ) || pŒ∏(x‚Çú‚Çã‚ÇÅ|x‚Çú))]

Simplification (Ho et al.):
All KL terms are between Gaussians ‚Üí closed form!

KL(N(Œº‚ÇÅ,Œ£‚ÇÅ) || N(Œº‚ÇÇ,Œ£‚ÇÇ)) = (1/2)[tr(Œ£‚ÇÇ‚Åª¬πŒ£‚ÇÅ) + (Œº‚ÇÇ-Œº‚ÇÅ)·µÄŒ£‚ÇÇ‚Åª¬π(Œº‚ÇÇ-Œº‚ÇÅ) - k + log(det(Œ£‚ÇÇ)/det(Œ£‚ÇÅ))]

For fixed Œ£Œ∏ = œÉÃÉ‚Çú¬≤I (same as true posterior):
L_t ‚àù ||ŒºÃÉ‚Çú - ŒºŒ∏||¬≤

Substituting ŒºÃÉ‚Çú = (1/‚àöŒ±‚Çú)(x‚Çú - Œ≤‚Çú/‚àö(1-·æ±‚Çú)¬∑Œµ):
L_t ‚àù (Œ≤‚Çú¬≤/(2Œ±‚Çú(1-·æ±‚Çú)))¬∑||Œµ - ŒµŒ∏||¬≤

Ignoring weighting factor (Ho et al. finding):
L_simple = E‚Çú[||Œµ - ŒµŒ∏||¬≤] works better empirically!

Sample complexity (Theorem - Song et al., 2021):
To achieve Œµ-accurate samples from p_data:
n_samples = O((d/Œµ¬≤)¬∑log(T/Œ¥))

where d = data dimensionality, T = diffusion steps
```

**Theorem 4 (DDIM - Deterministic Sampling):**

```
DDPM sampling: Stochastic (Langevin dynamics)
x‚Çú‚Çã‚ÇÅ = (1/‚àöŒ±‚Çú)(x‚Çú - (1-Œ±‚Çú)/‚àö(1-·æ±‚Çú)¬∑ŒµŒ∏(x‚Çú,t)) + œÉ‚Çú¬∑z,  z ~ N(0,I)

DDIM (Song et al., 2020): Deterministic (ODE)
x‚Çú‚Çã‚ÇÅ = ‚àö·æ±‚Çú‚Çã‚ÇÅ¬∑xÃÇ‚ÇÄ + ‚àö(1-·æ±‚Çú‚Çã‚ÇÅ)¬∑ŒµŒ∏(x‚Çú,t)

where xÃÇ‚ÇÄ = (x‚Çú - ‚àö(1-·æ±‚Çú)¬∑ŒµŒ∏(x‚Çú,t))/‚àö·æ±‚Çú  (predicted clean image)

Key advantages:
1. Deterministic: Same noise ‚Üí same output
2. Faster: Skip timesteps (100 ‚Üí 10 steps)
3. Inversion: Can encode real images to latent

Theorem (DDIM ODE Connection):
DDIM update is discretization of probability flow ODE:

dx_t/dt = f(x_t, t) - (1/2)g(t)¬≤¬∑‚àá_x log p_t(x_t)

where:
f(x_t, t) = drift coefficient
g(t) = diffusion coefficient
‚àá_x log p_t(x_t) = score function (predicted by ŒµŒ∏)

Continuous limit: DDIM converges to ODE solution
Speedup: 10-50√ó faster than DDPM with similar quality
```

**Theorem 5 (Latent Diffusion Models - Rombach et al., 2022):**

```
Problem: High-resolution images (1024√ó1024) have d ‚âà 3M dimensions
Diffusion complexity: O(T¬∑d¬≤) for attention ‚Üí prohibitive!

Solution: Work in latent space

Encoder: E: ‚Ñù·¥¥À£·µÇÀ£¬≥ ‚Üí ‚Ñù ∞À£ ∑À£·∂ú  (VAE encoder)
Decoder: D: ‚Ñù ∞À£ ∑À£·∂ú ‚Üí ‚Ñù·¥¥À£·µÇÀ£¬≥  (VAE decoder)

Typical compression: 8√ó (1024¬≤ ‚Üí 128¬≤)

Training:
1. Pre-train VAE: min_E,D [||x - D(E(x))||¬≤ + KL regularization]
2. Train diffusion in latent space z = E(x)

Modified diffusion:
q(z‚Çú | z‚Çú‚Çã‚ÇÅ) = N(z‚Çú; ‚àö(1-Œ≤‚Çú)¬∑z‚Çú‚Çã‚ÇÅ, Œ≤‚ÇúI)

Complexity reduction:
Original: O(T¬∑H¬≤W¬≤) ‚âà O(T¬∑10‚Å∂)  for 1024¬≤ images
Latent: O(T¬∑h¬≤w¬≤) ‚âà O(T¬∑16K)  for 128¬≤ latents

64√ó speedup in attention operations!

Conditioning (text-to-image):
Cross-attention between latent diffusion and text encoder:

Attention(Q, K, V) = softmax(QK^T/‚àöd)¬∑V

where:
Q = query from diffusion UNet
K, V = key/value from CLIP text encoder

This allows text-conditional generation:
p(z | c) where c = text embedding

Classifier-free guidance:
ŒµÃÉŒ∏(z‚Çú, c, t) = ŒµŒ∏(z‚Çú, ‚àÖ, t) + w¬∑(ŒµŒ∏(z‚Çú, c, t) - ŒµŒ∏(z‚Çú, ‚àÖ, t))

w = guidance weight (typically 7-15)
Higher w ‚Üí stronger conditioning, less diversity
```

**Sampling Algorithm Comparison:**

```
| Method | Steps | Stochastic | Invertible | Speed | Quality |
|--------|-------|------------|------------|-------|---------|
| DDPM | 1000 | Yes | No | 1√ó | Baseline |
| DDIM | 50-100 | No | Yes | 10-20√ó | Similar |
| DPM-Solver | 20-30 | No | Yes | 30-50√ó | Similar |
| LCM | 4-8 | No | Yes | 100-250√ó | Good |

DDPM: Original, slow but high quality
DDIM: Fast deterministic sampling
DPM-Solver: ODE solver, even faster
LCM (Latent Consistency Models): Distilled, 1-4 steps!
```

### Key Models

#### **Stable Diffusion** (Stability AI)
- **Architecture:** Latent Diffusion Model (LDM)
- **Key Innovation:** Work in latent space instead of pixel space
  - Encode images to latent space (VAE encoder)
  - Perform diffusion in latent space (8x smaller!)
  - Decode back to pixels (VAE decoder)
- **Variants:**
  - SD 1.5 (512√ó512)
  - SDXL (1024√ó1024, better quality)
  - SD Turbo (1-4 steps instead of 50!)

```python
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    "stabilityai/stable-diffusion-2-1"
)

image = pipe(
    "A serene landscape with mountains",
    num_inference_steps=50,
    guidance_scale=7.5
).images[0]
```

#### **DALL-E 3** (OpenAI)
- **Improvements over DALL-E 2:**
  - Better prompt following
  - Higher image quality
  - Improved text rendering
  - Safety mitigations

#### **Midjourney v6**
- **Architecture:** Proprietary
- **Strengths:** Artistic quality, prompt understanding
- **Use cases:** Creative design, concept art

### Advanced Techniques

#### **Classifier-Free Guidance**
Balance between diversity and prompt adherence:
```
ŒµÃÉ = Œµ·µ§ + s¬∑(Œµ·∂ú - Œµ·µ§)
```
Where:
- Œµ·µ§: unconditional prediction (no prompt)
- Œµ·∂ú: conditional prediction (with prompt)
- s: guidance scale (typically 7-15)

Higher s = stronger prompt following, less diversity

#### **ControlNet**
Add spatial control to diffusion models:
- Canny edges
- Depth maps
- Segmentation masks
- Pose estimation

```python
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel

controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny")
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    controlnet=controlnet
)

image = pipe(
    prompt="modern living room",
    image=canny_edge_image,
    num_inference_steps=20
).images[0]
```

**Reference:** *Adding Conditional Control to Text-to-Image Diffusion Models* - Zhang et al. (2023)

#### **DreamBooth & LoRA Fine-tuning**
Personalize diffusion models with few images (3-5):
```python
# Train custom concept with LoRA
# "a photo of sks person"
# Only 5 images needed!
# Fine-tune in <10 minutes on 1 GPU
```

**Reference:** *DreamBooth: Fine Tuning Text-to-Image Diffusion Models* - Ruiz et al. (2022)

---

## üëÅÔ∏è Vision Transformers

### Architecture Evolution

#### **Original ViT (Vision Transformer)**
```
Image (224√ó224√ó3)
  ‚Üì Split into patches (16√ó16)
  ‚Üì Linear embedding
  ‚Üì Add position embeddings
  ‚Üì Transformer encoder (12-24 layers)
  ‚Üì Classification head
```

**Key Insight:** Treat image patches as tokens, just like words in NLP!

**Patch Embedding:**
```
Number of patches = (H/P) √ó (W/P)
Example: 224√ó224 image, 16√ó16 patches ‚Üí 14√ó14 = 196 tokens
```

**Reference:** *An Image is Worth 16x16 Words* - Dosovitskiy et al. (2020)

#### **CLIP (Contrastive Language-Image Pretraining)**
Joint text-image embeddings:
```
Images ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îú‚îÄ‚îÄ‚Üí Contrastive Learning ‚îÄ‚îÄ‚Üí Aligned Embeddings
Texts  ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Training:**
```
Maximize similarity: sim(img_i, text_i)
Minimize similarity: sim(img_i, text_j) for i‚â†j
```

**Applications:**
- Zero-shot image classification
- Image search with text queries
- Multimodal embeddings

```python
import torch
from transformers import CLIPProcessor, CLIPModel

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

images = [...]  # List of PIL images
texts = ["a dog", "a cat", "a bird"]

inputs = processor(text=texts, images=images, return_tensors="pt", padding=True)
outputs = model(**inputs)

# Get similarity scores
logits_per_image = outputs.logits_per_image  # Image-text similarity
probs = logits_per_image.softmax(dim=1)
```

**Reference:** *Learning Transferable Visual Models From Natural Language Supervision* - Radford et al. (2021)

#### **SAM (Segment Anything Model)**
Universal image segmentation:
```
Image ‚Üí Image Encoder (ViT) ‚Üí Embeddings
Prompt (points/boxes/text) ‚Üí Prompt Encoder ‚Üí Query
Embeddings + Query ‚Üí Mask Decoder ‚Üí Segmentation Mask
```

**Key Features:**
- Zero-shot segmentation
- Interactive prompting
- 1 billion+ masks trained
- Works on any image domain

```python
from segment_anything import sam_model_registry, SamPredictor

sam = sam_model_registry["vit_h"](checkpoint="sam_vit_h.pth")
predictor = SamPredictor(sam)

predictor.set_image(image)
masks, scores, logits = predictor.predict(
    point_coords=np.array([[500, 375]]),
    point_labels=np.array([1]),
    multimask_output=True
)
```

**Reference:** *Segment Anything* - Kirillov et al. (2023)

#### **DINOv2 (Self-Supervised Vision)**
Self-supervised learning for vision features:
- No labels needed!
- Learns robust image representations
- Excellent for transfer learning

**Applications:**
- Feature extraction
- Image retrieval
- Few-shot learning
- Downstream task fine-tuning

**Reference:** *DINOv2: Learning Robust Visual Features without Supervision* - Oquab et al. (2023)

---

## üîç Retrieval-Augmented Generation (RAG)

### Overview
Combine LLMs with external knowledge retrieval for factual, up-to-date responses.

### Architecture
```
User Query
    ‚Üì
Retriever (find relevant docs)
    ‚Üì
Retrieved Context + Query
    ‚Üì
LLM Generator
    ‚Üì
Grounded Response
```

### Implementation Pipeline

#### **1. Document Processing**
```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Split documents into chunks
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", " ", ""]
)

chunks = splitter.split_documents(documents)
```

#### **2. Embedding & Vector Storage**
```python
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS

# Create embeddings
embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

# Store in vector database
vectorstore = FAISS.from_documents(chunks, embeddings)
```

#### **3. Retrieval**
```python
# Retrieve relevant chunks
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5}
)

relevant_docs = retriever.get_relevant_documents(query)
```

#### **4. Generation**
```python
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI

qa_chain = RetrievalQA.from_chain_type(
    llm=OpenAI(temperature=0),
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True
)

result = qa_chain({"query": "What is the main topic?"})
answer = result["result"]
sources = result["source_documents"]
```

### Advanced RAG Techniques

#### **Hybrid Search**
Combine dense (semantic) and sparse (keyword) retrieval:
```python
# BM25 (sparse) + Dense embeddings
from rank_bm25 import BM25Okapi

# Weighted combination
final_score = Œ± * dense_score + (1-Œ±) * sparse_score
```

#### **Reranking**
Use cross-encoder to rerank retrieved documents:
```python
from sentence_transformers import CrossEncoder

reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
scores = reranker.predict([(query, doc) for doc in docs])
reranked_docs = [doc for _, doc in sorted(zip(scores, docs), reverse=True)]
```

#### **HyDE (Hypothetical Document Embeddings)**
Generate hypothetical answers, then retrieve:
```
Query ‚Üí LLM generates hypothetical answer ‚Üí Embed ‚Üí Retrieve similar docs
```

**Reference:** *Precise Zero-Shot Dense Retrieval without Relevance Labels* - Gao et al. (2022)

#### **Self-RAG**
Model decides when to retrieve and how to use retrieved info:
```
Generate ‚Üí Self-evaluate ‚Üí Retrieve if needed ‚Üí Refine ‚Üí Output
```

**Reference:** *Self-RAG: Learning to Retrieve, Generate, and Critique* - Asai et al. (2023)

### Best Practices

1. **Chunking Strategy:**
   - Use semantic chunking (sentence/paragraph boundaries)
   - Overlap chunks (20-30%) for context
   - Typical size: 500-1500 tokens

2. **Embedding Models:**
   - OpenAI: `text-embedding-3-large` (best quality)
   - Open source: `BAAI/bge-large-en-v1.5`
   - Sentence transformers: `all-MiniLM-L6-v2` (fast)

3. **Vector Databases:**
   - Small scale: FAISS (in-memory)
   - Production: Pinecone, Weaviate, Qdrant
   - Self-hosted: Milvus, Chroma

4. **Evaluation Metrics:**
   - Retrieval: Recall@k, MRR, NDCG
   - Generation: Faithfulness, answer relevance
   - End-to-end: Human evaluation, LLM-as-judge

---

## üß© Mixture of Experts (MoE)

### Rigorous Theory of Mixture of Experts

**Mathematical Foundation:**

```
Problem: Scale model capacity without proportional compute increase

Key Insight (Jacobs et al., 1991):
Conditional computation via sparse expert selection

Ensemble learning: Different models for different inputs
vs Dense models: Same parameters for all inputs
```

**Theorem 6 (MoE Sparse Gating - Shazeer et al., 2017):**

```
Standard MoE formulation:
y = Œ£·µ¢‚Çå‚ÇÅ‚Åø G(x)·µ¢ ¬∑ E·µ¢(x)

where:
- n = total number of experts
- E·µ¢: ‚Ñù·µà ‚Üí ‚Ñù·µà' (i-th expert network)
- G(x): ‚Ñù·µà ‚Üí Œî‚Åø (gating function, outputs probability simplex)

Gating function:
G(x) = softmax(W_g ¬∑ x)
G(x)·µ¢ = exp(W_g^(i) ¬∑ x) / Œ£‚±º exp(W_g^(j) ¬∑ x)

Sparsity constraint (Top-k selection):
KeepTopK(v, k) = {
  v·µ¢  if v·µ¢ in top-k values
  -‚àû  otherwise (masked out in softmax)
}

Sparse MoE:
G_sparse(x) = softmax(KeepTopK(W_g ¬∑ x, k))

Result: Only k experts have G(x)·µ¢ > 0
Computation: O(k ¬∑ d') instead of O(n ¬∑ d')

Example:
n = 128 experts, k = 2
Computation reduction: 128/2 = 64√ó fewer expert evaluations!
```

**Theorem 7 (Load Balancing and Capacity Factor):**

```
Problem: Some experts become overused (load imbalance)
Result: Training instability and underutilization

Load balancing loss (auxiliary):
L_load = Œ± ¬∑ Œ£·µ¢‚Çå‚ÇÅ‚Åø f·µ¢ ¬∑ P·µ¢

where:
- f·µ¢ = fraction of tokens routed to expert i
- P·µ¢ = router probability for expert i (before top-k)
- Œ± = load balancing coefficient (typically 0.01)

Optimal: f·µ¢ = 1/n (uniform distribution)

Capacity factor c:
Each expert processes at most c ¬∑ (B/n) tokens
where B = batch size

If expert receives more: drop excess tokens (expert capacity overflow)

Theorem (Load Balancing Convergence):
With L_load penalty and capacity c ‚â• 1.5:

E[max_i f_i - min_i f_i] ‚â§ O(‚àö(log(n)/T))

where T = training steps

Practical: Use c = 1.25-2.0
- Lower c: More efficient, risk dropping tokens
- Higher c: Less efficient, guaranteed processing

Token dropping rate:
p_drop ‚âà max(0, f·µ¢ - c/n)

Example: n=8, c=1.5, f·µ¢=0.3 (expert overloaded)
p_drop ‚âà max(0, 0.3 - 1.5/8) ‚âà 0.112 (11% tokens dropped!)
```

**Theorem 8 (MoE Parameter and Compute Scaling):**

```
Dense Transformer:
Parameters: P_dense = d_model √ó d_ff √ó n_layers
FLOPS per token: F_dense = 2 ¬∑ P_dense

MoE Transformer (Switch Transformer - Fedus et al., 2021):
Parameters: P_MoE = d_model √ó d_ff √ó n_experts √ó n_layers
Active params: P_active = d_model √ó d_ff √ó k √ó n_layers
FLOPS per token: F_MoE ‚âà 2 ¬∑ P_active

Parameter efficiency:
P_MoE / P_dense = n_experts (e.g., 8√ó more parameters)
F_MoE / F_dense = k (e.g., 2√ó FLOPS if k=2)

Concrete example (Mixtral 8x7B):
- Per expert: 7B parameters
- Total: 8 √ó 7B = 56B parameters
- Top-k: k = 2
- Active: 2 √ó 7B = 14B parameters per token
- Memory saving: 56B/14B = 4√ó (vs dense 56B model)
- FLOPS: Same as dense 14B model

Scaling law (Empirical - Fedus et al.):
For fixed compute budget C:

Performance_MoE ‚àù (C ¬∑ n_experts / k)^Œ±

where Œ± ‚âà 0.05-0.1 (task dependent)

Implication: 8√ó experts with k=2 gives:
Performance improvement ‚àù (8/2)^0.05 ‚âà 1.08 (8% better)
```

**Theorem 9 (Expert Specialization and Entropy):**

```
Entropy of routing distribution (per token):
H(x) = -Œ£·µ¢‚Çå‚ÇÅ‚Åø G(x)·µ¢ ¬∑ log G(x)·µ¢

Properties:
- H(x) = 0: Deterministic routing (1 expert always chosen)
- H(x) = log(n): Uniform routing (all experts equally likely)

Empirical observations (Mixtral, GPT-4):
1. Early layers: H ‚âà 0.7¬∑log(n) (balanced)
2. Later layers: H ‚âà 0.3¬∑log(n) (specialized)

Expert specialization metric:
S·µ¢ = KL(f·µ¢(D) || uniform)

where f·µ¢(D) = distribution over data for expert i

High S·µ¢ ‚Üí Expert i is specialized (handles specific data types)

Empirical findings (Mixtral analysis):
- Expert 1: Math (S‚ÇÅ = 2.3)
- Expert 2: Code (S‚ÇÇ = 2.1)
- Expert 3: General text (S‚ÇÉ = 0.4)
- Expert 4: Multilingual (S‚ÇÑ = 1.8)

Specialization emerges automatically from routing gradient!
```

**Theorem 10 (MoE Training Stability):**

```
Challenges:
1. Router collapse: All tokens routed to few experts
2. Gradient imbalance: Some experts receive no gradients
3. Representation collapse: Experts become identical

Solutions:

1. Router z-loss (Zoph et al., 2022):
L_z = (1/B) Œ£‚Çì (log Œ£·µ¢ exp(logits_i(x)))¬≤

Encourages: logits to be small in magnitude
Effect: Prevents router from being overconfident

2. Expert dropout:
Randomly drop entire experts during training
Forces robustness and prevents over-reliance

3. Auxiliary load balancing:
L_total = L_task + Œ±¬∑L_load + Œ≤¬∑L_z

Typical: Œ± = 0.01, Œ≤ = 0.001

Convergence theorem (Empirical):
With proper regularization (L_load + L_z):

P(expert_utilization > 0.5/n) ‚Üí 0 as T ‚Üí ‚àû

I.e., all experts remain active during training
```

**Comparison: MoE vs Dense Models:**

```
| Metric | Dense 175B | MoE 8√ó175B (k=2) | MoE 64√ó22B (k=2) |
|--------|-----------|------------------|------------------|
| Total Params | 175B | 1.4T | 1.4T |
| Active Params | 175B | 350B | 44B |
| FLOPS/token | 350T | 700T | 88T |
| Memory (inference) | 700GB | 700GB (smart routing) | 700GB |
| Memory (training) | 1.4TB | 11TB (all experts) | 11TB |
| Training time | 1√ó | 1.5√ó | 1√ó |
| Quality (perplexity) | Baseline | -15% | -8% |

Key observations:
1. Inference: Sparse activation ‚Üí similar FLOPS to smaller dense
2. Training: All experts loaded ‚Üí high memory requirement
3. Quality: More total params ‚Üí better performance at fixed FLOPS
```

### Architecture
```
Input
  ‚Üì
Router Network (decides which experts to use)
  ‚Üì
Top-K Experts (only K out of N experts activated)
  ‚Üì
Weighted combination of expert outputs
  ‚Üì
Output
```

### Mathematical Formulation
```
y = Œ£·µ¢ G·µ¢(x) ¬∑ E·µ¢(x)
```
Where:
- G(x): Gating function (softmax of router logits)
- E·µ¢(x): i-th expert network
- Only top-k experts have non-zero G(x)

### Advantages

**1. Sparse Activation:**
- Total parameters: 1T
- Active parameters per token: 100B
- **8x memory efficiency during inference!**

**2. Specialization:**
- Different experts learn different skills
- Expert 1: Math & logic
- Expert 2: Creative writing
- Expert 3: Code generation
- Expert 4: Factual knowledge

**3. Scalability:**
- Add more experts without proportionally increasing compute
- Scale parameters independently of FLOP

### Modern MoE Models

#### **GPT-4** (Speculative - Unconfirmed)
- *Architecture details not officially disclosed by OpenAI*
- *Speculation based on analysis suggests possible MoE with:*
  - *~8 experts √ó ~220B parameters each ‚âà ~1.76T total*
  - *Top-2 routing per token*
  - *Active: ~440B parameters per forward pass*
- **Important:** These are unverified estimates from the research community

#### **Mixtral 8x7B** (Mistral AI)
- 8 experts √ó 7B parameters = 56B total
- Top-2 routing
- Active: 14B parameters (matches GPT-3.5 performance!)
- **Open source and Apache 2.0 licensed**

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mixtral-8x7B-Instruct-v0.1",
    device_map="auto",
    load_in_4bit=True  # Quantized for 1 GPU!
)

tokenizer = AutoTokenizer.from_pretrained("mistralai/Mixtral-8x7B-Instruct-v0.1")

inputs = tokenizer("Explain quantum computing", return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=200)
```

#### **Switch Transformer** (Google)
- 1.6 trillion parameters
- Top-1 routing (simplest)
- Trained on massive TPU clusters

**Reference:** *Switch Transformers: Scaling to Trillion Parameter Models* - Fedus et al. (2021)

### Training Challenges

**1. Load Balancing:**
All experts should be used equally:
```
L_balance = Œ± ¬∑ Œ£·µ¢ f·µ¢ ¬∑ P·µ¢
```
Where:
- f·µ¢: fraction of tokens routed to expert i
- P·µ¢: fraction of router probability for expert i

**2. Expert Collapse:**
Router learns to use only a few experts ‚Üí Add auxiliary loss

**3. Communication Overhead:**
Experts on different devices ‚Üí Use expert parallelism

---

## üéØ Constitutional AI and RLHF

### RLHF (Reinforcement Learning from Human Feedback)

#### **Three-Stage Process:**

**Stage 1: Supervised Fine-Tuning (SFT)**
```
Pretrained LLM ‚Üí Fine-tune on high-quality demonstrations ‚Üí SFT Model
```

**Stage 2: Reward Model Training**
```
Pairs of outputs ‚Üí Human ranks preferred output ‚Üí Train reward model
```
Reward model learns: `r(x, y)` = quality score for output y given input x

**Stage 3: RL Optimization**
Use PPO (Proximal Policy Optimization):
```
Objective: max ùîº[r(x, y)] - Œ≤¬∑KL(œÄ || œÄ_SFT)
```
Where:
- r(x, y): reward from trained reward model
- KL divergence: keep model close to SFT model
- Œ≤: controls how much we deviate from SFT

```python
# Conceptual RLHF pipeline
# Stage 1: SFT
sft_model = train_supervised(base_model, demonstrations)

# Stage 2: Reward model
reward_model = train_reward_model(
    comparisons=[
        (prompt, good_response, bad_response),
        ...
    ]
)

# Stage 3: PPO training
for batch in dataloader:
    prompts = batch["prompts"]
    responses = sft_model.generate(prompts)
    rewards = reward_model(prompts, responses)

    # PPO update
    ppo_loss = compute_ppo_loss(rewards, responses)
    update_model(ppo_loss)
```

**Reference:** *Training Language Models to Follow Instructions with Human Feedback* - Ouyang et al. (2022)

### Constitutional AI (CAI)

Align models using AI feedback instead of human feedback!

#### **Process:**

**1. Critique Phase:**
```
Model Output ‚Üí AI Critic (using constitution) ‚Üí Identified problems
```

**2. Revision Phase:**
```
Original output + Critique ‚Üí Revise ‚Üí Improved output
```

**3. Train on Revisions:**
```
Train model to directly produce revised outputs
```

**Constitution Example:**
```yaml
principles:
  - "Choose the response that is most helpful, honest, and harmless"
  - "Avoid outputs that could be dangerous or illegal"
  - "Prefer responses that respect human autonomy"
  - "Don't help with unethical requests"
```

**Advantages:**
- Scalable (no human labeling bottleneck)
- Transparent (principles are explicit)
- Iterative improvement
- Reduces harmful outputs

**Reference:** *Constitutional AI: Harmlessness from AI Feedback* - Bai et al. (2022)

### DPO (Direct Preference Optimization)

Simpler alternative to RLHF - skip the reward model!

**Direct optimization:**
```
L = -ùîº[(log œÉ(Œ≤ log(œÄ(y_w|x)/œÄ_ref(y_w|x)) - Œ≤ log(œÄ(y_l|x)/œÄ_ref(y_l|x))))]
```
Where:
- y_w: preferred (winning) response
- y_l: dispreferred (losing) response
- œÄ_ref: reference model (SFT model)

**Advantages:**
- No separate reward model needed
- Simpler training pipeline
- More stable than PPO
- Same or better performance

```python
# DPO training (simplified)
for batch in preference_data:
    prompt, chosen, rejected = batch

    chosen_logprob = model.log_prob(chosen | prompt)
    rejected_logprob = model.log_prob(rejected | prompt)

    ref_chosen_logprob = ref_model.log_prob(chosen | prompt)
    ref_rejected_logprob = ref_model.log_prob(rejected | prompt)

    loss = -log_sigmoid(
        beta * (chosen_logprob - ref_chosen_logprob) -
        beta * (rejected_logprob - ref_rejected_logprob)
    )

    update_model(loss)
```

**Reference:** *Direct Preference Optimization* - Rafailov et al. (2023)

---

## ‚ö° Model Quantization

### Overview
Reduce model precision to decrease memory and increase inference speed.

### Quantization Levels

| Precision | Bits | Memory | Speed | Quality |
|-----------|------|--------|-------|---------|
| FP32 | 32 | 1.0x | 1.0x | 100% |
| FP16 | 16 | 0.5x | 2.0x | ~99.9% |
| BF16 | 16 | 0.5x | 2.0x | ~99.95% |
| INT8 | 8 | 0.25x | 3-4x | ~99% |
| INT4 | 4 | 0.125x | 5-6x | ~95-98% |

### Quantization Methods

#### **1. Post-Training Quantization (PTQ)**
Quantize after training - no retraining needed!

**Symmetric Quantization:**
```
Q(x) = round(x / scale) ¬∑ scale
scale = max(|x|) / (2^(bits-1) - 1)
```

**Asymmetric Quantization:**
```
Q(x) = round((x - zero_point) / scale)
scale = (max(x) - min(x)) / (2^bits - 1)
zero_point = round(-min(x) / scale)
```

#### **2. Quantization-Aware Training (QAT)**
Include quantization in training loop:
```python
# Fake quantization during training
def fake_quantize(x, scale, zero_point, bits=8):
    qmin, qmax = 0, 2**bits - 1
    x_int = torch.round(x / scale + zero_point)
    x_int = torch.clamp(x_int, qmin, qmax)
    x_dequant = (x_int - zero_point) * scale
    return x_dequant

# Forward pass
x_quant = fake_quantize(x, scale, zero_point)
# Backward pass uses gradients w.r.t. original x
```

#### **3. GPTQ (GPT Quantization)**
Optimal 4-bit quantization for LLMs:
- Layer-wise quantization
- Minimizes reconstruction error
- Calibration on small dataset (~128 samples)

**7B model in 4GB of memory instead of 28GB!**

```python
from transformers import AutoModelForCausalLM, GPTQConfig

quantization_config = GPTQConfig(
    bits=4,
    dataset="c4",
    tokenizer=tokenizer
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    quantization_config=quantization_config,
    device_map="auto"
)
```

**Reference:** *GPTQ: Accurate Post-Training Quantization for GPT Models* - Frantar et al. (2022)

#### **4. GGUF/GGML Format**
Quantization format optimized for CPU inference:
- Multiple quantization schemes (Q4_0, Q4_1, Q5_0, Q8_0)
- Highly optimized for Apple Silicon and AVX2
- Used by llama.cpp

**Quantization Schemes:**
```
Q4_0: 4-bit, small file size, lowest quality
Q4_K_M: 4-bit, medium quality (recommended)
Q5_K_M: 5-bit, higher quality
Q8_0: 8-bit, best quality
```

#### **5. AWQ (Activation-aware Weight Quantization)**
Protect important weights from quantization:
- Analyze activation patterns
- Keep salient weights in higher precision
- Better quality than naive quantization

**Reference:** *AWQ: Activation-aware Weight Quantization* - Lin et al. (2023)

### Practical Example: Quantize Llama 2

```python
# Option 1: GPTQ (4-bit for GPU)
from transformers import AutoModelForCausalLM, AutoTokenizer
from auto_gptq import AutoGPTQForCausalLM

model = AutoGPTQForCausalLM.from_quantized(
    "TheBloke/Llama-2-7B-GPTQ",
    device="cuda:0",
    use_safetensors=True
)

# Option 2: GGUF (for CPU, especially Apple Silicon)
# Using llama-cpp-python
from llama_cpp import Llama

model = Llama(
    model_path="llama-2-7b.Q4_K_M.gguf",
    n_ctx=2048,
    n_threads=8
)

# Option 3: BitsAndBytes (simple integration)
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    load_in_4bit=True,
    device_map="auto"
)
```

### When to Use Each Method

| Method | Use Case | Quality | Speed | Ease |
|--------|----------|---------|-------|------|
| GPTQ | GPU inference, LLMs | High | Fast | Medium |
| GGUF | CPU inference, local | Medium-High | Medium | Easy |
| AWQ | GPU inference, quality-critical | Highest | Fast | Medium |
| BitsAndBytes | Quick prototyping | Medium | Fast | Very Easy |

---

## üé≠ Multimodal Models

### Overview
Models that understand multiple modalities: text, images, audio, video.

### State-of-the-Art Models

#### **GPT-4V (Vision)**
- Text + image understanding
- Can analyze charts, diagrams, screenshots
- Spatial reasoning and OCR

#### **Gemini Ultra**
- Native multimodal training
- Text + image + audio
- Strong on mathematical diagrams

#### **LLaVA (Large Language and Vision Assistant)**
Open-source vision-language model:
```python
from llava.model import LlavaLlamaForCausalLM
from llava.conversation import conv_templates

# Load model
model = LlavaLlamaForCausalLM.from_pretrained("liuhaotian/llava-v1.5-7b")

# Multimodal conversation
conv = conv_templates["v1"].copy()
conv.append_message(conv.roles[0], "What's in this image?")
prompt = conv.get_prompt()

# Process image + text
outputs = model.generate(
    input_ids=text_inputs,
    images=image_tensor,
    max_new_tokens=200
)
```

**Training Strategy:**
1. Pretrain vision encoder (CLIP)
2. Pretrain LLM
3. Train projection layer (vision ‚Üí text space)
4. Fine-tune end-to-end on instruction data

**Reference:** *Visual Instruction Tuning* - Liu et al. (2023)

#### **BLIP-2**
Bootstrapping vision-language pretraining:
- Frozen vision encoder
- Frozen LLM
- Learnable Q-Former (queries visual features)

**Advantage:** Reuse existing pretrained models efficiently!

**Reference:** *BLIP-2: Bootstrapping Language-Image Pre-training* - Li et al. (2023)

---

## üöÑ Efficient Training Techniques

### 1. Flash Attention
Faster and more memory-efficient attention:
```
Standard Attention: O(N¬≤) memory
Flash Attention: O(N) memory
Speed: 2-4x faster!
```

**How it works:**
- Tiling: Compute attention in blocks
- Recomputation: Recompute attention in backward pass
- Avoid materializing full N√óN matrix

```python
# PyTorch 2.0+
import torch.nn.functional as F

# Automatic with torch.compile
output = F.scaled_dot_product_attention(
    query, key, value,
    is_causal=True  # Uses Flash Attention if available
)
```

**Reference:** *FlashAttention: Fast and Memory-Efficient Exact Attention* - Dao et al. (2022)

### 2. Gradient Checkpointing
Trade compute for memory:
```
Normal: Store all activations (high memory)
Checkpointing: Recompute activations in backward pass (low memory)
```

```python
# HuggingFace Transformers
model.gradient_checkpointing_enable()

# PyTorch
from torch.utils.checkpoint import checkpoint

def forward_with_checkpointing(x):
    return checkpoint(expensive_function, x)
```

### 3. Mixed Precision Training
Use FP16/BF16 for speed, FP32 for stability:
```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for batch in dataloader:
    with autocast():
        loss = model(batch)

    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

**Benefits:**
- 2-3x faster training
- 50% less memory
- Minimal accuracy loss

### 4. DeepSpeed ZeRO
Distribute optimizer states, gradients, and parameters:
```
ZeRO-1: Partition optimizer states ‚Üí 4x memory reduction
ZeRO-2: + Partition gradients ‚Üí 8x memory reduction
ZeRO-3: + Partition parameters ‚Üí Linear scaling!
```

```python
# DeepSpeed config
{
    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "cpu"
        },
        "offload_param": {
            "device": "cpu"
        }
    }
}
```

**Train 175B model on 64 GPUs (vs 1024 GPUs without ZeRO)!**

**Reference:** *ZeRO: Memory Optimizations Toward Training Trillion Parameter Models* - Rajbhandari et al. (2020)

### 5. FSDP (Fully Sharded Data Parallel)
PyTorch's native alternative to DeepSpeed:
```python
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP

model = FSDP(
    model,
    sharding_strategy=ShardingStrategy.FULL_SHARD,
    mixed_precision=MixedPrecision(...)
)
```

---

## üîÆ Emerging Architectures

### 1. Mamba (State Space Models)
Alternative to transformers with linear-time inference:
```
Transformers: O(N¬≤) attention
Mamba: O(N) selective state space
```

**Advantages:**
- Constant memory during inference
- Scales to very long sequences (100K+ tokens)
- Competitive with transformers on language tasks

**Reference:** *Mamba: Linear-Time Sequence Modeling with Selective State Spaces* - Gu & Dao (2023)

### 2. Retentive Networks
Parallel training, recurrent inference:
```
Training: O(N¬≤) parallel (like transformers)
Inference: O(1) memory per step (like RNNs)
```

**Best of both worlds!**

**Reference:** *Retentive Network: A Successor to Transformer* - Sun et al. (2023)

### 3. Hyena Hierarchy
Sub-quadratic attention alternative using convolutions:
```
Long convolutions (FFT-based) + Gating
‚Üí Sub-quadratic complexity
‚Üí Competitive with attention
```

**Reference:** *Hyena Hierarchy: Towards Larger Convolutional Language Models* - Poli et al. (2023)

---

## üìö Key Papers Reference

### LLMs
- GPT-4 Technical Report - OpenAI (2023)
- Llama 2 - Touvron et al. (2023)
- Constitutional AI - Bai et al. (2022)
- Scaling Laws - Kaplan et al. (2020)

### Diffusion Models
- Denoising Diffusion Probabilistic Models - Ho et al. (2020)
- Latent Diffusion Models - Rombach et al. (2022)
- ControlNet - Zhang et al. (2023)

### Vision
- Vision Transformer - Dosovitskiy et al. (2020)
- CLIP - Radford et al. (2021)
- Segment Anything - Kirillov et al. (2023)

### Efficiency
- LoRA - Hu et al. (2021)
- FlashAttention - Dao et al. (2022)
- QLoRA - Dettmers et al. (2023)
- ZeRO - Rajbhandari et al. (2020)

### RAG
- Self-RAG - Asai et al. (2023)
- HyDE - Gao et al. (2022)
- Dense Passage Retrieval - Karpukhin et al. (2020)

---

## üõ†Ô∏è Practical Tools & Frameworks

### LLM Development
- **HuggingFace Transformers:** Unified API for all models
- **LangChain:** LLM application framework
- **LlamaIndex:** Data framework for LLM apps
- **vLLM:** High-throughput inference engine
- **OpenLLM:** Deploy LLMs in production

### Fine-tuning
- **Axolotl:** Simplified fine-tuning tool
- **PEFT:** Parameter-efficient fine-tuning
- **Ludwig:** Low-code ML platform
- **Unsloth:** Fast and memory-efficient fine-tuning

### Quantization
- **llama.cpp:** CPU/Apple Silicon inference
- **GPTQ-for-LLaMa:** GPU quantization
- **BitsAndBytes:** Easy 4/8-bit quantization
- **AutoGPTQ:** Automated GPTQ

### Deployment
- **TensorRT-LLM:** NVIDIA optimized inference
- **Text Generation Inference (TGI):** HuggingFace inference
- **Ollama:** Local LLM runner
- **LM Studio:** GUI for local LLMs

### Monitoring & Evaluation
- **Weights & Biases:** Experiment tracking
- **MLflow:** ML lifecycle management
- **Arize:** ML observability
- **LangSmith:** LLM app debugging

---

## üéØ Interview Questions

### LLMs
1. **Explain the difference between GPT-4 and Llama 2 architectures.**
2. **How does LoRA achieve parameter-efficient fine-tuning?**
3. **What are the trade-offs between instruction tuning and RLHF?**
4. **Explain the Chinchilla scaling laws and their implications.**
5. **How would you deploy a 70B parameter model with limited GPU memory?**

### Diffusion Models
1. **Explain the forward and reverse diffusion processes.**
2. **What is classifier-free guidance and why is it important?**
3. **How does Stable Diffusion differ from DALL-E 2?**
4. **What is the role of the VAE in latent diffusion models?**
5. **How would you add spatial control to image generation?**

### RAG
1. **Explain the RAG architecture and its components.**
2. **What are the differences between dense and sparse retrieval?**
3. **How would you evaluate a RAG system?**
4. **What is HyDE and when would you use it?**
5. **How do you prevent hallucinations in RAG?**

### Efficiency
1. **Explain the difference between 4-bit and 8-bit quantization.**
2. **How does Flash Attention achieve memory efficiency?**
3. **What is the difference between ZeRO-2 and ZeRO-3?**
4. **When would you use gradient checkpointing?**
5. **Compare GPTQ and GGUF quantization formats.**

---

## üöÄ Next Steps

### Hands-On Projects
1. **Fine-tune Llama 2 with LoRA** on custom dataset
2. **Build a RAG system** for your documents
3. **Deploy quantized model** on consumer hardware
4. **Create custom ControlNet** for specific art style
5. **Implement multi-modal chatbot** with vision

### Further Learning
- Follow ML papers on arXiv (cs.AI, cs.LG, cs.CL, cs.CV)
- Join Discord communities (HuggingFace, LocalLLaMA)
- Contribute to open-source projects
- Participate in Kaggle competitions
- Build and share your own models

---

**Last Updated:** November 2025
**Next Update:** January 2026 (Q1 2026 techniques)

**Disclaimer:** Model specifications for proprietary models (GPT-4, Gemini, Claude) are based on publicly available information and may not reflect actual implementations. Always verify critical information from official sources.

---

*For the latest updates and community discussions, see the main [README](./README.md)*
