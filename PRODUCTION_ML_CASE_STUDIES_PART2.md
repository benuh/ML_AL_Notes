# Production ML Case Studies - Part 2

**Cases 2-5: Fraud Detection, Predictive Maintenance, Churn Prediction, Content Moderation**

> Continuation of [PRODUCTION_ML_CASE_STUDIES.md](./PRODUCTION_ML_CASE_STUDIES.md)

---

## Case Study 2: Financial Fraud Detection

### 2.1 Business Context

**Company:** Digital payment platform (50M transactions/day, $10B annual volume)

**Business Problem:**
- Fraud losses: $15M/year
- False positive rate: 5% (blocking legitimate transactions)
- Manual review backlog: 10,000 cases/day
- Response time: Too slow (fraudsters adapt quickly)

**Success Metrics:**
- Reduce fraud losses by 40%
- Reduce false positive rate to <2%
- Detection latency <50ms
- Precision @top-1000 >90%

---

### 2.2 Data Architecture

**Real-Time Feature Engineering:**

```python
import numpy as np
from datetime import datetime, timedelta

class FraudFeatureEngine:
    """
    Real-time feature engineering for fraud detection.
    """

    def __init__(self, redis_client):
        self.redis = redis_client

    def extract_transaction_features(self, transaction):
        """
        Extract features from a single transaction.
        """
        features = {}

        # === Basic Transaction Features ===
        features['amount'] = transaction['amount']
        features['log_amount'] = np.log1p(transaction['amount'])
        features['hour'] = transaction['timestamp'].hour
        features['day_of_week'] = transaction['timestamp'].weekday()
        features['is_weekend'] = 1 if transaction['timestamp'].weekday() >= 5 else 0

        # Payment method
        features['payment_method'] = transaction['payment_method']  # credit_card, debit, etc.
        features['is_new_payment_method'] = self._is_new_payment_method(
            transaction['user_id'],
            transaction['payment_method']
        )

        # === User Historical Features ===
        user_history = self._get_user_history(transaction['user_id'])

        # Transaction velocity (last 1h, 24h, 7d)
        features['txn_count_1h'] = user_history['txn_count_1h']
        features['txn_count_24h'] = user_history['txn_count_24h']
        features['txn_count_7d'] = user_history['txn_count_7d']

        # Amount statistics
        features['avg_txn_amount_30d'] = user_history['avg_amount_30d']
        features['std_txn_amount_30d'] = user_history['std_amount_30d']

        # Deviation from normal behavior
        if features['std_txn_amount_30d'] > 0:
            features['amount_zscore'] = (
                (transaction['amount'] - features['avg_txn_amount_30d']) /
                features['std_txn_amount_30d']
            )
        else:
            features['amount_zscore'] = 0

        # Time since last transaction
        features['time_since_last_txn_minutes'] = (
            (transaction['timestamp'] - user_history['last_txn_time']).total_seconds() / 60
            if user_history['last_txn_time'] else 999999
        )

        # === Merchant Features ===
        merchant_stats = self._get_merchant_stats(transaction['merchant_id'])

        features['merchant_fraud_rate_30d'] = merchant_stats['fraud_rate_30d']
        features['merchant_total_txns_30d'] = merchant_stats['total_txns_30d']
        features['merchant_avg_amount'] = merchant_stats['avg_amount']

        # Is this merchant new for this user?
        features['is_new_merchant'] = self._is_new_merchant(
            transaction['user_id'],
            transaction['merchant_id']
        )

        # === Location Features ===
        # Distance from user's typical location
        user_location = user_history.get('typical_location')
        if user_location:
            features['distance_from_home_km'] = self._calculate_distance(
                transaction['location'],
                user_location
            )
        else:
            features['distance_from_home_km'] = 0

        # Velocity (distance / time since last transaction)
        if (features['time_since_last_txn_minutes'] > 0 and
            features['time_since_last_txn_minutes'] < 1440):  # Within 24 hours
            last_location = user_history.get('last_location')
            if last_location:
                distance_km = self._calculate_distance(transaction['location'], last_location)
                # Impossible velocity check (>800 km/h suggests fraud)
                velocity = distance_km / (features['time_since_last_txn_minutes'] / 60)
                features['velocity_kmh'] = velocity
                features['impossible_velocity'] = 1 if velocity > 800 else 0
            else:
                features['velocity_kmh'] = 0
                features['impossible_velocity'] = 0
        else:
            features['velocity_kmh'] = 0
            features['impossible_velocity'] = 0

        # === Device Features ===
        device_stats = self._get_device_stats(transaction['device_id'])

        features['is_new_device'] = self._is_new_device(
            transaction['user_id'],
            transaction['device_id']
        )
        features['device_fraud_rate'] = device_stats['fraud_rate_30d']

        # === Network Features ===
        # Graph-based features (user-merchant-device network)
        features['connected_fraud_cases'] = self._count_connected_fraud(
            transaction['user_id'],
            transaction['merchant_id'],
            transaction['device_id']
        )

        return features

    def _get_user_history(self, user_id):
        """Get user's transaction history from Redis."""
        key = f"user_history:{user_id}"
        history = self.redis.hgetall(key)

        return {
            'txn_count_1h': int(history.get('txn_count_1h', 0)),
            'txn_count_24h': int(history.get('txn_count_24h', 0)),
            'txn_count_7d': int(history.get('txn_count_7d', 0)),
            'avg_amount_30d': float(history.get('avg_amount_30d', 0)),
            'std_amount_30d': float(history.get('std_amount_30d', 0)),
            'last_txn_time': pd.to_datetime(history.get('last_txn_time')) if 'last_txn_time' in history else None,
            'typical_location': eval(history.get('typical_location')) if 'typical_location' in history else None,
            'last_location': eval(history.get('last_location')) if 'last_location' in history else None
        }

    def update_user_history(self, user_id, transaction):
        """Update user history in Redis (called after transaction)."""
        key = f"user_history:{user_id}"

        # Increment counters with expiration
        self.redis.hincrby(key, 'txn_count_1h', 1)
        self.redis.expire(f"{key}:txn_count_1h", 3600)

        self.redis.hincrby(key, 'txn_count_24h', 1)
        self.redis.expire(f"{key}:txn_count_24h", 86400)

        # Update other fields
        self.redis.hset(key, 'last_txn_time', transaction['timestamp'].isoformat())
        self.redis.hset(key, 'last_location', str(transaction['location']))

    def _calculate_distance(self, loc1, loc2):
        """Calculate haversine distance between two (lat, lon) points."""
        from math import radians, cos, sin, asin, sqrt

        lat1, lon1 = radians(loc1[0]), radians(loc1[1])
        lat2, lon2 = radians(loc2[0]), radians(loc2[1])

        dlat = lat2 - lat1
        dlon = lon2 - lon1

        a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
        c = 2 * asin(sqrt(a))
        r = 6371  # Earth radius in km

        return c * r
```

---

### 2.3 Model Architecture

**Gradient Boosting + Deep Learning Ensemble:**

```python
import torch
import torch.nn as nn
import xgboost as xgb

class FraudDetectionEnsemble:
    """
    Ensemble combining XGBoost and Neural Network.
    """

    def __init__(self):
        # XGBoost for tabular features
        self.xgb_model = xgb.XGBClassifier(
            n_estimators=500,
            max_depth=6,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            objective='binary:logistic',
            eval_metric='auc',
            tree_method='gpu_hist'  # GPU acceleration
        )

        # Neural network for complex interactions
        self.nn_model = FraudNN(input_dim=100, hidden_dims=[256, 128, 64])

        # Meta-learner (combines predictions)
        self.meta_model = nn.Sequential(
            nn.Linear(2, 8),
            nn.ReLU(),
            nn.Linear(8, 1),
            nn.Sigmoid()
        )

    def train(self, X_train, y_train, X_val, y_val):
        """Train ensemble."""

        # Train XGBoost
        self.xgb_model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            early_stopping_rounds=50,
            verbose=True
        )

        # Train Neural Network
        train_nn(self.nn_model, X_train, y_train, X_val, y_val)

        # Generate meta-features
        xgb_train_pred = self.xgb_model.predict_proba(X_train)[:, 1]
        nn_train_pred = predict_nn(self.nn_model, X_train)

        meta_X_train = np.column_stack([xgb_train_pred, nn_train_pred])

        # Train meta-learner
        train_meta_learner(self.meta_model, meta_X_train, y_train)

    def predict_proba(self, X):
        """Predict fraud probability."""
        xgb_pred = self.xgb_model.predict_proba(X)[:, 1]
        nn_pred = predict_nn(self.nn_model, X)

        meta_X = np.column_stack([xgb_pred, nn_pred])
        meta_X_tensor = torch.FloatTensor(meta_X)

        with torch.no_grad():
            final_pred = self.meta_model(meta_X_tensor).numpy()

        return final_pred


class FraudNN(nn.Module):
    """
    Neural network for fraud detection.
    """

    def __init__(self, input_dim, hidden_dims=[256, 128, 64]):
        super(FraudNN, self).__init__()

        layers = []
        prev_dim = input_dim

        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.3)
            ])
            prev_dim = hidden_dim

        layers.append(nn.Linear(prev_dim, 1))
        layers.append(nn.Sigmoid())

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)
```

---

### 2.4 Production System

**Real-Time Scoring Service:**

```python
from fastapi import FastAPI
import asyncio

app = FastAPI()

# Load models
fraud_ensemble = FraudDetectionEnsemble()
fraud_ensemble.load_models()

# Feature engine
feature_engine = FraudFeatureEngine(redis_client)

# Rule engine (for obvious fraud patterns)
class RuleEngine:
    """
    Rule-based fraud detection for obvious cases.
    """

    @staticmethod
    def check_rules(transaction, features):
        """
        Check rule-based patterns.

        Returns:
            (is_fraud, reason, confidence)
        """

        # Rule 1: Amount exceeds user's typical amount by 10x
        if features['amount_zscore'] > 10:
            return True, "Anomalous amount", 0.99

        # Rule 2: Impossible velocity
        if features['impossible_velocity'] == 1:
            return True, "Impossible travel velocity", 0.95

        # Rule 3: Too many transactions in short time
        if features['txn_count_1h'] > 10:
            return True, "Velocity abuse", 0.90

        # Rule 4: Known fraudulent device
        if features['device_fraud_rate'] > 0.5:
            return True, "High-risk device", 0.85

        # Rule 5: Merchant has very high fraud rate
        if features['merchant_fraud_rate_30d'] > 0.3:
            return True, "High-risk merchant", 0.75

        return False, None, 0.0


@app.post("/score_transaction")
async def score_transaction(transaction: dict):
    """
    Score transaction for fraud in real-time.

    SLA: <50ms p99
    """
    import time
    start_time = time.time()

    try:
        # Extract features
        features = feature_engine.extract_transaction_features(transaction)

        # Check rules first (faster, handles obvious cases)
        is_fraud_rule, reason, rule_confidence = RuleEngine.check_rules(transaction, features)

        if is_fraud_rule:
            decision = "DECLINE"
            fraud_score = rule_confidence
            method = "rule"
        else:
            # ML model scoring
            feature_vector = prepare_feature_vector(features)
            fraud_score = fraud_ensemble.predict_proba([feature_vector])[0]

            # Decision logic
            if fraud_score > 0.9:
                decision = "DECLINE"
            elif fraud_score > 0.5:
                decision = "REVIEW"
            else:
                decision = "APPROVE"

            method = "model"
            reason = f"ML score: {fraud_score:.3f}"

        # Update user history asynchronously
        asyncio.create_task(
            feature_engine.update_user_history(transaction['user_id'], transaction)
        )

        latency_ms = (time.time() - start_time) * 1000

        return {
            "transaction_id": transaction['transaction_id'],
            "decision": decision,
            "fraud_score": float(fraud_score),
            "reason": reason,
            "method": method,
            "latency_ms": latency_ms
        }

    except Exception as e:
        # Fail open (approve) to avoid blocking legitimate transactions
        return {
            "transaction_id": transaction['transaction_id'],
            "decision": "APPROVE",
            "fraud_score": 0.0,
            "reason": f"Error: {str(e)}",
            "method": "fallback",
            "error": True
        }
```

---

### 2.5 Results

**Performance Metrics:**

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Fraud Losses | $15M/year | $8.5M/year | **-43%** ‚úÖ |
| False Positive Rate | 5.0% | 1.8% | **-64%** ‚úÖ |
| Precision @1000 | 45% | 92% | **+104%** ‚úÖ |
| Detection Latency p99 | 120ms | 42ms | **-65%** ‚úÖ |
| Manual Review Queue | 10K/day | 2K/day | **-80%** ‚úÖ |

**ROI:** $6.5M savings - $800K implementation cost = **8x ROI**

---

## Case Study 3: Predictive Maintenance for Manufacturing

### 3.1 Business Context

**Company:** Industrial equipment manufacturer (500 machines across 20 facilities)

**Business Problem:**
- Unplanned downtime costs: $5M/year
- Over-maintenance costs: $2M/year
- Equipment lifespan: Underutilized by 30%
- Safety incidents: 12/year

**Success Metrics:**
- Reduce unplanned downtime by 50%
- Reduce maintenance costs by 25%
- Improve equipment utilization by 20%
- Zero safety incidents

---

### 3.2 Data Sources

**Sensor Data (IoT):**

```python
# Time-series sensor data
sensor_data = {
    'timestamp': datetime,
    'machine_id': str,
    'temperature': float,  # ¬∞C
    'vibration': float,  # mm/s
    'pressure': float,  # bar
    'rpm': int,  # rotations per minute
    'power_consumption': float,  # kW
    'oil_level': float,  # %
    'noise_level': float  # dB
}
```

**Maintenance Logs:**

```python
maintenance_log = {
    'timestamp': datetime,
    'machine_id': str,
    'maintenance_type': str,  # scheduled, corrective, emergency
    'failure_mode': str,  # bearing, motor, hydraulic, etc.
    'downtime_hours': float,
    'cost': float,
    'parts_replaced': list
}
```

---

### 3.3 Feature Engineering for Time Series

```python
class MaintenanceFeatureEngine:
    """
    Feature engineering for predictive maintenance.
    """

    def extract_statistical_features(self, sensor_ts, window_hours=24):
        """
        Extract statistical features from sensor time series.

        Args:
            sensor_ts: DataFrame with sensor readings
            window_hours: Rolling window size

        Returns:
            Statistical features
        """
        features = {}

        for sensor in ['temperature', 'vibration', 'pressure', 'rpm', 'power_consumption']:
            # Rolling statistics
            rolling = sensor_ts[sensor].rolling(window=window_hours)

            features[f'{sensor}_mean'] = rolling.mean().iloc[-1]
            features[f'{sensor}_std'] = rolling.std().iloc[-1]
            features[f'{sensor}_min'] = rolling.min().iloc[-1]
            features[f'{sensor}_max'] = rolling.max().iloc[-1]
            features[f'{sensor}_range'] = features[f'{sensor}_max'] - features[f'{sensor}_min']

            # Trend (slope of linear regression)
            features[f'{sensor}_trend'] = self._calculate_trend(sensor_ts[sensor].tail(window_hours))

            # Anomaly score (Z-score)
            mean_30d = sensor_ts[sensor].tail(30*24).mean()
            std_30d = sensor_ts[sensor].tail(30*24).std()
            current_value = sensor_ts[sensor].iloc[-1]

            features[f'{sensor}_zscore'] = (
                (current_value - mean_30d) / std_30d if std_30d > 0 else 0
            )

        # Cross-sensor features
        features['temp_vibration_correlation'] = sensor_ts['temperature'].tail(window_hours).corr(
            sensor_ts['vibration'].tail(window_hours)
        )

        # Operating condition score
        features['operating_hours_since_maintenance'] = self._get_hours_since_maintenance()

        return features

    def extract_frequency_features(self, vibration_signal):
        """
        Extract frequency-domain features using FFT.

        Args:
            vibration_signal: 1D array of vibration measurements

        Returns:
            Frequency features
        """
        from scipy import signal
        from scipy.fft import fft, fftfreq

        # FFT
        fft_values = fft(vibration_signal)
        fft_freq = fftfreq(len(vibration_signal))

        # Power spectrum
        power_spectrum = np.abs(fft_values) ** 2

        features = {
            # Dominant frequency
            'dominant_frequency': fft_freq[np.argmax(power_spectrum)],

            # Spectral centroid
            'spectral_centroid': np.sum(fft_freq * power_spectrum) / np.sum(power_spectrum),

            # Spectral spread
            'spectral_spread': np.sqrt(
                np.sum(((fft_freq - features['spectral_centroid'])**2) * power_spectrum) /
                np.sum(power_spectrum)
            ),

            # Harmonics energy ratio
            'harmonics_ratio': self._calculate_harmonics_ratio(power_spectrum, fft_freq)
        }

        return features

    def _calculate_trend(self, series):
        """Calculate trend using linear regression."""
        from scipy import stats
        x = np.arange(len(series))
        slope, _, _, _, _ = stats.linregress(x, series)
        return slope
```

---

### 3.4 Survival Analysis Model

```python
from lifelines import CoxPHFitter
import torch
import torch.nn as nn

class PredictiveMaintenanceModel:
    """
    Survival analysis model for predicting time to failure.
    """

    def __init__(self):
        # Cox Proportional Hazards model
        self.cox_model = CoxPHFitter()

        # Deep survival network
        self.deep_survival = DeepSurvivalNetwork(input_dim=100)

    def train(self, training_data):
        """
        Train on historical failure data.

        Data format:
        - duration: time until failure (or censoring)
        - event: 1 if failure occurred, 0 if censored
        - features: sensor features
        """

        # Train Cox model
        self.cox_model.fit(
            training_data,
            duration_col='duration',
            event_col='event'
        )

        # Train deep survival network
        train_deep_survival(self.deep_survival, training_data)

    def predict_failure_probability(self, features, time_horizon_hours):
        """
        Predict probability of failure within time horizon.

        Args:
            features: Current sensor features
            time_horizon_hours: Prediction horizon

        Returns:
            failure_probability: P(failure within time_horizon)
            expected_ttf: Expected time to failure
        """

        # Cox model prediction
        survival_function = self.cox_model.predict_survival_function(features)
        cox_prob = 1 - survival_function[time_horizon_hours]

        # Deep survival prediction
        deep_prob = self.deep_survival.predict_probability(features, time_horizon_hours)

        # Ensemble (weighted average)
        failure_probability = 0.6 * cox_prob + 0.4 * deep_prob

        # Expected time to failure
        expected_ttf = self.cox_model.predict_expectation(features)

        return failure_probability, expected_ttf


class DeepSurvivalNetwork(nn.Module):
    """
    Deep learning model for survival analysis.
    """

    def __init__(self, input_dim, hidden_dims=[128, 64, 32]):
        super(DeepSurvivalNetwork, self).__init__()

        # Feature encoder
        layers = []
        prev_dim = input_dim

        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.2)
            ])
            prev_dim = hidden_dim

        # Hazard rate prediction
        layers.append(nn.Linear(prev_dim, 1))

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        """
        Predict hazard rate.
        """
        return self.network(x)

    def predict_probability(self, features, time_horizon):
        """
        Predict failure probability.
        """
        self.eval()
        with torch.no_grad():
            hazard = self.forward(torch.FloatTensor(features))
            # Convert hazard to survival probability
            survival_prob = torch.exp(-hazard * time_horizon)
            failure_prob = 1 - survival_prob

        return failure_prob.item()
```

---

### 3.5 Maintenance Scheduling Optimization

```python
from scipy.optimize import linear_sum_assignment

class MaintenanceScheduler:
    """
    Optimize maintenance schedule considering:
    - Failure probabilities
    - Resource constraints
    - Production schedule
    - Cost optimization
    """

    def __init__(self, machines, technicians, budget):
        self.machines = machines
        self.technicians = technicians
        self.budget = budget

    def optimize_schedule(self, failure_predictions, production_schedule):
        """
        Create optimal maintenance schedule.

        Objective:
        Minimize: downtime_cost + maintenance_cost + failure_risk_cost
        Subject to:
        - Technician availability
        - Budget constraints
        - Production priorities
        """

        # Build cost matrix
        n_machines = len(self.machines)
        n_time_slots = 7 * 24  # Next week, hourly slots

        cost_matrix = np.zeros((n_machines, n_time_slots))

        for i, machine in enumerate(self.machines):
            for t in range(n_time_slots):
                # Cost of maintenance at time t
                maint_cost = self._calculate_maintenance_cost(machine, t)

                # Production impact cost
                prod_cost = self._calculate_production_impact(machine, t, production_schedule)

                # Risk cost (if we delay)
                risk_cost = failure_predictions[machine]['risk'] * (t / 24)  # Increases with delay

                cost_matrix[i, t] = maint_cost + prod_cost + risk_cost

        # Solve assignment problem
        machine_indices, time_indices = linear_sum_assignment(cost_matrix)

        # Create schedule
        schedule = []
        for machine_idx, time_idx in zip(machine_indices, time_indices):
            schedule.append({
                'machine_id': self.machines[machine_idx],
                'scheduled_time': datetime.now() + timedelta(hours=time_idx),
                'estimated_cost': cost_matrix[machine_idx, time_idx],
                'failure_risk': failure_predictions[self.machines[machine_idx]]['probability']
            })

        return schedule
```

---

### 3.6 Results

**Business Impact:**

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Unplanned Downtime | 1,200 hrs/year | 580 hrs/year | **-52%** ‚úÖ |
| Maintenance Costs | $2M/year | $1.5M/year | **-25%** ‚úÖ |
| Equipment Utilization | 65% | 82% | **+26%** ‚úÖ |
| Safety Incidents | 12/year | 1/year | **-92%** ‚úÖ |
| Mean Time Between Failures | 45 days | 78 days | **+73%** ‚úÖ |

**ROI:** $5.5M savings/year - $1.2M implementation = **4.6x ROI**

---

## Case Study 4: Customer Churn Prediction (SaaS)

### 4.1 Quick Summary

**Business:** B2B SaaS platform ($50M ARR, 5,000 customers)

**Problem:** 18% annual churn rate costing $9M/year

**Solution:**
- Gradient Boosting model predicting churn 60 days in advance
- Automated intervention system
- Customer health scoring

**Results:**
- Reduced churn from 18% to 12% (-33%)
- Saved $3M in annual recurring revenue
- Increased customer lifetime value by 28%

**Key Features:**
- Product usage metrics (daily active users, feature adoption)
- Support interaction patterns
- Payment history
- NPS scores and surveys
- Competitive signals

---

## Case Study 5: Real-Time Content Moderation

### 5.1 Quick Summary

**Business:** Social media platform (100M users, 500K posts/hour)

**Problem:**
- Manual moderation can't scale
- 48-hour response time unacceptable
- Inconsistent policy enforcement

**Solution:**
- Multi-modal model (text + image + video)
- Real-time scoring (<100ms)
- Human-in-the-loop for borderline cases
- Active learning pipeline

**Results:**
- Reduced response time from 48 hours to <1 second
- 95% accuracy on policy violations
- 80% reduction in moderator workload
- Consistent policy enforcement

**Architecture:**
- BERT for text understanding
- ResNet + ViT for image classification
- 3D CNN for video analysis
- Ensemble fusion layer
- Confidence calibration
- Explainability (highlight violating content)

---

## Cross-Case Insights

### Common Success Factors

1. **Start Simple**
   - Begin with rule-based systems
   - Baseline models (logistic regression, random forest)
   - Iterate to complex solutions

2. **Data Quality > Model Complexity**
   - Invest in data pipelines
   - Feature engineering is critical
   - Monitor data drift

3. **Production-First Mindset**
   - Design for latency from day one
   - Build monitoring early
   - Plan for failures

4. **Business Metrics Matter Most**
   - Optimize for ROI, not just accuracy
   - A/B test everything
   - Track business impact

5. **Continuous Improvement**
   - Retrain regularly
   - Collect feedback
   - Adapt to changing patterns

### Common Pitfalls

1. ‚ùå **Not monitoring model performance**
2. ‚ùå **Ignoring data quality issues**
3. ‚ùå **Optimizing wrong metrics**
4. ‚ùå **Lack of fallback strategies**
5. ‚ùå **Poor communication with stakeholders**

---

## Summary

These 5 case studies demonstrate:

‚úÖ **Real-world complexity** - Beyond Kaggle datasets
‚úÖ **End-to-end solutions** - From problem to production
‚úÖ **Business impact** - Measurable ROI
‚úÖ **Technical depth** - Architecture, code, deployment
‚úÖ **Lessons learned** - What worked and what didn't

**Key Takeaway:** Production ML is 20% modeling, 80% engineering

---

**Use these case studies to:**
- Understand production ML workflows
- Learn system design patterns
- Prepare for ML system design interviews
- Build similar solutions in your organization

**Happy Building! üöÄ**
