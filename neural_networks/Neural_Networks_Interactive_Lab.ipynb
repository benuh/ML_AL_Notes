{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ§  Neural Networks - From Scratch to Deep Learning\n",
    "\n",
    "## Build Your Understanding from the Ground Up\n",
    "\n",
    "**What you'll learn:**\n",
    "- What neural networks are and how they work\n",
    "- Neurons, layers, and activation functions\n",
    "- Forward propagation and backpropagation\n",
    "- Training, optimization, and loss functions\n",
    "- Build a neural network from scratch\n",
    "- CNNs for images, RNNs for sequences\n",
    "- Hands-on: Recognize handwritten digits\n",
    "\n",
    "**Prerequisites:**\n",
    "- Basic Python\n",
    "- High school math (algebra, calculus helpful)\n",
    "\n",
    "**Time:** 120-150 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š Table of Contents\n",
    "\n",
    "1. [Introduction to Neural Networks](#intro)\n",
    "2. [The Biological Inspiration](#biology)\n",
    "3. [Perceptron - The Building Block](#perceptron)\n",
    "4. [Activation Functions](#activation)\n",
    "5. [Multi-Layer Networks](#multilayer)\n",
    "6. [Forward Propagation](#forward)\n",
    "7. [Backpropagation](#backprop)\n",
    "8. [Training Process](#training)\n",
    "9. [Build from Scratch](#fromscratch)\n",
    "10. [Convolutional Neural Networks](#cnn)\n",
    "11. [Recurrent Neural Networks](#rnn)\n",
    "12. [Modern Architectures](#modern)\n",
    "13. [Exercises](#exercises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "!pip install -q torch torchvision numpy matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## 1. ğŸ¯ Introduction to Neural Networks\n",
    "\n",
    "### What is a Neural Network?\n",
    "\n",
    "A **neural network** is a computing system inspired by biological brains, composed of connected units (neurons) that process information.\n",
    "\n",
    "### Why Neural Networks?\n",
    "\n",
    "**Traditional Programming:**\n",
    "```\n",
    "Rules + Data â†’ Answers\n",
    "```\n",
    "\n",
    "**Neural Networks (Machine Learning):**\n",
    "```\n",
    "Data + Answers â†’ Rules (learned automatically!)\n",
    "```\n",
    "\n",
    "### Evolution Timeline\n",
    "\n",
    "```\n",
    "1943: McCulloch-Pitts Neuron\n",
    "  â†“\n",
    "1958: Perceptron (Rosenblatt)\n",
    "  â†“\n",
    "1986: Backpropagation (Rumelhart, Hinton, Williams)\n",
    "  â†“\n",
    "1998: LeNet (LeCun) - CNNs for handwriting\n",
    "  â†“\n",
    "2012: AlexNet - Deep Learning breakthrough\n",
    "  â†“\n",
    "2014: GANs, Attention mechanisms\n",
    "  â†“\n",
    "2017: Transformers - \"Attention is All You Need\"\n",
    "  â†“\n",
    "2020s: GPT, BERT, Vision Transformers, Diffusion models\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='biology'></a>\n",
    "## 2. ğŸ§¬ The Biological Inspiration\n",
    "\n",
    "### Biological Neuron\n",
    "\n",
    "```\n",
    "           BIOLOGICAL NEURON\n",
    "           =================\n",
    "\n",
    "Dendrites â”€â”€â”\n",
    "            â”‚\n",
    "Dendrites â”€â”€â”¤\n",
    "            â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "Dendrites â”€â”€â”¼â”€â”€â”€â”€â”€â†’â”‚ Cell Body  â”‚â”€â”€â”€â”€â”€â†’â”‚  Axon   â”‚â”€â”€â”€â”€â†’ Output\n",
    "            â”‚      â”‚  (Soma)    â”‚      â”‚         â”‚\n",
    "Dendrites â”€â”€â”˜      â”‚ Processes  â”‚      â”‚ Transmitsâ”‚\n",
    "                   â”‚ signals    â”‚      â”‚ signal   â”‚\n",
    "(Inputs)           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Artificial Neuron\n",
    "\n",
    "```\n",
    "           ARTIFICIAL NEURON\n",
    "           =================\n",
    "\n",
    "xâ‚ â”€â”€â”€â”€â”€â”€â†’ wâ‚ â”€â”€â”\n",
    "                 â”‚\n",
    "xâ‚‚ â”€â”€â”€â”€â”€â”€â†’ wâ‚‚ â”€â”€â”¤\n",
    "                 â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "xâ‚ƒ â”€â”€â”€â”€â”€â”€â†’ wâ‚ƒ â”€â”€â”¼â”€â”€â”€â”€â†’â”‚ Î£ (Sum)      â”‚\n",
    "                 â”‚     â”‚ z = Î£(wáµ¢xáµ¢)  â”‚\n",
    "xâ‚„ â”€â”€â”€â”€â”€â”€â†’ wâ‚„ â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â”‚\n",
    "(Inputs) (Weights)            â–¼\n",
    "                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                       â”‚ Activation   â”‚\n",
    "                       â”‚ f(z + b)     â”‚â”€â”€â”€â”€â†’ y (Output)\n",
    "                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Formula: y = f(Î£(wáµ¢xáµ¢) + b)\n",
    "```\n",
    "\n",
    "### Key Parallels\n",
    "\n",
    "| Biological | Artificial |\n",
    "|------------|------------|\n",
    "| Dendrites | Inputs (x) |\n",
    "| Synapse strength | Weights (w) |\n",
    "| Cell body | Summation (Î£) |\n",
    "| Activation threshold | Activation function |\n",
    "| Axon | Output (y) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='perceptron'></a>\n",
    "## 3. âš¡ Perceptron - The Building Block\n",
    "\n",
    "### The Perceptron\n",
    "\n",
    "The simplest neural network: a single neuron\n",
    "\n",
    "```\n",
    "Input:  x = [xâ‚, xâ‚‚, xâ‚ƒ]\n",
    "Weights: w = [wâ‚, wâ‚‚, wâ‚ƒ]\n",
    "Bias: b\n",
    "\n",
    "Step 1: Weighted sum\n",
    "  z = wâ‚xâ‚ + wâ‚‚xâ‚‚ + wâ‚ƒxâ‚ƒ + b\n",
    "\n",
    "Step 2: Activation\n",
    "  y = f(z)  where f is an activation function\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a simple perceptron\n",
    "class Perceptron:\n",
    "    \"\"\"A simple perceptron for binary classification\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, learning_rate=0.1):\n",
    "        self.weights = np.random.randn(input_size)\n",
    "        self.bias = 0\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def activation(self, z):\n",
    "        \"\"\"Step function: 1 if z > 0, else 0\"\"\"\n",
    "        return 1 if z > 0 else 0\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"Make prediction\"\"\"\n",
    "        z = np.dot(x, self.weights) + self.bias\n",
    "        return self.activation(z)\n",
    "    \n",
    "    def train(self, X, y, epochs=100):\n",
    "        \"\"\"Train using perceptron learning rule\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            for xi, yi in zip(X, y):\n",
    "                # Predict\n",
    "                prediction = self.predict(xi)\n",
    "                \n",
    "                # Update if wrong\n",
    "                error = yi - prediction\n",
    "                self.weights += self.learning_rate * error * xi\n",
    "                self.bias += self.learning_rate * error\n",
    "\n",
    "# Test on AND gate\n",
    "print(\"ğŸ”¬ Training Perceptron on AND Gate\\n\")\n",
    "\n",
    "# AND gate truth table\n",
    "X_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_and = np.array([0, 0, 0, 1])\n",
    "\n",
    "# Train\n",
    "perceptron = Perceptron(input_size=2)\n",
    "perceptron.train(X_and, y_and, epochs=10)\n",
    "\n",
    "# Test\n",
    "print(\"AND Gate Results:\")\n",
    "for x, y in zip(X_and, y_and):\n",
    "    pred = perceptron.predict(x)\n",
    "    print(f\"  {x} â†’ {pred} (expected {y}) {'âœ…' if pred == y else 'âŒ'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='activation'></a>\n",
    "## 4. ğŸ“Š Activation Functions\n",
    "\n",
    "### Why Activation Functions?\n",
    "\n",
    "Without activation functions, neural networks would just be **linear models**:\n",
    "```\n",
    "Layer 1: y = Wâ‚x + bâ‚\n",
    "Layer 2: y = Wâ‚‚yâ‚ + bâ‚‚ = Wâ‚‚(Wâ‚x + bâ‚) + bâ‚‚\n",
    "       = Wâ‚‚Wâ‚x + Wâ‚‚bâ‚ + bâ‚‚\n",
    "       = Wx + b  (still linear!)\n",
    "```\n",
    "\n",
    "Activation functions add **non-linearity** â†’ Can learn complex patterns!\n",
    "\n",
    "### Common Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize activation functions\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "# Define activation functions\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "tanh = lambda x: np.tanh(x)\n",
    "relu = lambda x: np.maximum(0, x)\n",
    "leaky_relu = lambda x: np.where(x > 0, x, 0.01 * x)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "activations = [\n",
    "    (sigmoid, 'Sigmoid: Ïƒ(x) = 1/(1+e^(-x))'),\n",
    "    (tanh, 'Tanh: tanh(x)'),\n",
    "    (relu, 'ReLU: max(0, x)'),\n",
    "    (leaky_relu, 'Leaky ReLU: max(0.01x, x)')\n",
    "]\n",
    "\n",
    "for ax, (func, title) in zip(axes.flat, activations):\n",
    "    y = func(x)\n",
    "    ax.plot(x, y, linewidth=2)\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('f(x)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Properties\n",
    "print(\"\\nğŸ“Š Activation Function Properties:\\n\")\n",
    "print(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(\"â”‚ Function    â”‚ Range    â”‚ Use Case   â”‚ Pros/Cons       â”‚\")\n",
    "print(\"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
    "print(\"â”‚ Sigmoid     â”‚ (0, 1)   â”‚ Binary out â”‚ Smooth, vanish. â”‚\")\n",
    "print(\"â”‚ Tanh        â”‚ (-1, 1)  â”‚ Hidden     â”‚ Zero-centered   â”‚\")\n",
    "print(\"â”‚ ReLU        â”‚ [0, âˆ)   â”‚ Hidden     â”‚ Fast, sparse    â”‚\")\n",
    "print(\"â”‚ Leaky ReLU  â”‚ (-âˆ, âˆ)  â”‚ Hidden     â”‚ No dead neurons â”‚\")\n",
    "print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='multilayer'></a>\n",
    "## 5. ğŸ—ï¸ Multi-Layer Networks\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "            MULTI-LAYER NEURAL NETWORK\n",
    "            ===========================\n",
    "\n",
    "Input Layer    Hidden Layer 1   Hidden Layer 2   Output Layer\n",
    "\n",
    "    xâ‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—‹â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â•±  â”‚ â•²         â”‚\n",
    "    xâ‚‚ â”€â”€â”€â”€â”€â—‹â”€â”€â”€â”€â—‹â”€â”€â—‹â”€â”€â”€â”€â”€â”€â”€â—‹â”€â”€â”€â”€â”€â”€â”€â”€ Å·â‚\n",
    "           â•±â”‚ â•²  â”‚ â•± â•²      â”‚ â•²\n",
    "    xâ‚ƒ â”€â”€â”€â—‹â”€â”€â—‹â”€â”€â”€â—‹â”€â”€â”€â”€â—‹â”€â”€â”€â”€â”€â—‹â”€â”€â”€â”€â”€â”€â”€â”€ Å·â‚‚\n",
    "            â•² â”‚   â”‚   â•±\n",
    "    xâ‚„ â”€â”€â”€â”€â”€â”€â—‹â”€â”€â”€â—‹â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "(4 neurons) (4 neurons)  (3 neurons)  (2 neurons)\n",
    "\n",
    "Each connection has a weight\n",
    "Each neuron has a bias\n",
    "```\n",
    "\n",
    "### Layer Types\n",
    "\n",
    "1. **Input Layer**: Receives raw input data\n",
    "2. **Hidden Layers**: Process and transform data\n",
    "3. **Output Layer**: Produces final prediction\n",
    "\n",
    "### Network Depth\n",
    "\n",
    "- **Shallow**: 1-2 hidden layers\n",
    "- **Deep**: 3+ hidden layers (\"Deep Learning\")\n",
    "\n",
    "```\n",
    "Shallow Network:  Input â†’ Hidden â†’ Output\n",
    "Deep Network:     Input â†’ Hâ‚ â†’ Hâ‚‚ â†’ Hâ‚ƒ â†’ ... â†’ Output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a multi-layer network with PyTorch\n",
    "class SimpleNN(nn.Module):\n",
    "    \"\"\"Simple feedforward neural network\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Build layers\n",
    "        layers = []\n",
    "        \n",
    "        # Input to first hidden\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        # Hidden to hidden\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        # Last hidden to output\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Create a network\n",
    "model = SimpleNN(input_size=10, hidden_sizes=[64, 32, 16], output_size=2)\n",
    "\n",
    "print(\"ğŸ—ï¸ Neural Network Architecture:\\n\")\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='forward'></a>\n",
    "## 6. â¡ï¸ Forward Propagation\n",
    "\n",
    "### The Forward Pass\n",
    "\n",
    "Data flows from input to output:\n",
    "\n",
    "```\n",
    "Layer 1:\n",
    "  zâ‚ = Wâ‚x + bâ‚\n",
    "  aâ‚ = f(zâ‚)\n",
    "\n",
    "Layer 2:\n",
    "  zâ‚‚ = Wâ‚‚aâ‚ + bâ‚‚\n",
    "  aâ‚‚ = f(zâ‚‚)\n",
    "\n",
    "Output:\n",
    "  Å· = aâ‚‚\n",
    "```\n",
    "\n",
    "### Example\n",
    "\n",
    "```\n",
    "Input: x = [1.0, 2.0]\n",
    "Weights: W = [[0.5, -0.3],\n",
    "              [0.2,  0.8]]\n",
    "Bias: b = [0.1, -0.2]\n",
    "\n",
    "Step 1: Compute z\n",
    "  z = Wx + b = [0.5*1.0 + (-0.3)*2.0 + 0.1,\n",
    "                0.2*1.0 +   0.8*2.0 + (-0.2)]\n",
    "    = [-0.5, 1.6]\n",
    "\n",
    "Step 2: Apply ReLU\n",
    "  a = ReLU(z) = [0, 1.6]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement forward propagation from scratch\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def forward_pass(x, W1, b1, W2, b2):\n",
    "    \"\"\"Two-layer network forward pass\"\"\"\n",
    "    # Layer 1\n",
    "    z1 = np.dot(x, W1) + b1\n",
    "    a1 = relu(z1)\n",
    "    \n",
    "    # Layer 2  \n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = relu(z2)\n",
    "    \n",
    "    return a2, (z1, a1, z2)  # Return output and intermediates\n",
    "\n",
    "# Example\n",
    "x = np.array([1.0, 2.0, 3.0])\n",
    "W1 = np.random.randn(3, 4) * 0.01\n",
    "b1 = np.zeros(4)\n",
    "W2 = np.random.randn(4, 2) * 0.01\n",
    "b2 = np.zeros(2)\n",
    "\n",
    "output, intermediates = forward_pass(x, W1, b1, W2, b2)\n",
    "\n",
    "print(\"ğŸ”„ Forward Propagation Example:\\n\")\n",
    "print(f\"Input (x):         {x}\")\n",
    "print(f\"Hidden layer (a1): {intermediates[1]}\")\n",
    "print(f\"Output (Å·):        {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='backprop'></a>\n",
    "## 7. â¬…ï¸ Backpropagation\n",
    "\n",
    "### How Networks Learn\n",
    "\n",
    "**Backpropagation** = Backwards propagation of errors\n",
    "\n",
    "```\n",
    "1. Forward pass: Make prediction\n",
    "2. Compute loss: How wrong were we?\n",
    "3. Backward pass: Compute gradients\n",
    "4. Update weights: Improve for next time\n",
    "```\n",
    "\n",
    "### The Math (Chain Rule)\n",
    "\n",
    "```\n",
    "Loss: L = (Å· - y)Â²\n",
    "\n",
    "To update weight wâ‚:\n",
    "  âˆ‚L/âˆ‚wâ‚ = âˆ‚L/âˆ‚Å· Â· âˆ‚Å·/âˆ‚z Â· âˆ‚z/âˆ‚wâ‚\n",
    "\n",
    "Update rule:\n",
    "  wâ‚_new = wâ‚_old - learning_rate * âˆ‚L/âˆ‚wâ‚\n",
    "```\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "```\n",
    "        Loss\n",
    "         |\n",
    "         |     â—  â† Start (high loss)\n",
    "         |    /\n",
    "         |   â—  â† Step 1\n",
    "         |  /\n",
    "         | â—   â† Step 2\n",
    "         |/\n",
    "     â”€â”€â”€â”€â—â”€â”€â”€â”€ â† Minimum (low loss)\n",
    "       Weight\n",
    "\n",
    "Follow the gradient downhill!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient descent\n",
    "def loss_function(w):\n",
    "    \"\"\"Simple quadratic loss\"\"\"\n",
    "    return (w - 2) ** 2 + 1\n",
    "\n",
    "def gradient(w):\n",
    "    \"\"\"Derivative of loss\"\"\"\n",
    "    return 2 * (w - 2)\n",
    "\n",
    "# Gradient descent\n",
    "w = 5.0  # Starting point\n",
    "learning_rate = 0.1\n",
    "history = [w]\n",
    "\n",
    "for i in range(20):\n",
    "    grad = gradient(w)\n",
    "    w = w - learning_rate * grad\n",
    "    history.append(w)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Left: Loss curve\n",
    "plt.subplot(1, 2, 1)\n",
    "w_range = np.linspace(-1, 6, 100)\n",
    "plt.plot(w_range, loss_function(w_range), 'b-', linewidth=2, label='Loss')\n",
    "plt.plot(history, [loss_function(w) for w in history], 'ro-', markersize=8, label='GD steps')\n",
    "plt.xlabel('Weight (w)', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Gradient Descent Optimization', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Weight convergence\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history, 'go-', markersize=8, linewidth=2)\n",
    "plt.axhline(y=2, color='r', linestyle='--', label='Optimal w=2')\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Weight (w)', fontsize=12)\n",
    "plt.title('Weight Convergence', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ… Converged to w = {history[-1]:.4f} (optimal: 2.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fromscratch'></a>\n",
    "## 9. ğŸ”¨ Build a Neural Network from Scratch\n",
    "\n",
    "Let's implement a complete neural network with backpropagation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete neural network from scratch\n",
    "class NeuralNetworkFromScratch:\n",
    "    def __init__(self, layer_sizes):\n",
    "        \"\"\"Initialize network with given layer sizes\"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            W = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.01\n",
    "            b = np.zeros((1, layer_sizes[i+1]))\n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def relu_derivative(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward propagation\"\"\"\n",
    "        self.z_values = []\n",
    "        self.activations = [X]\n",
    "        \n",
    "        A = X\n",
    "        for W, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            Z = np.dot(A, W) + b\n",
    "            A = self.relu(Z)\n",
    "            self.z_values.append(Z)\n",
    "            self.activations.append(A)\n",
    "        \n",
    "        # Output layer (no activation)\n",
    "        Z = np.dot(A, self.weights[-1]) + self.biases[-1]\n",
    "        self.z_values.append(Z)\n",
    "        self.activations.append(Z)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def backward(self, X, y, learning_rate=0.01):\n",
    "        \"\"\"Backpropagation\"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradient\n",
    "        dZ = self.activations[-1] - y\n",
    "        \n",
    "        # Backpropagate through layers\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            # Gradients\n",
    "            dW = np.dot(self.activations[i].T, dZ) / m\n",
    "            db = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "            \n",
    "            # Update weights\n",
    "            self.weights[i] -= learning_rate * dW\n",
    "            self.biases[i] -= learning_rate * db\n",
    "            \n",
    "            # Propagate gradient to previous layer\n",
    "            if i > 0:\n",
    "                dZ = np.dot(dZ, self.weights[i].T) * self.relu_derivative(self.z_values[i-1])\n",
    "    \n",
    "    def train(self, X, y, epochs=1000, learning_rate=0.01):\n",
    "        \"\"\"Train the network\"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward\n",
    "            predictions = self.forward(X)\n",
    "            \n",
    "            # Loss\n",
    "            loss = np.mean((predictions - y) ** 2)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backward\n",
    "            self.backward(X, y, learning_rate)\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        return losses\n",
    "\n",
    "print(\"âœ… Neural network class implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a simple dataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate data\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
    "y = y.reshape(-1, 1)  # Reshape for network\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create and train network\n",
    "print(\"ğŸš€ Training Neural Network from Scratch\\n\")\n",
    "nn = NeuralNetworkFromScratch([10, 16, 8, 1])\n",
    "losses = nn.train(X_train, y_train, epochs=500, learning_rate=0.1)\n",
    "\n",
    "# Plot training\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.title('Training Progress', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Test accuracy\n",
    "predictions = nn.forward(X_test)\n",
    "predicted_classes = (predictions > 0.5).astype(int)\n",
    "accuracy = np.mean(predicted_classes == y_test)\n",
    "print(f\"\\nâœ… Test Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‰ Conclusion\n",
    "\n",
    "You've learned:\n",
    "\n",
    "âœ… What neural networks are and how they work\n",
    "\n",
    "âœ… Neurons, layers, and activation functions\n",
    "\n",
    "âœ… Forward propagation and backpropagation\n",
    "\n",
    "âœ… Training process and gradient descent\n",
    "\n",
    "âœ… Built a neural network from scratch\n",
    "\n",
    "âœ… CNNs for images and RNNs for sequences\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Experiment with different architectures\n",
    "2. Learn about regularization (dropout, batch norm)\n",
    "3. Study optimization techniques (Adam, RMSprop)\n",
    "4. Explore transfer learning\n",
    "5. Build real applications\n",
    "\n",
    "**Happy learning! ğŸ§ **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
